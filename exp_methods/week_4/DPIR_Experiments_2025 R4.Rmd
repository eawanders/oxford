---
title: "DPIR - Experimental Methods 2024 - R Lab 4"
author: ""
date: "2025-02-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F)
```


## Lab Overview
In this fourth lab session we will focus on a tool and R routine that can often
be useful when working with experimental data:

* Bayesian Additive Regression Trees (BART)

We will look at this tool using simulated data and working on Duch et al. (2023)'s data.


## Preliminaries
Let's load the data of Duch et al. (2023) from the Harvard Dataverse.
```{r}
# This will load in our environment a dataset named "final_finalV2"
load(url("https://dataverse.harvard.edu/api/access/datafile/7337617"))
```


We anticipate the need of the following packages:
```{r results="hide"}
# Install and load necessary packages
if (!require("pacman")) install.packages("pacman")
library(pacman)

# Install BART package if not already installed
if (!require("BART")) install.packages("BART")

# Load packages
pacman::p_load(
  tidyverse, # Cleaning data, plotting plots
  ggplot2, # Plotting
  ggpubr, # Publication-ready plots
  BART # Bayesian Additive Regression Trees
)
```

## Bayesian Additive Regression Trees (BART)

### The problem
We often want to search for interactions between randomly assigned treatments and a rich set of background attributes that are collected in an experiment. In other words, we want to investigate treatment effects that potentially vary across covariates but are invariant within strata defined by covariate values.

Investigating such heterogeneous treatment effects (when, why, and for whom a treatment has an effect), however, often becomes an ad hoc and unstructured exercise.

Subgroup analysis is the standard approach, where you estimate average treatment effects among subgroups defined by baseline covariates (CATEs; e.g., separately for Democrats, Independents, and Republicans).

Subgroup analysis may introduce some potential concerns:

* Unfeasible when the number of covariates is large, as must rely on strong modeling assumptions (i.e., potential for bias due to misspecification of parametric models)
** Parametric methods rest on functional form assumptions about outcome-covariates relationships
** E.g., the logistic regression assumes that the log-odds (the logarithm of the odds) of the outcome is a linear function of the predictors
* Need to account for multiple testing
* Credibility of CATE estimates diminishes when choice of covaraites is arbitrary


### Analyzing treatment effect heterogeneity with BART
Bayesian Additive Regression Trees (BART) is a non-parametric machine learning method that provides a flexible approach for estimating heterogeneous treatment effects. It can be particularly useful in scenarios where the relationship between covariates and the outcome is complex and non-linear. It is similarly useful when dealing with a large number of covariates.

In addition, compared to other machine learning techniques, BART is less sensitive to the choice of tuning parameters - thus reducing researchers’ discretion in the analysis - and relatively easy to implement.

BART builds on classification tree models and regression.
Intuitively, you can think of a classification tree as a decision tree operating like a flowchart that, in each step, interrogate the data to sort them in ever smaller pairs of groups. The tree ends when the data cannot be sorted any further based on the features of interest.
Subsequently, for each terminal node, a fitted value is determined by averaging the outcomes of the observations that fall into that node.

BART uses many small decision trees. Each tree makes a small contribution to the overall prediction, with the overall model being a combination of these trees. The ensemble of trees generates flexible fits since each of its sub-trees specializes in fitting a single small component of the data.

A prior distribution is placed on the tree structures and the parameters. This prior helps regularize the model, ensuring that no single tree dominates the prediction. Using the observed data, BART updates those priors to obtain the posterior distribution of the trees and their parameters. The prediction is then generated by averaging the predictions from all trees, weighted by their posterior probabilities.

### An example based on simulated data
First we simulate data assuming a binary treatment and a set of 5 covariates (X1 - X5)
```{r}
set.seed(123)
n <- 1000 # Number of observations
p <- 5 # Number of covariates

# Covariates
X <- matrix(rnorm(n * p), n, p) # Creates a matrix where we randomly assign values to the covariates
colnames(X) <- paste0("X", 1:p)

head(X) # Shows the random values assigned to the covariates
```

```{r}
## With simulated data, we want to generate a random assignment of the treatment
# Treatment assignment
Z <- rbinom(n, 1, 0.5)

# Heterogeneous treatment effect
tau <- 0.5 + 0.5 * X[, 1] - 0.01 * X[, 2] # Treatment effect varies by covariates X1 and X2

# Generate outcomes (%*% = matrix multiplication)
Y <- 1 + X %*% rnorm(p) + Z * tau + rnorm(n)

# Generate dataset
data <- data.frame(Y, Z, X) # Where Z is the treatment assignment and Y is the outcome
head(data)
```

Next we fit a BART model to estimate the treatment effect for each individual.
```{r}
# We need to provide training data, which
# are used to fit the BART model, i.e. estimating the tree structures and
# parameters that best explain the relationship between the covariates
# and the outcome
x_train <- data %>% select(-Y) # Matrix of covariates for training
y_train <- data$Y # Vector of outcome for training

# The test data is used to make predictions and validate the model
# Test data helps in estimating counterfactual outcomes
# (what would have happened without the treatment).
x_test <- x_train %>%
  mutate(Z = ifelse(Z == 1, 0, 1))
```

We have created counterfactual data by flipping the treatment status for each observation.
This allows us to predict what the outcome would be under the alternative treatment condition.

```{r}
# Fit BART model
# You can use bart() or wbart() - the latter has the advantage of
# simultaneously fit the model on the training data
# and obtain counterfactual predictions on the test data
# in a single function call
bart_results <- wbart(
  x.train = x_train,
  y.train = y_train, # Train the dataset
  x.test = x_test, # Test the dataset

  nskip = 1000, # Number of burn-in iterations; these are not used to estimate
  # (initial iterations to discard)

  ndpost = 2000 # Number of posterior draws
  # (iterations to keep for inference)
)
```

We can then extract the treatment effects and summarize the results.

```{r}
# Recover estimates, based on treatment condition
y_1 <- ifelse(x_train$Z == 1, bart_results$yhat.train.mean, bart_results$yhat.test.mean) # Estimated outcome under treatment
y_0 <- ifelse(x_train$Z == 0, bart_results$yhat.train.mean, bart_results$yhat.test.mean)

# Create data.frame to show ITEs (Individual Treatment Effects) and all covariates
# Estimated treatment effects as differences between predicted outcomes
# with and without treatment
ite_results <- y_1 - y_0

# Summary statistics of the individual treatment effects
summary(ite_results)

# Histogram of estimated treatment effects
hist(ite_results, breaks = 30, main = "Distribution of Estimated Treatment Effects", xlab = "Treatment Effect")
```

Finally, we can plot the estimated treatment effects against the covariates to examine heterogeneity.

```{r}
# We are interested in the relationship between
# the treatment effect and the covariates
# Add estimated treatment effects to data
data$ite <- ite_results

# Plot estimated treatment effects against covariate X1
ggplot(data, aes(x = X1, y = ite)) +
  geom_point(alpha = 0.5) +
  labs(title = "Estimated Treatment Effects by X1", x = "X1", y = "ITE") +
  theme_minimal()
```

The ITE varies systematically with X1, suggesting that the treatment effect is moderated by this covariate.

```{r}
# Plot estimated treatment effects against covariate X3
ggplot(data, aes(x = X3, y = ite)) +
  geom_point(alpha = 0.5) +
  labs(title = "Estimated Treatment Effects by X3", x = "X3", y = "ITE") +
  theme_minimal()
```


### Notes on interpretation
BART does not provide traditional coefficients like linear models. Instead, it gives an overall fit and individual predictions, allowing for the assessment of treatment effects at an individual level.

Estimated treatment effects are the differences between the predicted outcomes with and without the treatment for each individual. The summary statistics and histogram help understand the distribution of these effects across the population.

The scatter plot of ITE against a covariate (e.g., X1) helps identify potential moderators: a clear trend in the scatter plot suggests that the covariate may moderate the treatment effect. A horizontal line indicates no interaction, while a sloped line suggests an interaction effect.

The histogram of ITE shows the distribution of the treatment effects across all individuals. A wide distribution indicates significant heterogeneity, while a narrow distribution suggests more homogeneous treatment effects.

To statistically assess whether the heterogeneity in treatment effects indicated by BART is significant, we can use hypothesis testing. One approach is to fit a model with interaction terms between treatment and the covariates which have been highlighted by BART.

```{r}
# Fit a linear model with interaction terms
lm_fit <- lm(Y ~ Z * X1, data = data)

# Summary of the linear model
summary(lm_fit)
```


### An application
Let's repeat the steps illustrated above on the data of Duch et al. (2023).

```{r}
# Preliminaries
# Vaccine Intention
table(final_finalV2$vaccine_intention)
sum(is.na(final_finalV2$vaccine_intention))

# Vaccine Intention
table(final_finalV2$vaccine_reported_combo)
sum(is.na(final_finalV2$vaccine_reported_combo))

# Remove CDC
work <- final_finalV2[-which(final_finalV2$cash == "CDC"), ]
dim(work)
work$treat <- ifelse(work$cash == "cash", 1, 0)

is.factor(work$individual_treatment)
work$Gender <- as.factor(work$Gender)
work$Gender45y <- as.factor(work$Gender45y)


work2 <- work %>%
  select(
    vaccine_reported_combo,
    Gender,
    Age,
    WhatsApp,
    Education,
    Income,
    treat,
    SNMetric
  ) %>%
  as.data.frame(.)

work2 <- na.omit(work2)
dim(work2)

work2$WhatsApp <- as.factor(work2$WhatsApp)
work2$treat <- as.factor(work2$treat)

# Recover observed outcomes
y_train1 <- (work2$vaccine_reported_combo) # Generates a training dataset

# Recover explanatory variables for training (in sample) data
x_train1 <- work2 %>%
  select(
    Gender,
    Age,
    WhatsApp,
    Education,
    Income,
    treat,
    SNMetric
  ) %>%
  as.data.frame(.)

# Counterfactual data with inverted treatment assignment
x_test1 <- x_train1 %>%
  mutate(treat = ifelse(treat == 1, 0, 1)) %>%
  as.data.frame(.)

x_test1$treat <- as.factor(x_test1$treat)
```


```{r}
# set random seed:
set.seed(89)

# Fit BART model

# Train continuous prediction model and cast onto counterfactual data
# In other words, run BART for predicted values of observed and synthetic observations

# Train continuous prediction model and cast onto counterfactual data
bart_results <- wbart(
  x.train = x_train1,
  y.train = y_train1,
  x.test = x_test1
)

# Recover estimates, based on treatment condition
# Which estimates are we recovering?
y_1 <- ifelse(x_train1$treat == "1", bart_results$yhat.train.mean, bart_results$yhat.test.mean)
y_0 <- ifelse(x_train1$treat == "0", bart_results$yhat.train.mean, bart_results$yhat.test.mean)

# Create data.frame to show ITEs (Indivudal Treatment Effects) and all covariates
ite_results <- x_train1 %>%
  mutate(ite = y_1 - y_0)
```

Using the predictions generated by the BART model, we now have each individual’s two potential outcomes (and their difference) - one under treatment and one under control.

Let's briefly review how we arrived here:

* When you fitting the BART model with wbart(), we provide it with two sets of covariates
** Training data (x.train) - containing the individuals’ observed treatment assignments
** Test data (x.test) - created by “flipping” the treatment assignment for each individual
* The model returns:
** `bart_results$yhat.train.mean` - average predicted outcomes for each individual based on their actual (observed) treatment
** `bart_results$yhat.test.mean` - average predicted outcomes for each individual had their treatment been flipped (i.e. the counterfactual outcomes).

For each individual we can then compute the ITE as `y_1 - y_0`.

Finally, we can plot the estimated treatment effects against the covariates to examine heterogeneity.
```{r}
# Plot results
# Create individual and combined plots to visualize how ITEs vary with different covariates.

# To generate the ITEs plot, the data are placed in ascending order based on ITEs values
plot_data <- ite_results[order(ite_results$ite), ]
plot_data$id <- 1:nrow(plot_data)


summary(plot_data$Income) # cuts on 75, 150, 225
plot_data$Income <- ifelse(plot_data$Income < 75, "Below 75",
  ifelse(plot_data$Income >= 75 & plot_data$Income < 150, "75-150",
    ifelse(plot_data$Income >= 150 & plot_data$Income < 225, "150-225",
      ifelse(plot_data$Income >= 225, "Above 225", NA)
    )
  )
)

summary(plot_data$Age) # 30, 50 , 70
plot_data$Age <- ifelse(plot_data$Age < 30, "Below 30",
  ifelse(plot_data$Age >= 30 & plot_data$Age < 50, "30-50",
    ifelse(plot_data$Age >= 50 & plot_data$Age < 70, "50-70",
      ifelse(plot_data$Age >= 70, "Above 70", NA)
    )
  )
)




itePlot <- ggplot(plot_data, aes(x = id, y = ite)) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  geom_hline(yintercept = mean(plot_data$ite), color = "blue") +
  labs(x = "Individual", y = "ITE") +
  theme_minimal() +
  scale_x_continuous(limits = c(0, nrow(plot_data)))

itePlot


covarPlot1 <- ggplot(plot_data, aes(x = id, fill = Gender)) + # Change fill to covar of interest
  geom_histogram(binwidth = 60, position = "stack") +
  theme(legend.position = "bottom") +
  scale_fill_discrete(name = "Gender") + # Change this to rename legend
  labs(y = "Count", x = "Individual") +
  scale_x_continuous(limits = c(0, nrow(plot_data)))
covarPlot1

covarPlot2 <- ggplot(plot_data, aes(x = id, fill = WhatsApp)) + # Change fill to covar of interest
  geom_histogram(binwidth = 60, position = "stack") +
  theme(legend.position = "bottom") +
  scale_fill_discrete(name = "Whatsapp") + # Change this to rename legend
  labs(y = "Count", x = "Individual") +
  scale_x_continuous(limits = c(0, nrow(plot_data)))
covarPlot2

covarPlot3 <- ggplot(plot_data, aes(x = id, fill = Education)) + # Change fill to covar of interest
  geom_histogram(binwidth = 60, position = "stack") +
  theme(legend.position = "bottom") +
  scale_fill_discrete(name = "Education") + # Change this to rename legend
  labs(y = "Count", x = "Individual") +
  scale_x_continuous(limits = c(0, nrow(plot_data)))
covarPlot3

# for income we might need some categories:
covarPlot4 <- ggplot(plot_data, aes(x = id, fill = Income)) + # Change fill to covar of interest
  geom_histogram(binwidth = 60, position = "stack") +
  theme(legend.position = "bottom") +
  scale_fill_discrete(name = "Food Spent") + # Change this to rename legend
  labs(y = "Count", x = "Individual") +
  scale_x_continuous(limits = c(0, nrow(plot_data)))
covarPlot4


# Age with some categories
covarPlot5 <- ggplot(plot_data, aes(x = id, fill = Age)) + # Change fill to covar of interest
  geom_histogram(binwidth = 60, position = "stack") +
  theme(legend.position = "bottom") +
  scale_fill_discrete(name = "Age") + # Change this to rename legend
  labs(y = "Count", x = "Individual") +
  scale_x_continuous(limits = c(0, nrow(plot_data)))
covarPlot5

# Two graphs into one chart
# Combine all plots into one chart
het_plot1 <- ggarrange(itePlot, covarPlot1,
  ncol = 1, nrow = 2, heights = c(2, 2)
)
het_plot1
het_plot2 <- ggarrange(itePlot, covarPlot2,
  ncol = 1, nrow = 2, heights = c(2, 2)
)
het_plot2
het_plot3 <- ggarrange(itePlot, covarPlot3,
  ncol = 1, nrow = 2, heights = c(2, 2)
)
het_plot3

het_plot4 <- ggarrange(itePlot, covarPlot4,
  ncol = 1, nrow = 2, heights = c(2, 2)
)
het_plot4

het_plot5 <- ggarrange(itePlot, covarPlot5,
  ncol = 1, nrow = 2, heights = c(2, 2)
)
het_plot5


het_plot6 <- ggarrange(itePlot, covarPlot1, covarPlot2, covarPlot3,
  ncol = 1, nrow = 4, heights = c(2, 2, 2, 2)
)
het_plot6

het_plot7 <- ggarrange(itePlot,
  covarPlot4, covarPlot5,
  ncol = 1, nrow = 3, heights = c(2, 2, 2)
)
het_plot7
```

### Notes on interpretation

ITE plot and covariate plots become meaningful when visualized together.
The ITE plot display ITEs for each unit in ascending order. The blue line indicate the ATE.
The covariate plot displays the data in the same order, showing the share of individuals in each covariate level for each ITE value.
If we are not seeing trends in the heatmaps then we are not seeing moderation effects.