{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antndlcrx/Intro-to-LLMs-DPIR/blob/main/llm_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GlQculfR2bl"
      },
      "source": [
        "<img src=\"https://cdn.githubraw.com/antndlcrx/Intro-to-Python-DPIR/main/images/logo_dpir.png?raw=true:,  width=35\" alt=\"My Image\" width=175>\n",
        "\n",
        "# üèõÔ∏è **LLM Fundamentals**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tDc9vibwVK6"
      },
      "source": [
        "This is a key session in which you will learn what language models are, how they are build, and how they generate text sequences. When you are interacting with, say, a chat-bot, there are multiple fascinating concepts and ideas at play (what is a meaning of a word? what is a meaning of a sentence? how do we encode them?), as well as smart and elegant (and at times redundant) algorithms (tokenization, attention, backpropagation, etc.) that bring these concepts to life.\n",
        "\n",
        "Their particular implementations determine how well a model perfoms, both as a general model of language and as a tool for solving your task. For instance, choosing appropriate way to tokenize text can make or break model's ability to write code. Therefore, having a good understanding of each of the building blocks of an LLM is of great importance.\n",
        "\n",
        "üóìÔ∏è This session introduces:\n",
        "- What does it mean to build a model of a language?  \n",
        "- What do we practically need to do to build a language model?\n",
        "- How to process human readable text input into machine readable input and back?\n",
        "- Model architecture (transformer and attention)\n",
        "- How does a language model generate text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHMrcCLO4A-_"
      },
      "source": [
        "## **1**.&nbsp; **What is a Language Model?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA1SWJuqxZRt"
      },
      "source": [
        "Before we jump to technical details of language model implementation, let us stop and think what a language model is in general?\n",
        "\n",
        "- What does it mean to model a language?\n",
        "- Why would a language model be useful?\n",
        "- What makes a good model?\n",
        "\n",
        "Think about these questions, note your answers. Once you have them, I invite you to read the [chapter on language modelling](https://lena-voita.github.io/nlp_course/language_modeling.html) from the amazing [NLP Course for You](https://lena-voita.github.io/nlp_course.html) course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWM1ROXn4FWM"
      },
      "source": [
        "### 1.1 üîÆ **Language Modeling**\n",
        "\n",
        "To build a language model, we come up with an idea of an algorithm that would translate our intuition from the previous exercise into a series of executable steps. This would be language modelling<a href=\"#footnote1\"><sup>1</sup></a>.\n",
        "\n",
        "Language modeling is a fundamental task in natural language processing (NLP), where the goal is to predict the next word in a sequence based on the preceding words. In other words, a language model aims to estimate the probability distribution of word sequences in a given language. Training a language model involves maximizing the likelihood that the model assigns to actual word sequences observed in the training data.\n",
        "\n",
        "```perl\n",
        "Predict next word based on previous context\n",
        "\n",
        "Given a word sequence:\n",
        "      w‚ÇÅ ‚Üí w‚ÇÇ ‚Üí w‚ÇÉ ‚Üí ... ‚Üí w‚Çô‚Çã‚ÇÅ ‚Üí w‚Çô\n",
        "       ‚îÇ    ‚îÇ    ‚îÇ            ‚îÇ\n",
        "       ‚ñº    ‚ñº    ‚ñº            ‚ñº\n",
        "Predict:   Predict:   Predict:          Predict:\n",
        "   w‚ÇÇ         w‚ÇÉ         w‚ÇÑ                 w‚Çô\n",
        "given:      given:      given:            given:\n",
        "  w‚ÇÅ       w‚ÇÅ,w‚ÇÇ      w‚ÇÅ,w‚ÇÇ,w‚ÇÉ        w‚ÇÅ,w‚ÇÇ,...,w‚Çô‚Çã‚ÇÅ\n",
        "```\n",
        "\n",
        "**Mathematical Description**\n",
        "\n",
        "Formally, given a sequence of words $(w_1, w_2, \\dots, w_N)$, the language modeling objective is to maximize the joint probability of the entire sequence. Using the chain rule of probability, this joint probability can be decomposed into a product of conditional probabilities:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_N) = P(w_1) \\cdot P(w_2 \\mid w_1) \\cdot P(w_3 \\mid w_1, w_2) \\dots P(w_N \\mid w_1, w_2, \\dots, w_{N-1})\n",
        "$$\n",
        "\n",
        "In practice, a language model predicts each word $w_n$ based solely on the preceding words, thus learning conditional probabilities:\n",
        "\n",
        "$$\n",
        "P(w_n \\mid w_1, w_2, \\dots, w_{n-1})\n",
        "$$\n",
        "\n",
        "---\n",
        "<p id=\"footnote1\"><sup>1</sup> There are several approaches to language modelling: traditional count-based ones and neural-network-based ones. Among the latter, there is <em>autoregressive language modelling</em> and <em>masked language modelling</em>. For the majority of this course, we will be working with autoregressive language models (you may know them as generative language models). For that reason, we will focus on autoregressive language modelling.</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlG9prfj0X6i"
      },
      "source": [
        "### 1.2. üìâ**Finding a Good Model: Negative Log Likelihood**\n",
        "To effectively train language models, the objective is typically framed as minimizing the **negative log-likelihood** of these probabilities over the training corpus:\n",
        "\n",
        "$$\n",
        "\\text{Minimize: } -\\log P(w_1, w_2, \\dots, w_N) = -\\sum_{n=1}^{N} \\log P(w_n \\mid w_1, w_2, \\dots, w_{n-1})\n",
        "$$\n",
        "\n",
        "This is exactly your maximul likelihood from stats!\n",
        "\n",
        "- **Likelihood**: how probable is our data given the current model. We want our language model to assign high probabilities to the actual words seen during training.\n",
        "- **Why Log?**: Probabilities multiply quickly, becoming very small numbers. **Taking a logarithm transforms these multiplications into sums, which are numerically stable and computationally convenient**. Instead of multiplying many small probabilities, we add their logarithms.\n",
        "- **Why Negative?**: Our goal is to maximize likelihood, but mathematically it is more convenient to frame optimization problems as minimization. Thus, we minimize the negative of the log likelihood. **Minimizing the negative log likelihood is equivalent to maximizing the likelihood**.\n",
        "\n",
        "This negative log-likelihood measure is computationally convenient and helps the model learn meaningful linguistic patterns by penalizing low-probability predictions.\n",
        "\n",
        "```perl\n",
        "Minimize negative log-likelihood:\n",
        "  \n",
        "‚àí log [P(w‚ÇÅ, w‚ÇÇ, ..., w‚Çô)]\n",
        "           ‚îÇ\n",
        "           ‚ñº\n",
        "= ‚àí [log P(w‚ÇÅ) + log P(w‚ÇÇ|w‚ÇÅ) + log P(w‚ÇÉ|w‚ÇÅ,w‚ÇÇ) + ... + log P(w‚Çô|w‚ÇÅ,...,w‚Çô‚Çã‚ÇÅ)]\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPum6HZ0exKk"
      },
      "source": [
        "### 1.2. ü§î **Language Model Evaluation: Perplexity**\n",
        "**Perplexity** is a measure used to evaluate how well a language model predicts unseen text. Intuitively, it answers the question: \"How many equally likely words is my model choosing between?\"\n",
        "\n",
        "Formally, perplexity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{Perplexity} = e^{-\\frac{1}{N}\\sum_{n=1}^{N}\\log P(w_n|w_1,\\dots,w_{n-1})}\n",
        "$$\n",
        "\n",
        "We initially computed the **average negative log likelihood** (or cross-entropy). Taking the exponential **transforms the log-scale back to a normal scale**, giving us a measure that's intuitive to interpret as an effective \"branching factor.\"\n",
        "\n",
        "- **Lower perplexity** ‚Üí the model is confident and accurate, fewer \"choices\" per step.\n",
        "- **Higher perplexity** ‚Üí the model is uncertain, predicting many possible next words.\n",
        "\n",
        "üß† Quick Intuitive Example:\n",
        "- A perplexity of 1000 means the model is roughly guessing among 1000 possible words for every prediction ‚Äî a poor model.\n",
        "\n",
        "- A perplexity of 10 means the model consistently narrows down to about 10 possible words ‚Äî a much better model.\n",
        "\n",
        "> üìñ For more:\n",
        "- [Language Modelling NLP Course for You](https://lena-voita.github.io/nlp_course/language_modeling.html)\n",
        "- [Perplexity of fixed-length models\n",
        "by ü§ó](https://huggingface.co/docs/transformers/en/perplexity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrL78bFpTqN3"
      },
      "source": [
        "## **2**.&nbsp; üçì **Processing Text: Tokenisation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K59EBJm_lpYx"
      },
      "source": [
        "### **Motivation**\n",
        "\n",
        "**Tokenization** is the critical step of **converting human-readable text into numerical representations that language models can process**.\n",
        "\n",
        "After defining language modeling as predicting words from context, the natural next question is: **how exactly do we represent words numerically**? This is precisely the role of tokenization.\n",
        "\n",
        "Tokenization serves several key purposes:\n",
        "\n",
        "1. **Reduction of Vocabulary Size**:\n",
        "Natural languages are vast, filled with misspellings, colloquialisms, technical jargon, and new words constantly emerging. Tokenization condenses this enormous diversity into a fixed, manageable vocabulary, making computations feasible.\n",
        "\n",
        "2. **Efficient Computation**:\n",
        "Transforming words (or subwords) into numeric indices lets models efficiently perform mathematical operations required by neural networks.\n",
        "\n",
        "3. **Meaningful Representation**:\n",
        "Tokenization methods, especially subword approaches (like Byte-Pair Encoding or WordPiece), effectively handle semantic similarities. They break words down into meaningful parts, allowing models to generalize across related terms or word forms, even if the model hasn‚Äôt explicitly encountered them during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJsFQgn7WcP-"
      },
      "source": [
        "### **Lets Build a Tokenizer!**\n",
        "\n",
        "Language models do not work on raw text. Insead, they operate on tokens, which are integer numbers denoting a unique identity (id) for every unique token in our vocabulary.\n",
        "\n",
        "In this part of the tutorial we cover the functionality of a tokenizer.\n",
        "- Tokenizer helps create vocabulary: a registry where every unique token we encounter is mapped to a unique token_id.\n",
        "- Tokenizer uses this vocabulary to convert any sequence of tokens to token ids, which we then pass to a language model (they are the model's expected input!).\n",
        "- Tokenizer also performs the reverse mapping: from token ids back to human readable words. This helps us convert language model outputs - sequences of token ids - back to human readable text!\n",
        "\n",
        "To build a tokenizer, we first download training data, which are Jane Ostein novels. We then clean them up to remove unnecessary information and symbols, leaving out just chapter headings and chapter content. Then, we develop a `SimpleTokenizer`, which is a class (blueprint) for creating tokenizers from any text corpus that we want! Each of those would be a different tokenizer (depending on text data we feed to it), but will perform exactly the same functionality!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmg3RY6Pvbm_",
        "outputId": "766c7581-a90f-49b6-dd89-e9b18950a692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading pride_and_prejudice.txt...\n",
            "Downloading sense_and_sensibility.txt...\n",
            "Downloading mansfield_park.txt...\n"
          ]
        }
      ],
      "source": [
        "#@title Download Data for this Session\n",
        "\n",
        "import requests\n",
        "\n",
        "GUTENBERG_URLS = {\n",
        "    \"pride_and_prejudice.txt\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"sense_and_sensibility.txt\": \"https://www.gutenberg.org/files/161/161-0.txt\",\n",
        "    \"mansfield_park.txt\": \"https://www.gutenberg.org/files/141/141-0.txt\"\n",
        "}\n",
        "\n",
        "DELIMITER = \"\\n<|endoftext|>\\n\\n\"\n",
        "COMBINED_FILENAME = \"austen_combined.txt\"\n",
        "\n",
        "def download_file(filename, url):\n",
        "    \"\"\"Download a file and save it locally.\"\"\"\n",
        "    print(f\"Downloading {filename}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "def combine_files(file_list, output_file, delimiter):\n",
        "    \"\"\"Combine a list of files into one, separated by a delimiter.\"\"\"\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "        for fname in file_list:\n",
        "            with open(fname, \"r\", encoding=\"utf-8\") as infile:\n",
        "                text = infile.read().strip()\n",
        "                outfile.write(text + delimiter)\n",
        "\n",
        "\n",
        "for fname, url in GUTENBERG_URLS.items():\n",
        "    download_file(fname, url)\n",
        "combine_files(GUTENBERG_URLS.keys(), COMBINED_FILENAME, DELIMITER)\n",
        "\n",
        "\n",
        "# with open(\"austen_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#     raw_text = f.read()\n",
        "\n",
        "# len(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz1D2qvgANBO",
        "outputId": "b2e90486-cf69-4d82-d819-10b2c54f71ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2302675"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load data\n",
        "with open(\"austen_combined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "len(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "1iPI44wHDyXk"
      },
      "outputs": [],
      "source": [
        "#@title Clean Up the Raw Text\n",
        "import re\n",
        "\n",
        "def clean_gutenberg_text(text):\n",
        "    \"\"\"\n",
        "    Cleans Gutenberg text by removing header, footer, and metadata.\n",
        "    \"\"\"\n",
        "    # Remove header\n",
        "    text = re.split(r\"\\*\\*\\* START OF (THE|THIS) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", text, flags=re.IGNORECASE)[-1]\n",
        "\n",
        "    # Remove footer\n",
        "    text = re.split(r\"\\*\\*\\* END OF (THE|THIS) PROJECT GUTENBERG EBOOK .* \\*\\*\\*\", text, flags=re.IGNORECASE)[0]\n",
        "\n",
        "    # Remove illustration tags and bracketed contents\n",
        "    text = re.sub(r\"\\[Illustration.*?\\]\", \"\", text, flags=re.DOTALL)\n",
        "\n",
        "    # Remove \"Contents\" and chapter listings (ToC)\n",
        "    text = re.split(r\"Contents\\n\\n\", text, flags=re.IGNORECASE)\n",
        "    if len(text) > 1:\n",
        "        text = re.split(r\"\\n{2,}(CHAPTER\\s+I\\b)\", text[1], flags=re.IGNORECASE)\n",
        "        text = \"\".join(text[-2:]) if len(text) >= 2 else text[-1]\n",
        "    else:\n",
        "        text = text[0]\n",
        "\n",
        "    # Remove excessive newlines and whitespace\n",
        "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text)\n",
        "\n",
        "    # Strip leading and trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_gutenberg_text(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "9kO6rvKoIjqu",
        "outputId": "4da78aad-0f45-4ff7-ac91-63f17388035c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'CHAPTER I\\n\\nAbout thirty years ago Miss Maria Ward, of Huntingdon, with only seven\\nthousand pounds, had the good luck to captivate Sir Thomas Bertram, of\\nMansfield Park, in the county of Northampton, and to be thereby raised\\nto the rank of a baronet‚Äôs lady, with all the comforts and consequences\\nof a'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text[:300]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pGPE4QvdVCq8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0;31mSignature:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m/\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDocstring:\u001b[0m\n",
            "Return a list of the words in the string, using sep as the delimiter string.\n",
            "\n",
            "sep\n",
            "  The delimiter according which to split the string.\n",
            "  None (the default value) means split according to any whitespace,\n",
            "  and discard empty strings from the result.\n",
            "maxsplit\n",
            "  Maximum number of splits to do.\n",
            "  -1 (the default value) means no limit.\n",
            "\u001b[0;31mType:\u001b[0m      method_descriptor"
          ]
        }
      ],
      "source": [
        "str.split?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fc8-Z31i1otK"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2669089182.py, line 30)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.word_set = # split the raw text on whitespace; collect and sort unique words\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise: Build a Tokenizer!\n",
        "\n",
        "# Hint 1: try out the .split() and .join() str methods. Use \"str.split?\" syntax to read documentation if needed.\n",
        "# Hint 2: utilisee python dictionary methods like .get(), .items() to work with dictionaries.\n",
        "\n",
        "class SimpleTokenizer():\n",
        "    \"\"\"\n",
        "    A simple word-level tokenizer for converting text into integer token IDs and back.\n",
        "\n",
        "    This tokenizer:\n",
        "    - Builds a vocabulary from training text by splitting on whitespace.\n",
        "    - Encodes input text as a list of token IDs based on the vocabulary.\n",
        "    - Decodes token ID sequences back into human-readable text.\n",
        "    - Includes a special \"<unk>\" token for unknown words not seen during training.\n",
        "\n",
        "    Attributes:\n",
        "        word_set (list): Sorted list of unique words from the training text.\n",
        "        vocab (dict): Maps words to integer token IDs.\n",
        "        inverse_vocab (dict): Maps token IDs back to words.\n",
        "    \"\"\"\n",
        "\n",
        "    # TASK 1: DEFINE CLASS ATTRIBUTES\n",
        "    def __init__(self, train_text):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer with a vocabulary built from the training text.\n",
        "\n",
        "        Args:\n",
        "            train_text (str): A raw string of training text.\n",
        "        \"\"\"\n",
        "        self.word_set = # split the raw text on whitespace; collect and sort unique words\n",
        "        self.vocab =  # create a mapping from word to token ID\n",
        "        self.vocab[\"<unk>\"] = # assign a special token ID for unknown words\n",
        "        self.inverse_vocab =  # create a reverse mapping from ID to word\n",
        "\n",
        "    # TASK 2: DEFINE ENCODE METHOD\n",
        "    def encode(self, text:str):\n",
        "        \"\"\"\n",
        "        Convert a space-separated string into a list of token IDs.\n",
        "        Each word is looked up in the vocabulary; unknown words are mapped to the '<unk>' token ID.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input string to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            list[int]: A list of token IDs.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        return\n",
        "\n",
        "    # TASK 3: DEFINE DECODE METHOD\n",
        "    def decode(self, token_ids: list[int]):\n",
        "        \"\"\"\n",
        "        Convert a list of token IDs back into a space-separated string.\n",
        "        Each token ID is mapped back to its corresponding word using the inverse vocabulary.\n",
        "\n",
        "        Args:\n",
        "            token_ids (list[int]): A sequence of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: A decoded string formed by joining the tokens with spaces.\n",
        "        \"\"\"\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fsxt6iTzmxV4",
        "outputId": "be17b9e0-50e7-4b85-ba50-7772f4fd4e4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded sequence: [202, 531, 48, 14232, 15730, 1957, 694, 663, 1133, 10216, 529, 15542, 10303, 12783, 14269, 11083, 7324, 14131, 7103, 9231, 14352, 3359, 951, 1055, 150, 10216, 653, 797, 8110, 14131, 4393, 10216, 749, 2170, 14352, 2772, 14180, 11626, 14352, 14131, 11638, 10216, 1550, 2758, 8789, 15542, 2028, 14131, 3828, 2170, 4152, 10216, 2167, 7377, 7832, 2170, 8828, 8146, 73, 528, 6068, 10278, 14131, 7225, 10216, 14131, 9356, 2170, 7605, 14686, 14131, 8888, 7694, 2057, 7605, 14352, 2772, 2531, 8916, 14280, 14269, 11082, 12885, 10216, 2248, 5889, 3677, 14352, 8578, 942, 7324, 14634, 13033, 14352, 2772, 2926, 3282, 7605, 5668, 2170, 13743, 10216, 14139, 1691, 2440, 14252, 694, 1132, 2170, 694, 417, 11608, 2440, 7377, 2440, 694, 665, 4986, 10038, 12561, 14352, 11149, 14139, 9347, 15542, 2074, 5879, 1832, 190, 14171, 3466, 2373, 10038, 13150, 9315, 9482, 10216, 8828, 6752, 8110, 14131, 15643, 2440, 14171, 2373, 11229, 15585, 14352, 4862, 14148, 694, 1133, 2531, 14131, 5743, 10216, 7332, 1550, 5422, 15732, 6773, 7643, 10143, 14352, 2772, 2539, 14352, 14131, 873, 703, 737, 1550, 6815, 10216, 7605, 3210, 15542] \n",
            "\n",
            " Decoded sequence CHAPTER I About thirty years ago Miss Maria Ward, of Huntingdon, with only seven thousand pounds, had the good luck to captivate Sir Thomas Bertram, of Mansfield Park, in the county of Northampton, and to be thereby raised to the rank of a baronet‚Äôs lady, with all the comforts and consequences of an handsome house and large income. All Huntingdon exclaimed on the greatness of the match, and her uncle, the lawyer, himself, allowed her to be at least three thousand pounds short of any equitable claim to it. She had two sisters to be benefited by her elevation; and such of their acquaintance as thought Miss Ward and Miss Frances quite as handsome as Miss Maria, did not scruple to predict their marrying with almost equal advantage. But there certainly are not so many men of large fortune in the world as there are pretty women to deserve them. Miss Ward, at the end of half a dozen years, found herself obliged to be attached to the Rev. Mr. Norris, a friend of her brother-in-law, with\n"
          ]
        }
      ],
      "source": [
        "# let us test the tokenizer we crated!\n",
        "\n",
        "tokenizer = SimpleTokenizer(cleaned_text) # creates an instance of SimpleTokenizer, built from cleaned_text\n",
        "\n",
        "test = tokenizer.encode(cleaned_text[:995])\n",
        "test_decoded = tokenizer.decode(test)\n",
        "\n",
        "print(\"Encoded sequence:\", test, \"\\n\"*2, \"Decoded sequence\", test_decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "rUP5c_4DDm_i"
      },
      "outputs": [],
      "source": [
        "#@title Solution: Build a Tokenizer\n",
        "class SimpleTokenizer():\n",
        "    \"\"\"\n",
        "    A simple word-level tokenizer for converting text into integer token IDs and back.\n",
        "\n",
        "    This tokenizer:\n",
        "    - Builds a vocabulary from training text by splitting on whitespace.\n",
        "    - Encodes input text as a list of token IDs based on the vocabulary.\n",
        "    - Decodes token ID sequences back into human-readable text.\n",
        "    - Includes a special \"<unk>\" token for unknown words not seen during training.\n",
        "\n",
        "    Attributes:\n",
        "        word_set (list): Sorted list of unique words from the training text.\n",
        "        vocab (dict): Maps words to integer token IDs.\n",
        "        inverse_vocab (dict): Maps token IDs back to words.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, train_text):\n",
        "        \"\"\"\n",
        "        Initialize the tokenizer with a vocabulary built from the training text.\n",
        "\n",
        "        Args:\n",
        "            train_text (str): A raw string of training text.\n",
        "        \"\"\"\n",
        "        self.word_set = sorted(set(train_text.split()))  # split the raw text on whitespace; collect and sort unique words\n",
        "        self.vocab = {el:i for i, el in enumerate(self.word_set)}  # create a mapping from word to token ID\n",
        "        self.vocab[\"<unk>\"] = len(self.word_set) + 1  # assign a special token ID for unknown words\n",
        "        self.inverse_vocab = {i:el for el, i in self.vocab.items()}  # create a reverse mapping from ID to word\n",
        "\n",
        "    def encode(self, text:str):\n",
        "        \"\"\"\n",
        "        Convert a space-separated string into a list of token IDs.\n",
        "        Each word is looked up in the vocabulary; unknown words are mapped to the '<unk>' token ID.\n",
        "\n",
        "        Args:\n",
        "            text (str): The input string to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            list[int]: A list of token IDs.\n",
        "        \"\"\"\n",
        "        # for every word in the input string, get its token ID (or use <unk> if not found)\n",
        "        token_ids = [self.vocab.get(x, self.vocab[\"<unk>\"]) for x in text.split()]\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: list[int]):\n",
        "        \"\"\"\n",
        "        Convert a list of token IDs back into a space-separated string.\n",
        "        Each token ID is mapped back to its corresponding word using the inverse vocabulary.\n",
        "\n",
        "        Args:\n",
        "            token_ids (list[int]): A sequence of token IDs to decode.\n",
        "\n",
        "        Returns:\n",
        "            str: A decoded string formed by joining the tokens with spaces.\n",
        "        \"\"\"\n",
        "        # for every token ID in the list, get its word representation from the inverse vocab\n",
        "        words = [self.inverse_vocab[x] for x in token_ids]\n",
        "        return \" \".join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueTctjeoqhZI",
        "outputId": "45801cc9-9ff2-4069-88d3-9fec76ad216a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded sequence: [531, 9038, 15200, 9812, 16182, 8110, 14131, 5964, 8110, 14131, 16182, 10568, 15409, 16182, 2373, 8656, 13150, 16182] \n",
            "\n",
            " Decoded sequence I like walking my <unk> in the evenings in the <unk> park where <unk> are just so <unk>\n"
          ]
        }
      ],
      "source": [
        "# try any other imaginable text input!\n",
        "test = tokenizer.encode(\"I like walking my dog in the evenings in the University park where sunsets are just so beautiful.\")\n",
        "test_decoded = tokenizer.decode(test)\n",
        "print(\"Encoded sequence:\", test, \"\\n\"*2, \"Decoded sequence\", test_decoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P17zLxshnVg"
      },
      "source": [
        "### üîó **Byte-Pair Encoding (BPE)**\n",
        "\n",
        "A common challenge in language modeling is dealing with words that weren't present in the training data. **Byte-Pair Encoding (BPE)**, introduced by [Sennrich et al. (2015)](https://arxiv.org/abs/1508.07909), elegantly solves this by breaking words into smaller, meaningful subword units. **The key idea is simple yet powerful: it iteratively merges the most frequent pairs of bytes or characters in the training corpus to build a flexible vocabulary**.\n",
        "\n",
        "By doing so, **BPE allows models to handle unseen or rare words effectively, dramatically improving their generalization**.\n",
        "\n",
        " > üìñ For an in-depth exploration of BPE, check out the [Hugging Face NLP Course (Chapter 6)](https://huggingface.co/learn/nlp-course/en/chapter6/5), or watch [Andrej Karpathy's \"Let's Build a GPT Tokenizer\" video](https://www.youtube.com/watch?v=zduSFxRajkE).\n",
        "\n",
        "\n",
        "> üìö Several libraries implement BPE:\n",
        "\n",
        "- [**Tiktoken** by OpenAI](https://github.com/openai/tiktoken)\n",
        "- [**SentencePiece** by Google](https://github.com/google/sentencepiece)\n",
        "\n",
        "üõ†Ô∏è Try exploring how tokenizers process text directly in the [Tiktokenizer app](https://tiktokenizer.vercel.app/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anxtsqqdgBAS",
        "outputId": "56397b6c-b8c8-415b-82a6-54d3a11ac128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# we need to install the library to get pretrained tokenizers\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d5R8ZHNTgDUH"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "# we select the tokenizer used to train the original gpt-2 model\n",
        "bpe_tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxI6pvjOMaVn",
        "outputId": "c42832bb-c330-4dac-bff7-09629cdbff14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bpe_tokenizer.n_vocab # n.vocab stores how many unique tokens we have in the tokenizers vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "JihDogMOhJVM",
        "outputId": "c752f9dd-5471-4f29-d2de-1a37d8fd6c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[40, 588, 6155, 616, 3290, 287, 262, 37119, 287, 262, 2059, 3952, 810, 4252, 28709, 389, 655, 523, 4950, 13]\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I like walking my dog in the evenings in the University park where sunsets are just so beautiful.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = bpe_tokenizer.encode(\"I like walking my dog in the evenings in the University park where sunsets are just so beautiful.\")\n",
        "print(tokens)\n",
        "bpe_tokenizer.decode(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow0jmpcErKEL"
      },
      "source": [
        "Now that we have a working tokenizer that can generalize well, we can proceed to prepare data for language model training. We need to convert it to sets of input target pairs for the model to predict, so that it can learn language!\n",
        "\n",
        "We would need üõ†Ô∏è to convert our data from Jane Austin books into this format.  `DataSet` and `DataLoader` from PyTorch are perfectly designed to do just that!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMKnoa91pOI7"
      },
      "source": [
        "## **Extra:üîßTorch DataLoader and DataSet**\n",
        "\n",
        "In PyTorch, a `Dataset` provides an **organized way of accessing and managing your data**, while a `DataLoader` handles *batching*, *shuffling*, and **efficiently loading data during training**. Specifically, a `Dataset` defines how individual data samples (inputs and labels) are accessed, while a `DataLoader` wraps around it to deliver batches seamlessly to your model. Together, they simplify data management, enhance training speed, and help ensure reproducible and robust training pipelines.\n",
        "\n",
        "- **Batch**: A batch is a group of samples processed together in a single forward and backward pass during training, which improves computational efficiency and stabilizes gradient updates.\n",
        "\n",
        "- **Shuffling**: Shuffling randomly reorders the dataset before each epoch to prevent the model from learning spurious patterns tied to the order of the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_haAkNHvOrcG"
      },
      "source": [
        "We use these üõ†Ô∏è to create a flow of input-target pairs of tokens to train a language model.\n",
        "\n",
        "Suppose we have a sequence of tokens:\n",
        "\n",
        "```ini\n",
        "token_ids = [t‚ÇÄ, t‚ÇÅ, t‚ÇÇ, t‚ÇÉ, ..., t‚Çô‚Çã‚ÇÇ, t‚Çô‚Çã‚ÇÅ, t‚Çô]\n",
        "```\n",
        "\n",
        "We construct training examples by defining a context lenghth (for example `context_len=4`) and a sliding window (`stride`=2) as follows:\n",
        "\n",
        "```less\n",
        "Iteration 1:\n",
        "    Input (X):   [t‚ÇÄ,   t‚ÇÅ,   t‚ÇÇ,   t‚ÇÉ]\n",
        "    Target (Y):  [t‚ÇÅ,   t‚ÇÇ,   t‚ÇÉ,   t‚ÇÑ]\n",
        "\n",
        "Iteration 2 (stride forward by 2):\n",
        "    Input (X):   [t‚ÇÇ,   t‚ÇÉ,   t‚ÇÑ,   t‚ÇÖ]\n",
        "    Target (Y):  [t‚ÇÉ,   t‚ÇÑ,   t‚ÇÖ,   t‚ÇÜ]\n",
        "\n",
        "Iteration 3:\n",
        "    Input (X):   [t‚ÇÑ,   t‚ÇÖ,   t‚ÇÜ,   t‚Çá]\n",
        "    Target (Y):  [t‚ÇÖ,   t‚ÇÜ,   t‚Çá,   t‚Çà]\n",
        "\n",
        "...\n",
        "```\n",
        "\n",
        "until no full sequences remain.\n",
        "\n",
        "The **context window** is the number of tokens an LLM considers simultaneously when predicting the next token. A **longer context window gives the model more information and improves its ability to capture meaningful relationships**, but at the cost of increased computational requirements.\n",
        "\n",
        "The **stride determines how much the context window moves forward between each training example**. A smaller stride creates more overlapping examples, increasing the amount of training data but also introducing redundancy. A larger stride reduces overlap and speeds up data preparation but can reduce the diversity of training examples. Choosing these parameters involves balancing model performance, computational efficiency, and the richness of training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RBy96RwXpS8G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-DlCVdGWkaH",
        "outputId": "e176b988-0b3f-4f5a-cbf7-0480bc3949be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "223694"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(bpe_tokenizer.encode(cleaned_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8UTHZrWtN9Ey"
      },
      "outputs": [],
      "source": [
        "#@title Create Dataset and DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset for next-token prediction in language modeling.\n",
        "\n",
        "    This dataset creates input-target pairs by sliding a window of `context_len`\n",
        "    over the tokenized text with a fixed `stride`.\n",
        "\n",
        "    Each sample consists of:\n",
        "    - X: A sequence of `context_len` token IDs (input).\n",
        "    - Y: The same sequence shifted one position to the right (target).\n",
        "\n",
        "    Args:\n",
        "        text (str): The raw training text.\n",
        "        tokenizer (Tokenizer): A tokenizer with an `encode()` method.\n",
        "        context_len (int): Length of each input sequence.\n",
        "        stride (int): Number of tokens to move the window by for the next sample.\n",
        "    \"\"\"\n",
        "\n",
        "    # we initialise any CustomDataset with text data (like a novel by Jane Austin), a tokenizer, desired context_len, and stride\n",
        "    def __init__(self, text, tokenizer, context_len, stride):\n",
        "        super().__init__()\n",
        "\n",
        "        # creating placeholders for inputs and outputs (both sequences of token ids)\n",
        "        self.Y = []\n",
        "        self.X = []\n",
        "\n",
        "        # Tokenize the entire input text\n",
        "        input_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # slide a window over the tokenized input to create input-target pairs\n",
        "        for i in range(0, len(input_ids) - context_len, stride):\n",
        "            xids = input_ids[i: i + context_len]  # input sequence\n",
        "            yids = input_ids[i + 1: i + 1 + context_len] # shifted target sequence\n",
        "\n",
        "            # as we iterate over dataset, we fill the placeholders\n",
        "            self.X.append(torch.tensor(xids))\n",
        "            self.Y.append(torch.tensor(yids))\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single input-target pair at index `idx`.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (input_tensor, target_tensor)\n",
        "        \"\"\"\n",
        "        return self.X[idx], self.Y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT6RTNlZ1Wfx",
        "outputId": "63b3d58a-290e-43a0-f2f1-3dda25135bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 3144; Val size: 350\n"
          ]
        }
      ],
      "source": [
        "#@title Instantiate the Dataset and create Train and Val Loaders\n",
        "ds = CustomDataset(cleaned_text, bpe_tokenizer, context_len=128, stride=64)\n",
        "\n",
        "\n",
        "### Train Val Split ###\n",
        "# to test how well our model can perform on unseen data, we create a data split to use a smaller version of it just for validation.\n",
        "#(we want model to generalize: develop language comprehension abilities, not \"memorise\" our training data!!)\n",
        "\n",
        "dataset_size = len(ds)\n",
        "train_size = int(0.9 * dataset_size) # training data will be 90% of all original data\n",
        "val_size = dataset_size - train_size # the remaining 10% will be our validation data\n",
        "\n",
        "print(f\"Train size: {train_size}; Val size: {val_size}\")\n",
        "\n",
        "# split data randomly, but in a reproducable way (with a set random number generator: seed)\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds = random_split(ds, [train_size, val_size], generator=generator)\n",
        "\n",
        "### Create DataLoaders ###\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=64, # n sequences of tokens per batch\n",
        "    shuffle=True,  # shuffle for training\n",
        "    drop_last=True, # drop incomplete batches\n",
        "    num_workers=0 # load data in the main process (safer for notebooks/debugging)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=64, # n sequences of tokens per batch\n",
        "    shuffle=False,  # no need to shuffle for validation\n",
        "    drop_last=True, # drop incomplete batches\n",
        "    num_workers=0 # load data in the main process (safer for notebooks/debugging)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2t4LfR8OPX0",
        "outputId": "3287da8e-6e39-4ac7-b225-7cd596bb8b22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch 0: \n",
            " tensor([[   11,   290,   787,  ...,    82, 45795,    11],\n",
            "        [   83, 30075,   607,  ...,  4621,   286,   340],\n",
            "        [  355,  4544, 24499,  ...,  2089,    11,   290],\n",
            "        ...,\n",
            "        [  447,   251,   198,  ...,  3368, 18877,    13],\n",
            "        [  392,   477,   326,  ...,   654,   286,   198],\n",
            "        [  611,  8168,   198,  ..., 40314,   284,   466]]) \n",
            " tensor([[  290,   787,   340,  ..., 45795,    11,   290],\n",
            "        [30075,   607,  7634,  ...,   286,   340,    11],\n",
            "        [ 4544, 24499,   198,  ...,    11,   290,   198],\n",
            "        ...,\n",
            "        [  251,   198,   198,  ..., 18877,    13,  4544],\n",
            "        [  477,   326,   636,  ...,   286,   198, 48466],\n",
            "        [ 8168,   198, 17772,  ...,   284,   466,   262]])\n",
            "batch 0: \n",
            " torch.Size([64, 128]) \n",
            " torch.Size([64, 128])\n"
          ]
        }
      ],
      "source": [
        "# test what our dataloader is doing\n",
        "torch.manual_seed(42)\n",
        "for i, (x,y) in enumerate(train_loader):\n",
        "    print(f'batch {i}:',\"\\n\", x, \"\\n\", y)\n",
        "    print(f'batch {i}:',\"\\n\", x.shape, \"\\n\", y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmC2wxGJPNnb",
        "outputId": "f1e6fbde-5618-4f35-86fb-17597032d460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0 (shape: torch.Size([64, 128]))\n",
            "\n",
            "Sample 1:\n",
            "Input : , and make it a very suffering\n",
            "exhibition to herself. Whatever might be its effect, however, she must\n",
            "stand the brunt of it again that very day.\n",
            "\n",
            "The first regular rehearsal of the three first acts was certainly to\n",
            "take place in the evening: Mrs. Grant and the Crawfords were engaged to\n",
            "return for that purpose as soon as they could after dinner; and every\n",
            "one concerned was looking forward with eagerness. There seemed a\n",
            "general diffusion of cheerfulness on the occasion. Tom was enjoying\n",
            "such an advance towards the end; Edmund was in spirits from the\n",
            "morning‚Äôs rehearsal,\n",
            "Target:  and make it a very suffering\n",
            "exhibition to herself. Whatever might be its effect, however, she must\n",
            "stand the brunt of it again that very day.\n",
            "\n",
            "The first regular rehearsal of the three first acts was certainly to\n",
            "take place in the evening: Mrs. Grant and the Crawfords were engaged to\n",
            "return for that purpose as soon as they could after dinner; and every\n",
            "one concerned was looking forward with eagerness. There seemed a\n",
            "general diffusion of cheerfulness on the occasion. Tom was enjoying\n",
            "such an advance towards the end; Edmund was in spirits from the\n",
            "morning‚Äôs rehearsal, and\n",
            "\n",
            "Sample 2:\n",
            "Input : touched her strongly. He was only too good to everybody. It was a\n",
            "letter, in short, which she would not but have had for the world, and\n",
            "which could never be valued enough. This was the end of it.\n",
            "\n",
            "Everybody at all addicted to letter-writing, without having much to\n",
            "say, which will include a large proportion of the female world at\n",
            "least, must feel with Lady Bertram that she was out of luck in having\n",
            "such a capital piece of Mansfield news as the certainty of the Grants\n",
            "going to Bath, occur at a time when she could make no advantage of it\n",
            "Target: ouched her strongly. He was only too good to everybody. It was a\n",
            "letter, in short, which she would not but have had for the world, and\n",
            "which could never be valued enough. This was the end of it.\n",
            "\n",
            "Everybody at all addicted to letter-writing, without having much to\n",
            "say, which will include a large proportion of the female world at\n",
            "least, must feel with Lady Bertram that she was out of luck in having\n",
            "such a capital piece of Mansfield news as the certainty of the Grants\n",
            "going to Bath, occur at a time when she could make no advantage of it,\n",
            "\n",
            "Sample 3:\n",
            "Input :  as Miss Crawford\n",
            "well knew; and her interest in a negotiation for William Price‚Äôs knave\n",
            "increased.\n",
            "\n",
            "‚ÄúWell,‚Äù continued Edmund, ‚Äúand how did you like what you saw?‚Äù\n",
            "\n",
            "‚ÄúVery much indeed. You are a lucky fellow. There will be work for five\n",
            "summers at least before the place is liveable.‚Äù\n",
            "\n",
            "‚ÄúNo, no, not so bad as that. The farmyard must be moved, I grant you;\n",
            "but I am not aware of anything else. The house is by no means bad, and\n",
            "Target:  Miss Crawford\n",
            "well knew; and her interest in a negotiation for William Price‚Äôs knave\n",
            "increased.\n",
            "\n",
            "‚ÄúWell,‚Äù continued Edmund, ‚Äúand how did you like what you saw?‚Äù\n",
            "\n",
            "‚ÄúVery much indeed. You are a lucky fellow. There will be work for five\n",
            "summers at least before the place is liveable.‚Äù\n",
            "\n",
            "‚ÄúNo, no, not so bad as that. The farmyard must be moved, I grant you;\n",
            "but I am not aware of anything else. The house is by no means bad, and\n",
            "\n",
            "\n",
            "Sample 4:\n",
            "Input :  delay their exercise‚Äù; and such hints producing nothing, he soon\n",
            "proceeded to a positive recommendation to Mrs. Price and her daughters\n",
            "to take their walk without loss of time. Now they came to an\n",
            "understanding. Mrs. Price, it appeared, scarcely ever stirred out of\n",
            "doors, except of a Sunday; she owned she could seldom, with her large\n",
            "family, find time for a walk. ‚ÄúWould she not, then, persuade her\n",
            "daughters to take advantage of such weather, and allow him the pleasure\n",
            "of attending them?‚Äù Mrs. Price was greatly obliged and very complying.\n",
            "Target:  their exercise‚Äù; and such hints producing nothing, he soon\n",
            "proceeded to a positive recommendation to Mrs. Price and her daughters\n",
            "to take their walk without loss of time. Now they came to an\n",
            "understanding. Mrs. Price, it appeared, scarcely ever stirred out of\n",
            "doors, except of a Sunday; she owned she could seldom, with her large\n",
            "family, find time for a walk. ‚ÄúWould she not, then, persuade her\n",
            "daughters to take advantage of such weather, and allow him the pleasure\n",
            "of attending them?‚Äù Mrs. Price was greatly obliged and very complying.\n",
            "\n",
            "\n",
            "Sample 5:\n",
            "Input :  and sent him half a guinea under the seal.\n",
            "Fanny‚Äôs feelings on the occasion were such as she believed herself\n",
            "incapable of expressing; but her countenance and a few artless words\n",
            "fully conveyed all their gratitude and delight, and her cousin began to\n",
            "find her an interesting object. He talked to her more, and, from all\n",
            "that she said, was convinced of her having an affectionate heart, and a\n",
            "strong desire of doing right; and he could perceive her to be farther\n",
            "entitled to attention by great sensibility of her situation, and great\n",
            "timidity. He had\n",
            "Target:  sent him half a guinea under the seal.\n",
            "Fanny‚Äôs feelings on the occasion were such as she believed herself\n",
            "incapable of expressing; but her countenance and a few artless words\n",
            "fully conveyed all their gratitude and delight, and her cousin began to\n",
            "find her an interesting object. He talked to her more, and, from all\n",
            "that she said, was convinced of her having an affectionate heart, and a\n",
            "strong desire of doing right; and he could perceive her to be farther\n",
            "entitled to attention by great sensibility of her situation, and great\n",
            "timidity. He had never\n"
          ]
        }
      ],
      "source": [
        "# test what our dataloader is doing, this time converting token sequences back to text\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "for i, (x, y) in enumerate(train_loader):\n",
        "    print(f\"Batch {i} (shape: {x.shape})\")\n",
        "\n",
        "    # Decode the first few examples in the batch\n",
        "    for idx in range(5):  # Show first 5 samples\n",
        "        input_text = bpe_tokenizer.decode(x[idx].tolist())\n",
        "        target_text = bpe_tokenizer.decode(y[idx].tolist())\n",
        "\n",
        "        print(f\"\\nSample {idx + 1}:\")\n",
        "        print(\"Input :\", input_text)\n",
        "        print(\"Target:\", target_text)\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt4bGZt4pUbg"
      },
      "source": [
        "## **3**.&nbsp; **Building a (Transformer) Language Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yTQG2aLIgSC"
      },
      "source": [
        "Now that we have prepared our data and downloaded an efficient tokenizer, we can proceed to building and training a language model itself!\n",
        "\n",
        "To do that, we implement the general pipeline:\n",
        "\n",
        "```perl\n",
        "      Input text\n",
        "          ‚îÇ\n",
        "          ‚ñº\n",
        "      Token IDs\n",
        "          ‚îÇ\n",
        "          ‚ñº\n",
        "      Embeddings\n",
        "          ‚îÇ\n",
        "          ‚ñº\n",
        "    Neural Network\n",
        "          ‚îÇ\n",
        "          ‚ñº\n",
        "Output Probabilities over Vocabulary\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdBDZkWypYRw"
      },
      "source": [
        "### **3. 1**.&nbsp; **Input Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjsX9WF6SXnr"
      },
      "source": [
        "We will start by creating an **embedding layer**.\n",
        "\n",
        "An **embedding layer is essentially a lookup table that maps each token ID to a fixed-size vector**. These vectors‚Äîcalled embeddings‚Äîare **dense representations that capture some of the meaning or usage patterns of tokens based on their context in the training data**. Rather than treating tokens as one-hot or integer indices, embeddings let the model work in a continuous vector space, where similar words can be placed closer together.\n",
        "\n",
        "‚ùóIn Transformer-based language models, we don't just represent individual tokens‚Äîwe also encode where each token appears in the sequence. This is done by adding positional embeddings to token embeddings, allowing the model to differentiate between, for example, \"dog bites man\" and \"man bites dog.\" Since Transformers process the entire input at once (not step-by-step like RNNs), positional information must be explicitly provided so the model understands word order.\n",
        "\n",
        "In our `BaseLanguageModel`, we rely on RNN, so we do not encode word positions, RNN does it by default.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq009rCNaG9i",
        "outputId": "c3c04e19-963a-4dff-829b-eb04d9c2dd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix:\n",
            " Parameter containing:\n",
            "tensor([[ 0.6784, -1.2345, -0.0431, -1.6047],\n",
            "        [ 0.3559, -0.6866, -0.4934,  0.2415],\n",
            "        [-1.1109,  0.0915, -2.3169, -0.2168],\n",
            "        [-0.3097, -0.3957,  0.8034, -0.6216],\n",
            "        [-0.5920, -0.0631, -0.8286,  0.3309]], requires_grad=True) \n",
            "\n",
            "Embedded input:\n",
            " tensor([[-0.5920, -0.0631, -0.8286,  0.3309],\n",
            "        [ 0.6784, -1.2345, -0.0431, -1.6047],\n",
            "        [ 0.3559, -0.6866, -0.4934,  0.2415]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#@title Embedding Layer Showcase\n",
        "\n",
        "# create an embedding layer with:\n",
        "# - vocab size of 5 (token IDs range from 0 to 4)\n",
        "# - embedding dimension of 4 (each token will be represented by a 4D vector)\n",
        "embd_layer = torch.nn.Embedding(5, 4)\n",
        "\n",
        "# create a toy input sequence of token IDs (e.g., as if these were words in a sentence)\n",
        "x = torch.tensor([4, 0, 1])\n",
        "\n",
        "# Take a look at the embedding matrix (5 tokens √ó 4 dimensions)\n",
        "print(\"Embedding matrix:\\n\", embd_layer.weight, \"\\n\")\n",
        "\n",
        "# use the embedding layer to look up vector representations of the input tokens\n",
        "print(\"Embedded input:\\n\", embd_layer(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRk0UvDHCaPh"
      },
      "source": [
        "### **3.2 Build a simple (RNN) language model**\n",
        "\n",
        "We will start with a basic language model consisting of just one *recurrent neural network (RNN)* layer<a href=\"#footnote2\"><sup>1</sup></a>, to develop an intuition on how a language model is constructed: its methods, attributes, and training behaviour.\n",
        "\n",
        "Building a language model from scratch‚Äîeven a simple one‚Äîis a powerful way to understand how machines learn patterns in language. Instead of treating language as a bag of words, we're teaching our model to predict the next word in a sentence based on the words that came before.\n",
        "\n",
        "\n",
        "\n",
        "<p id=\"footnote2\">\n",
        "  <sup>1</sup> ü§ñ What is an RNN?<br>\n",
        "  A recurrent neural network (RNN) is a type of neural network that is designed to process sequences of data‚Äîlike words in a sentence‚Äîone step at a time, while keeping track of what it has seen so far.\n",
        "  <strong>At each time step, it takes in a token (like a word or character), updates an internal \"memory\" (called a hidden state), and makes a prediction.</strong>\n",
        "  This memory allows it to capture context and order, which are crucial for understanding language.<br><br>\n",
        "  RNNs are a great starting point because they are conceptually simple and were historically the first models to successfully handle sequential tasks like text generation.\n",
        "  Although modern language models use more advanced architectures like <em>Transformers</em>, understanding RNNs gives you the foundation to appreciate how sequence modeling works‚Äîand where its limitations lie.\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0s13hziYy5a"
      },
      "outputs": [],
      "source": [
        "#@title Exercise: Complete a Language Model Implementation\n",
        "\n",
        "# TASK 1: Implement the right sequence of steps to do a forward pass (from inputs to logits/output \"probabilities/similarity scores\")\n",
        "# Hint: The steps are defined in the __init__() method. Use pytorch documentation to aid yourself.\n",
        "\n",
        "class BaseLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A basic autoregressive language model using a single RNN layer.\n",
        "\n",
        "    This model takes in sequences of token IDs and learns to predict the next token\n",
        "    at each position. It consists of an embedding layer, an RNN, and a linear output layer.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Dimensionality of the token embeddings.\n",
        "        n_hidden (int): Number of hidden units in the RNN.\n",
        "        tokenizer (SimpleTokenizer): Tokenizer with vocabulary used for encoding/decoding text.\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_hidden, tokenizer, device):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embd   # store n_embd as instance attribute named \"n_embd\"\n",
        "        self.n_hidden = n_hidden # store n_hidden as instance attribute named \"n_hidden\"\n",
        "        self.tokenizer = tokenizer # store tokenizer as instance attribute named \"tokenizer\"\n",
        "        self.device = device # store device as instance attribute named \"device\"\n",
        "\n",
        "        # Learnable embedding lookup table for tokens of dimension [vocab_size, n_embd]\n",
        "        self.embd = torch.nn.Embedding(self.tokenizer.n_vocab, self.n_embd)\n",
        "\n",
        "        # A simple recurrent neural network\n",
        "        self.rnn = torch.nn.RNN(self.n_embd, self.n_hidden, batch_first=True)\n",
        "\n",
        "        # Output layer: projects hidden states to vocabulary logits\n",
        "        self.out = torch.nn.Linear(self.n_hidden, self.tokenizer.n_vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the language model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, context_len].\n",
        "\n",
        "        Returns:\n",
        "            logits (Tensor): Predicted logits for each token in the vocabulary.\n",
        "        \"\"\"\n",
        "        # YOUR CODE HERE\n",
        "        return logits                       # returns raw \"similarity scores\" between sequence representation and all possible vocab token representations\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=10, lr=1e-3):\n",
        "        \"\"\"\n",
        "        Train the language model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): Training set wrapped in a DataLoader.\n",
        "            val_loader (DataLoader, optional): Validation set for evaluation.\n",
        "            epochs (int): Number of training epochs. Determines how many times we go over entire train data.\n",
        "            lr (float): Learning rate for optimizer. Determines how much we update model params.\n",
        "        \"\"\"\n",
        "        self.to(self.device) # which hardware to use to train model on\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr) # pick an algorithm to optimize model parameters\n",
        "        loss_fn = torch.nn.CrossEntropyLoss() # loss function we use to evaluate model performance\n",
        "\n",
        "        # go over train data to perform computation several times (set by epochs=n)\n",
        "        for epoch in range(epochs):\n",
        "            self.train() # put model to \"training mode\"\n",
        "            total_train_loss = 0.0 # placeholder to record our loss values (capturing model performance quality)\n",
        "\n",
        "            # go over each input - output sequence pairs in dataloader\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device) # put data on the same hardware as model, otherwise cant train model!\n",
        "\n",
        "                logits = self(X)                           # Forward pass: make predictions based on current model parameters\n",
        "                logits = logits.view(-1, self.tokenizer.n_vocab)  # Flatten to [batch * seq_len, vocab_size]; this is needed for torch.nn.CrossEntropyLoss()\n",
        "                Y = Y.view(-1)                             # Flatten targets to [batch * seq_len]; same as above\n",
        "\n",
        "                loss = loss_fn(logits, Y)               # calculate current model perfomrance, depicted in one scalar value\n",
        "\n",
        "                optimizer.zero_grad()               # remove information on gradients from the previous run, otherwise they get conflated with grads from current run!\n",
        "                loss.backward()                 # compute gradients of the loss fn with respect to model params to learn how to update them\n",
        "                optimizer.step()            # with information from gradients, update parameters in the right direction\n",
        "\n",
        "                total_train_loss += loss.item()     # record current loss\n",
        "\n",
        "            average_loss = total_train_loss / len(train_loader)     # record average loss over batches for the entire epoch\n",
        "\n",
        "            # Evaluate on validation set, if provided\n",
        "            if val_loader is not None:\n",
        "                self.eval() # set model to evaluation mode (turns of some functionality, not exactly needed here in BaseLanguageModel)\n",
        "                total_val_loss = 0.0\n",
        "\n",
        "\n",
        "                with torch.no_grad(): # do not compute and do not trace gradients!\n",
        "\n",
        "                    # for all input-output pairs in val dataloader:\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device) # put data on same hardware as model\n",
        "                        val_logits = self(Xv).view(-1, self.tokenizer.n_vocab) # adjust logits shape similar as in train\n",
        "                        Yv = Yv.view(-1)                # adjust ground truth labels shape similar as in train\n",
        "                        val_loss = loss_fn(val_logits, Yv) # compute loss\n",
        "                        total_val_loss += val_loss.item() # record loss\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)  # get average loss across batches for the entire epoch\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS3a0APFaeYk",
        "outputId": "072f91a5-f31d-46bd-abde-c9732082b067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 3144; Val size: 350\n"
          ]
        }
      ],
      "source": [
        "### Training set-up\n",
        "\n",
        "## uncomment below to change dataloader configurations! ###\n",
        "## Relevant params here are context_len, stride, batch_size! ###\n",
        "\n",
        "ds = CustomDataset(cleaned_text, bpe_tokenizer, context_len=128, stride=64)\n",
        "\n",
        "dataset_size = len(ds)\n",
        "train_size = int(0.9 * dataset_size) # training data will be 90% of all original data\n",
        "val_size = dataset_size - train_size # the remaining 10% will be our validation data\n",
        "\n",
        "print(f\"Train size: {train_size}; Val size: {val_size}\")\n",
        "\n",
        "# split data randomly, but in a reproducable way (with a set random number generator: seed)\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds = random_split(ds, [train_size, val_size], generator=generator)\n",
        "\n",
        "### Create DataLoaders ###\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=64, # n sequences of tokens per batch\n",
        "    shuffle=True,  # shuffle for training\n",
        "    drop_last=True, # drop incomplete batches\n",
        "    num_workers=0 # load data in the main process (safer for notebooks/debugging)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=64, # n sequences of tokens per batch\n",
        "    shuffle=False,  # no need to shuffle for validation\n",
        "    drop_last=True, # drop incomplete batches\n",
        "    num_workers=0 # load data in the main process (safer for notebooks/debugging)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aiZisfKw0hB",
        "outputId": "b5ae2e0e-893f-42ba-fdaa-fc732b9d5d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]  Train Loss: 6.264  |  Val Loss: 5.223\n",
            "Epoch [2/5]  Train Loss: 4.832  |  Val Loss: 4.663\n",
            "Epoch [3/5]  Train Loss: 4.355  |  Val Loss: 4.395\n",
            "Epoch [4/5]  Train Loss: 4.063  |  Val Loss: 4.219\n",
            "Epoch [5/5]  Train Loss: 3.842  |  Val Loss: 4.081\n"
          ]
        }
      ],
      "source": [
        "### training ###\n",
        "\n",
        "# uncomment in case CUDA out of memory\n",
        "# del model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "config = {\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\"\n",
        "}\n",
        "\n",
        "model = BaseLanguageModel(n_embd=128, n_hidden=128, **config)\n",
        "model.fit(train_loader, val_loader, epochs=5, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "cellView": "form",
        "id": "Km5D6MMCaInv"
      },
      "outputs": [],
      "source": [
        "#@title Solution: Build a Base LM\n",
        "\n",
        "class BaseLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A basic autoregressive language model using a single RNN layer.\n",
        "\n",
        "    This model takes in sequences of token IDs and learns to predict the next token\n",
        "    at each position. It consists of an embedding layer, an RNN, and a linear output layer.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Dimensionality of the token embeddings.\n",
        "        n_hidden (int): Number of hidden units in the RNN.\n",
        "        tokenizer (SimpleTokenizer): Tokenizer with vocabulary used for encoding/decoding text.\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_hidden, tokenizer, device):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embd   # store n_embd as instance attribute named \"n_embd\"\n",
        "        self.n_hidden = n_hidden # store n_hidden as instance attribute named \"n_hidden\"\n",
        "        self.tokenizer = tokenizer # store tokenizer as instance attribute named \"tokenizer\"\n",
        "        self.device = device # store device as instance attribute named \"device\"\n",
        "\n",
        "        # Learnable embedding lookup table for tokens of dimension [vocab_size, n_embd]\n",
        "        self.embd = torch.nn.Embedding(self.tokenizer.n_vocab, self.n_embd)\n",
        "\n",
        "        # A simple recurrent neural network\n",
        "        self.rnn = torch.nn.RNN(self.n_embd, self.n_hidden, batch_first=True)\n",
        "\n",
        "        # Output layer: projects hidden states to vocabulary logits\n",
        "        self.out = torch.nn.Linear(self.n_hidden, self.tokenizer.n_vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the language model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, context_len].\n",
        "\n",
        "        Returns:\n",
        "            logits (Tensor): Predicted logits for each token in the vocabulary.\n",
        "        \"\"\"\n",
        "        x = self.embd(x)                   # Embed token IDs to [batch, context_len, n_embd]; effectively \"picks up\" relevent token ids from embedding table\n",
        "        x, hidden = self.rnn(x)            # RNN processes the sequence ‚Üí [batch, context_len, n_hidden]\n",
        "        logits = self.out(x)               # Project to vocab size ‚Üí [batch, context_len, vocab_size]\n",
        "        return logits                       # returns raw \"similarity scores\" between sequence representation and all possible vocab token representations\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=10, lr=1e-3):\n",
        "        \"\"\"\n",
        "        Train the language model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): Training set wrapped in a DataLoader.\n",
        "            val_loader (DataLoader, optional): Validation set for evaluation.\n",
        "            epochs (int): Number of training epochs. Determines how many times we go over entire train data.\n",
        "            lr (float): Learning rate for optimizer. Determines how much we update model params.\n",
        "        \"\"\"\n",
        "        self.to(self.device) # which hardware to use to train model on\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr) # pick an algorithm to optimize model parameters\n",
        "        loss_fn = torch.nn.CrossEntropyLoss() # loss function we use to evaluate model performance\n",
        "\n",
        "        # go over train data to perform computation several times (set by epochs=n)\n",
        "        for epoch in range(epochs):\n",
        "            self.train() # put model to \"training mode\"\n",
        "            total_train_loss = 0.0 # placeholder to record our loss values (capturing model performance quality)\n",
        "\n",
        "            # go over each input - output sequence pairs in dataloader\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device) # put data on the same hardware as model, otherwise cant train model!\n",
        "\n",
        "                logits = self(X)                           # Forward pass: make predictions based on current model parameters\n",
        "                logits = logits.view(-1, self.tokenizer.n_vocab)  # Flatten to [batch * seq_len, vocab_size]; this is needed for torch.nn.CrossEntropyLoss()\n",
        "                Y = Y.view(-1)                             # Flatten targets to [batch * seq_len]; same as above\n",
        "\n",
        "                loss = loss_fn(logits, Y)               # calculate current model perfomrance, depicted in one scalar value\n",
        "\n",
        "                optimizer.zero_grad()               # remove information on gradients from the previous run, otherwise they get conflated with grads from current run!\n",
        "                loss.backward()                 # compute gradients of the loss fn with respect to model params to learn how to update them\n",
        "                optimizer.step()            # with information from gradients, update parameters in the right direction\n",
        "\n",
        "                total_train_loss += loss.item()     # record current loss\n",
        "\n",
        "            average_loss = total_train_loss / len(train_loader)     # record average loss over batches for the entire epoch\n",
        "\n",
        "            # Evaluate on validation set, if provided\n",
        "            if val_loader is not None:\n",
        "                self.eval() # set model to evaluation mode (turns of some functionality, not exactly needed here in BaseLanguageModel)\n",
        "                total_val_loss = 0.0\n",
        "\n",
        "\n",
        "                with torch.no_grad(): # do not compute and do not trace gradients!\n",
        "\n",
        "                    # for all input-output pairs in val dataloader:\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device) # put data on same hardware as model\n",
        "                        val_logits = self(Xv).view(-1, self.tokenizer.n_vocab) # adjust logits shape similar as in train\n",
        "                        Yv = Yv.view(-1)                # adjust ground truth labels shape similar as in train\n",
        "                        val_loss = loss_fn(val_logits, Yv) # compute loss\n",
        "                        total_val_loss += val_loss.item() # record loss\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)  # get average loss across batches for the entire epoch\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD2cT1SVcbBQ"
      },
      "outputs": [],
      "source": [
        "#@title Exercise: Add \"Generate\" method to the model\n",
        "\n",
        "# Task 1: implement an algorithm which given a sequence of tokens:\n",
        "    # - estimates probabilities for the next one,\n",
        "    # - samples a token from this probability distribution\n",
        "    # - adds the sampled token to the sequence\n",
        "    # - repeats the process up untill max_tokens\n",
        "\n",
        "# Hint 1: Pay attention to the dimensions of the logits tensor. Reminder: a tensor is essentially your np.array but with added perks (like automated gradient calculation and storage).\n",
        "# Hint 2: Use torch.softmax() method. Consult the pytorch documentation if needed.\n",
        "\n",
        "class BaseLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A basic autoregressive language model using a single RNN layer.\n",
        "\n",
        "    This model takes in sequences of token IDs and learns to predict the next token\n",
        "    at each position. It consists of an embedding layer, an RNN, and a linear output layer.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Dimensionality of the token embeddings.\n",
        "        n_hidden (int): Number of hidden units in the RNN.\n",
        "        tokenizer (SimpleTokenizer): Tokenizer with vocabulary used for encoding/decoding text.\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_hidden, tokenizer, device):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embd   # store n_embd as instance attribute named \"n_embd\"\n",
        "        self.n_hidden = n_hidden # store n_hidden as instance attribute named \"n_hidden\"\n",
        "        self.tokenizer = tokenizer # store tokenizer as instance attribute named \"tokenizer\"\n",
        "        self.device = device # store device as instance attribute named \"device\"\n",
        "\n",
        "        # Learnable embedding lookup table for tokens of dimension [vocab_size, n_embd]\n",
        "        self.embd = torch.nn.Embedding(self.tokenizer.n_vocab, self.n_embd)\n",
        "\n",
        "        # A simple recurrent neural network\n",
        "        self.rnn = torch.nn.RNN(self.n_embd, self.n_hidden, batch_first=True)\n",
        "\n",
        "        # Output layer: projects hidden states to vocabulary logits\n",
        "        self.out = torch.nn.Linear(self.n_hidden, self.tokenizer.n_vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the language model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, context_len].\n",
        "\n",
        "        Returns:\n",
        "            logits (Tensor): Predicted logits for each token in the vocabulary.\n",
        "        \"\"\"\n",
        "        x = self.embd(x)                   # Embed token IDs to [batch, context_len, n_embd]; effectively \"picks up\" relevent token ids from embedding table\n",
        "        x, hidden = self.rnn(x)            # RNN processes the sequence ‚Üí [batch, context_len, n_hidden]\n",
        "        logits = self.out(x)               # Project to vocab size ‚Üí [batch, context_len, vocab_size]\n",
        "        return logits                       # returns raw \"similarity scores\" between sequence representation and all possible vocab token representations\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=10, lr=1e-3):\n",
        "        \"\"\"\n",
        "        Train the language model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): Training set wrapped in a DataLoader.\n",
        "            val_loader (DataLoader, optional): Validation set for evaluation.\n",
        "            epochs (int): Number of training epochs. Determines how many times we go over entire train data.\n",
        "            lr (float): Learning rate for optimizer. Determines how much we update model params.\n",
        "        \"\"\"\n",
        "        self.to(self.device) # which hardware to use to train model on\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr) # pick an algorithm to optimize model parameters\n",
        "        loss_fn = torch.nn.CrossEntropyLoss() # loss function we use to evaluate model performance\n",
        "\n",
        "        # go over train data to perform computation several times (set by epochs=n)\n",
        "        for epoch in range(epochs):\n",
        "            self.train() # put model to \"training mode\"\n",
        "            total_train_loss = 0.0 # placeholder to record our loss values (capturing model performance quality)\n",
        "\n",
        "            # go over each input - output sequence pairs in dataloader\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device) # put data on the same hardware as model, otherwise cant train model!\n",
        "\n",
        "                logits = self(X)                           # Forward pass: make predictions based on current model parameters\n",
        "                logits = logits.view(-1, self.tokenizer.n_vocab)  # Flatten to [batch * seq_len, vocab_size]; this is needed for torch.nn.CrossEntropyLoss()\n",
        "                Y = Y.view(-1)                             # Flatten targets to [batch * seq_len]; same as above\n",
        "\n",
        "                loss = loss_fn(logits, Y)               # calculate current model perfomrance, depicted in one scalar value\n",
        "\n",
        "                optimizer.zero_grad()               # remove information on gradients from the previous run, otherwise they get conflated with grads from current run!\n",
        "                loss.backward()                 # compute gradients of the loss fn with respect to model params to learn how to update them\n",
        "                optimizer.step()            # with information from gradients, update parameters in the right direction\n",
        "\n",
        "                total_train_loss += loss.item()     # record current loss\n",
        "\n",
        "            average_loss = total_train_loss / len(train_loader)     # record average loss over batches for the entire epoch\n",
        "\n",
        "            # Evaluate on validation set, if provided\n",
        "            if val_loader is not None:\n",
        "                self.eval() # set model to evaluation mode (turns of some functionality, not exactly needed here in BaseLanguageModel)\n",
        "                total_val_loss = 0.0\n",
        "\n",
        "\n",
        "                with torch.no_grad(): # do not compute and do not trace gradients!\n",
        "\n",
        "                    # for all input-output pairs in val dataloader:\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device) # put data on same hardware as model\n",
        "                        val_logits = self(Xv).view(-1, self.tokenizer.n_vocab) # adjust logits shape similar as in train\n",
        "                        Yv = Yv.view(-1)                # adjust ground truth labels shape similar as in train\n",
        "                        val_loss = loss_fn(val_logits, Yv) # compute loss\n",
        "                        total_val_loss += val_loss.item() # record loss\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)  # get average loss across batches for the entire epoch\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Generate a sequence of text from a given prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Text to start generation from.\n",
        "            max_new_tokens (int): Number of tokens to generate.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text (prompt + completion).\n",
        "        \"\"\"\n",
        "        self.eval() # sets the model in evaluation mode (would ignore dropout lay)\n",
        "\n",
        "        # Encode prompt and add batch dimension\n",
        "        input_ids = self.tokenizer.encode(prompt)\n",
        "        # put them on same hardware as model, adding to it batch dimension 1 with unsqueeze (necessary to be able to run batched inference)\n",
        "        input_ids = torch.tensor(input_ids).to(self.device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():    # Don't compute grads!\n",
        "            for i in range(max_new_tokens):\n",
        "\n",
        "            # ADD YOUR CODE HERE:\n",
        "\n",
        "                logits =  # Get model output logits\n",
        "                last_logit =  # Get the logits for the last token; remember logits have shape [batch, sequence_len, vocab_size]\n",
        "                probs =  # Convert to probabilities (only the last dimension, that is probability distribution over all posibble tokens in vocab)\n",
        "\n",
        "                next_token = torch.multinomial(probs, num_samples=1) # Sample one token ID\n",
        "                input_ids = torch.cat((input_ids, next_token), dim=1)  # Append it to the sequence\n",
        "\n",
        "        generated_tokens = input_ids[0].tolist()  # Remove batch dimension\n",
        "        return self.tokenizer.decode(generated_tokens) # return decoded sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9AA72OEcFOq",
        "outputId": "20613883-60cc-4a49-b2ee-3439283b21f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]  Train Loss: 6.252  |  Val Loss: 5.204\n",
            "Epoch [2/5]  Train Loss: 4.813  |  Val Loss: 4.650\n",
            "Epoch [3/5]  Train Loss: 4.346  |  Val Loss: 4.380\n",
            "Epoch [4/5]  Train Loss: 4.053  |  Val Loss: 4.193\n",
            "Epoch [5/5]  Train Loss: 3.835  |  Val Loss: 4.075\n"
          ]
        }
      ],
      "source": [
        "### training ###\n",
        "\n",
        "# uncomment in case CUDA out of memory\n",
        "# del model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "config = {\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\"\n",
        "}\n",
        "\n",
        "model = BaseLanguageModel(n_embd=128, n_hidden=128, **config)\n",
        "model.fit(train_loader, val_loader, epochs=1, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-lEGJnr1JWL",
        "outputId": "221166b9-c139-4947-eb88-c34e5503df71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I went out with Lady Elizabeth to the meadows for the others that account as what, and had Rebecca stopped in comfort, Edmund‚Äôs\n",
            "crown, and all her\n",
            "leaving which hereafter?‚Äù\n",
            "\n",
            "‚ÄúAniamond without any time from him Mr.\n",
            "\n",
            "‚ÄúI am very much mistaken and her remarks together,\n",
            "and her\n"
          ]
        }
      ],
      "source": [
        "### inference ###\n",
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "-E5WUBxIb6nT"
      },
      "outputs": [],
      "source": [
        "#@title Solution: Add Generate Method to our Base LM\n",
        "\n",
        "class BaseLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A basic autoregressive language model using a single RNN layer.\n",
        "\n",
        "    This model takes in sequences of token IDs and learns to predict the next token\n",
        "    at each position. It consists of an embedding layer, an RNN, and a linear output layer.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Dimensionality of the token embeddings.\n",
        "        n_hidden (int): Number of hidden units in the RNN.\n",
        "        tokenizer (SimpleTokenizer): Tokenizer with vocabulary used for encoding/decoding text.\n",
        "        device (str): Device to run the model on ('cuda' or 'cpu').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_hidden, tokenizer, device):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embd   # store n_embd as instance attribute named \"n_embd\"\n",
        "        self.n_hidden = n_hidden # store n_hidden as instance attribute named \"n_hidden\"\n",
        "        self.tokenizer = tokenizer # store tokenizer as instance attribute named \"tokenizer\"\n",
        "        self.device = device # store device as instance attribute named \"device\"\n",
        "\n",
        "        # Learnable embedding lookup table for tokens of dimension [vocab_size, n_embd]\n",
        "        self.embd = torch.nn.Embedding(self.tokenizer.n_vocab, self.n_embd)\n",
        "\n",
        "        # A simple recurrent neural network\n",
        "        self.rnn = torch.nn.RNN(self.n_embd, self.n_hidden, batch_first=True)\n",
        "\n",
        "        # Output layer: projects hidden states to vocabulary logits\n",
        "        self.out = torch.nn.Linear(self.n_hidden, self.tokenizer.n_vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the language model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, context_len].\n",
        "\n",
        "        Returns:\n",
        "            logits (Tensor): Predicted logits for each token in the vocabulary.\n",
        "        \"\"\"\n",
        "        x = self.embd(x)                   # Embed token IDs to [batch, context_len, n_embd]; effectively \"picks up\" relevent token ids from embedding table\n",
        "        x, hidden = self.rnn(x)            # RNN processes the sequence ‚Üí [batch, context_len, n_hidden]\n",
        "        logits = self.out(x)               # Project to vocab size ‚Üí [batch, context_len, vocab_size]\n",
        "        return logits                       # returns raw \"similarity scores\" between sequence representation and all possible vocab token representations\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=10, lr=1e-3):\n",
        "        \"\"\"\n",
        "        Train the language model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): Training set wrapped in a DataLoader.\n",
        "            val_loader (DataLoader, optional): Validation set for evaluation.\n",
        "            epochs (int): Number of training epochs. Determines how many times we go over entire train data.\n",
        "            lr (float): Learning rate for optimizer. Determines how much we update model params.\n",
        "        \"\"\"\n",
        "        self.to(self.device) # which hardware to use to train model on\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr) # pick an algorithm to optimize model parameters\n",
        "        loss_fn = torch.nn.CrossEntropyLoss() # loss function we use to evaluate model performance\n",
        "\n",
        "        # go over train data to perform computation several times (set by epochs=n)\n",
        "        for epoch in range(epochs):\n",
        "            self.train() # put model to \"training mode\"\n",
        "            total_train_loss = 0.0 # placeholder to record our loss values (capturing model performance quality)\n",
        "\n",
        "            # go over each input - output sequence pairs in dataloader\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device) # put data on the same hardware as model, otherwise cant train model!\n",
        "\n",
        "                logits = self(X)                           # Forward pass: make predictions based on current model parameters\n",
        "                logits = logits.view(-1, self.tokenizer.n_vocab)  # Flatten to [batch * seq_len, vocab_size]; this is needed for torch.nn.CrossEntropyLoss()\n",
        "                Y = Y.view(-1)                             # Flatten targets to [batch * seq_len]; same as above\n",
        "\n",
        "                loss = loss_fn(logits, Y)               # calculate current model perfomrance, depicted in one scalar value\n",
        "\n",
        "                optimizer.zero_grad()               # remove information on gradients from the previous run, otherwise they get conflated with grads from current run!\n",
        "                loss.backward()                 # compute gradients of the loss fn with respect to model params to learn how to update them\n",
        "                optimizer.step()            # with information from gradients, update parameters in the right direction\n",
        "\n",
        "                total_train_loss += loss.item()     # record current loss\n",
        "\n",
        "            average_loss = total_train_loss / len(train_loader)     # record average loss over batches for the entire epoch\n",
        "\n",
        "            # Evaluate on validation set, if provided\n",
        "            if val_loader is not None:\n",
        "                self.eval() # set model to evaluation mode (turns of some functionality, not exactly needed here in BaseLanguageModel)\n",
        "                total_val_loss = 0.0\n",
        "\n",
        "\n",
        "                with torch.no_grad(): # do not compute and do not trace gradients!\n",
        "\n",
        "                    # for all input-output pairs in val dataloader:\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device) # put data on same hardware as model\n",
        "                        val_logits = self(Xv).view(-1, self.tokenizer.n_vocab) # adjust logits shape similar as in train\n",
        "                        Yv = Yv.view(-1)                # adjust ground truth labels shape similar as in train\n",
        "                        val_loss = loss_fn(val_logits, Yv) # compute loss\n",
        "                        total_val_loss += val_loss.item() # record loss\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)  # get average loss across batches for the entire epoch\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {average_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens):\n",
        "        \"\"\"\n",
        "        Generate a sequence of text from a given prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Text to start generation from.\n",
        "            max_new_tokens (int): Number of tokens to generate.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text (prompt + completion).\n",
        "        \"\"\"\n",
        "        self.eval() # sets the model in evaluation mode (would ignore dropout lay)\n",
        "\n",
        "        # Encode prompt and add batch dimension\n",
        "        input_ids = self.tokenizer.encode(prompt)\n",
        "        # put them on same hardware as model, adding to it batch dimension 1 with unsqueeze (necessary to be able to run batched inference)\n",
        "        input_ids = torch.tensor(input_ids).to(self.device).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():    # Don't compute grads!\n",
        "            for i in range(max_new_tokens):\n",
        "\n",
        "                logits = self(input_ids)  # Get model output logits\n",
        "\n",
        "                last_logit = logits[:, -1, :]                        # Get the logits for the last token; remember logits have shape [batch, sequence_len, vocab_size]\n",
        "                probs = torch.softmax(last_logit, dim=-1)            # Convert to probabilities (only the last dimension, that is probability distribution over all posibble tokens in vocab)\n",
        "\n",
        "                next_token = torch.multinomial(probs, num_samples=1) # Sample one token ID\n",
        "                input_ids = torch.cat((input_ids, next_token), dim=1)  # Append it to the sequence\n",
        "\n",
        "        generated_tokens = input_ids[0].tolist()  # Remove batch dimension\n",
        "        return self.tokenizer.decode(generated_tokens) # return decoded sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3ioHQop781n"
      },
      "outputs": [],
      "source": [
        "#@title Exercise: Play around with the model\n",
        "\n",
        "# experiment with prompts, try training the model with different configurations (be careful with CUDA out of memory issue!)\n",
        "# what is your impression? what do you notice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfIG8xPO3mbj"
      },
      "source": [
        "#### **Conclusion**\n",
        "\n",
        "Our `BaseLanguageModel` actually does quite well: it is able to produce sentences that look reasonable, have grammar structure that resembles real English, and can even‚Äîat times‚Äîspill out funny sentences that carry a bit of meaning.\n",
        "\n",
        "Yet, this approach is limited by the nature of **RNNs**.\n",
        "\n",
        "While RNNs can model sequences and maintain memory across time steps, they struggle with **long-range dependencies**. In practice, this means they can forget important context from earlier in the sequence, especially as the input gets longer. RNNs also process tokens **sequentially**, which makes them **slower** and harder to parallelize during training.\n",
        "\n",
        "These limitations motivated the development of more powerful architectures like **Transformers**, which can:\n",
        "- **Attend to all tokens at once**, regardless of distance\n",
        "- **Model richer context** using self-attention\n",
        "- **Train more efficiently** on large datasets using parallel computation\n",
        "\n",
        "In the next step of our journey, we will leave RNNs behind and begin building a Transformer-based language model‚Äîthe architecture behind models like GPT. But now that we havve implemented a model from scratch, seen it learn, and generated outputs, we are much better equipped to understand how modern language models work‚Äîand why they perform so well.\n",
        "\n",
        "üß†‚ú® This is what we will cover next!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbq8Fez8pai_"
      },
      "source": [
        "### **3.3 Transformer Architecture**\n",
        "\n",
        "Nearly all state-of-the-art (SOTA) large language models (LLMs) ‚Äî including GPT, BERT, and their predecessors ‚Äî are built on variations of the **Transformer** architecture. Originally introduced by [Vaswani et al., 2017](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf), the Transformer marked a major shift in how models understand and generate language.\n",
        "\n",
        "The core idea behind Transformers is to create a neural network that can **understand the meaning of a word by looking at all other words in the sequence at once**, rather than processing them one at a time (like RNNs do). This allows the model to grasp rich contextual relationships between tokens, leading to significantly better performance on language understanding and generation tasks.\n",
        "\n",
        "Transformer consists of a **series of consequtive transformer blocks**, each having a:\n",
        "\n",
        "-  **multi-head attention component**, where tokens communicate with each other and update own representation based on most relevant context\n",
        "- **feed forward (multi-layered perceptron)** component, where tokens \"think and reflect\" on their own representation separately, encoding more information. Technically, feed-forward is a position-wise nonlinear transformation that projects the embedding to a higher dimension and back, allowing for richer transformations (capture more abstract features).\n",
        "\n",
        "\n",
        "\n",
        "> üß∞ Explore Transformers interactively:\n",
        "> - [LLM Visualisation](https://bbycroft.net/llm)\n",
        "> - [Transformer Explainer (Polo Club)](https://poloclub.github.io/transformer-explainer/)\n",
        "> - [The Illustrated Transformer by Jay Alammar](https://jalammar.github.io/illustrated-transformer/)\n",
        "> - [Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)\n",
        "\n",
        "---\n",
        "\n",
        "### üîé **Attention ‚Äî The Heart of a Transformer**\n",
        "\n",
        "The core mechanism behind the Transformer's power is **self-attention**. Attention allows each token in a sequence to **update its representation by \"attending to\" all other tokens**, weighting them based on how relevant they are to the current token.\n",
        "\n",
        "This means the model doesn't just look at the nearby words ‚Äî it can learn to focus on **the most important context**, even if it's far away in the sentence. This enables highly **context-aware**, nuanced understanding of meaning.\n",
        "\n",
        "To implement this, the attention mechanism uses three vectors for each token:\n",
        "\n",
        "- **Query**: What am I looking for? (What kind of info do I want from others?)\n",
        "- **Key**: What kind of info do I carry? (So others can decide if they want to attend to me.)\n",
        "- **Value**: What content do I actually give other tokens if they attend to me?\n",
        "\n",
        "During attention, each token's **query** is compared to the **keys** of all other tokens using the **dot product**, which measures similarity. Then, a weighted average of the corresponding **values** is computed:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V\n",
        "$$\n",
        "\n",
        "- The **dot product** here acts as a similarity score: higher values mean stronger alignment between the query and a key.\n",
        "- The **softmax** turns these scores into a probability distribution.\n",
        "- The **resulting weighted sum of values** becomes the new representation for that token.\n",
        "\n",
        "#### üí° **Summary**\n",
        "\n",
        "üîë The token embedding is the starting point. It captures a \"summary\" or general information of what a token means.\n",
        "\n",
        "üõ† Q, K, V are different interpretations of the same token, depending on what role it's playing in attention.\n",
        "\n",
        "Attention allows tokens update own representation by gathering relevant information from other tokens in the sequence, in a very computationally efficient and parallelizable way. That makes attention easy to scale.\n",
        "\n",
        "We will implement an Attention Layer and replace the RNN Layer with it, to improve our Language Model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "cellView": "form",
        "id": "UMIlbHoRMaWT"
      },
      "outputs": [],
      "source": [
        "#@title Define SelfAttention Class\n",
        "\n",
        "class SelfAttention(torch.nn.Module):\n",
        "    def __init__(self, n_embd, head_size, contex_len, dropout):\n",
        "        \"\"\"\n",
        "        Implements single-head self-attention with causal masking and dropout.\n",
        "\n",
        "        Args:\n",
        "            n_embd (int): Dimensionality of input embeddings.\n",
        "            head_size (int): Size of the output vector for this attention head.\n",
        "            contex_len (int): Max sequence length, used to create the causal mask.\n",
        "            dropout (float): Dropout probability for regularizing attention weights.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "\n",
        "        # Linear layers to compute Query, Key, and Value vectors from input\n",
        "        self.Q = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.K = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.V = torch.nn.Linear(n_embd, head_size, bias=False)\n",
        "\n",
        "        # Dropout for attention weights to prevent overfitting\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # Lower-triangular mask for causal (autoregressive) attention\n",
        "        # What this does is hide every \"future\" token, so no attention weight is computed there.\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(contex_len, contex_len)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for self-attention.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (B, S, C)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (B, S, head_size)\n",
        "        \"\"\"\n",
        "        B, S, E = x.shape # batch, sequence_len, embedding dimension\n",
        "\n",
        "        # Compute Q, K, V projections\n",
        "        q = self.Q(x)  # (B, S, head_size)\n",
        "        k = self.K(x)  # (B, S, head_size)\n",
        "        v = self.V(x)  # (B, S, head_size)\n",
        "\n",
        "        # Compute raw attention scores (dot product QK^T)\n",
        "        att_weight = q @ k.transpose(-2, -1)  # (B, S, S)\n",
        "\n",
        "        # Apply causal mask to block future tokens\n",
        "        att_weight = att_weight.masked_fill(self.tril[:S, :S] == 0, float('-inf'))\n",
        "\n",
        "        # Scale scores to stabilise softmax\n",
        "        att_weight = att_weight / (k.shape[-1] ** 0.5)\n",
        "\n",
        "        # Convert scores to probabilities\n",
        "        att_weight = torch.softmax(att_weight, dim=-1)\n",
        "\n",
        "        # Apply dropout to attention weights\n",
        "        att_weight = self.dropout(att_weight)\n",
        "\n",
        "        # Weighted sum of values (V) using attention weights\n",
        "        out = att_weight @ v  # (B, S, head_size)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqfkHJ742K9L"
      },
      "source": [
        "### **Multi-Head Attention**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiUo1CvU2O9G"
      },
      "source": [
        "Words can relate to each other in many different ways within a sentence. A word might signal **grammar** (like subject-verb agreement), carry **semantic meaning**, or refer back to something mentioned earlier. These relationships often happen **simultaneously** and at **different levels of abstraction**. For that reason, in a Transformer, we do not just apply a single attention mechanism. Instead, we use **multiple attention heads**‚Äîeach one learns to focus on different types of relationships in the sequence.\n",
        "\n",
        "This concept is known as **multi-head attention**. Each head performs its own attention computation independently, potentially capturing different linguistic patterns. For instance, one head might learn to follow pronouns back to their antecedents, while another might track verb-object dependencies, and yet another might focus on word sense disambiguation.\n",
        "\n",
        "In implementation, this is done by **splitting the embedding dimension across multiple ‚Äúheads‚Äù**, applying self-attention in parallel across these slices, and then **concatenating** the outputs from all heads. A final linear projection mixes them back into a unified representation. This setup gives the model the ability to process richer and more diverse contextual information at every layer. You can see this implemented below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "cellView": "form",
        "id": "NUgfWMIS2OMM"
      },
      "outputs": [],
      "source": [
        "#@title Define MultiHeadAttention\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_embd, num_heads, context_len, dropout):\n",
        "        \"\"\"\n",
        "        Multi-head self-attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            n_embd (int): Total embedding dimension.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            context_len (int): Sequence length for causal mask.\n",
        "            dropout (float): Dropout probability for attention weights and output.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        head_size = n_embd // num_heads\n",
        "\n",
        "        # Create multiple independent attention heads\n",
        "        self.heads = torch.nn.ModuleList([\n",
        "            SelfAttention(n_embd, head_size, context_len, dropout)\n",
        "            for _ in range(num_heads)\n",
        "        ])\n",
        "\n",
        "        # Projection layer to combine the outputs of all heads\n",
        "        self.proj = torch.nn.Linear(num_heads * head_size, n_embd)\n",
        "\n",
        "        # Dropout applied after projection\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through multi-head attention.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape (B, T, n_embd)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Output tensor of shape (B, T, n_embd)\n",
        "        \"\"\"\n",
        "        # Run each head independently and concatenate their outputs\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)  # (B, T, num_heads * head_size)\n",
        "\n",
        "        # Project concatenated output back to embedding space\n",
        "        out = self.proj(out)\n",
        "\n",
        "        # Apply dropout after projection\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8cJgyds-8YrP"
      },
      "outputs": [],
      "source": [
        "#@title Build Transformer-Based Language Model\n",
        "\n",
        "class AttentionLanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-inspired language model using multi-head self-attention.\n",
        "\n",
        "    Replaces the RNN in the base model with attention mechanisms that allow each token\n",
        "    to attend to all previous tokens in the sequence. This enables richer context modeling\n",
        "    and better parallelization during training.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Embedding size per token.\n",
        "        tokenizer (SimpleTokenizer): Tokenizer to convert text to token IDs and back.\n",
        "        device (str): 'cuda' or 'cpu', based on available hardware.\n",
        "        context_len (int): Maximum sequence length.\n",
        "        n_heads (int): Number of attention heads in multi-head attention.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_embd,\n",
        "        tokenizer,\n",
        "        device=\"cpu\",\n",
        "        context_len=128,\n",
        "        n_heads=4,\n",
        "        dropout=0.2\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.context_len = context_len\n",
        "        self.vocab_size = tokenizer.n_vocab\n",
        "\n",
        "        # Token embeddings: convert token IDs to learnable vectors\n",
        "        self.embd = torch.nn.Embedding(self.vocab_size, n_embd)\n",
        "        # NEW! Positional embedding!\n",
        "        # Positional embeddings: inject order information (since attention is position-agnostic)\n",
        "        self.pos_embd = torch.nn.Embedding(self.context_len, n_embd)\n",
        "\n",
        "        # Multi-head attention layer: allows model to attend to multiple types of relationships in parallel\n",
        "        self.attn = MultiHeadAttention(n_embd, n_heads, context_len, dropout)\n",
        "\n",
        "        # Output projection: maps the contextualized token embeddings to vocabulary logits\n",
        "        self.out = torch.nn.Linear(n_embd, self.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Tensor of token IDs with shape [batch_size, sequence_len]\n",
        "\n",
        "        Returns:\n",
        "            logits (Tensor): Raw predictions for the next token (unnormalized), shape [batch_size, sequence_len, vocab_size]\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Get token and positional embeddings and sum them\n",
        "        embds = self.embd(x)  # (B, S, n_embd)\n",
        "        pos = torch.arange(0, seq_len, dtype=torch.long, device=self.device).unsqueeze(0)  # (1, S)\n",
        "        embds = embds + self.pos_embd(pos)  # (B, S, n_embd)\n",
        "\n",
        "        # Apply multi-head attention\n",
        "        attention_out = self.attn(embds)  # (B, S, n_embd)\n",
        "\n",
        "        # Project attention outputs to vocabulary space\n",
        "        logits = self.out(attention_out)  # (B, S, vocab_size)\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=5, lr=1e-3):\n",
        "        \"\"\"\n",
        "        Train the language model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): Dataloader for training data.\n",
        "            val_loader (DataLoader, optional): Dataloader for validation data.\n",
        "            epochs (int): Number of training epochs.\n",
        "            lr (float): Learning rate for the optimizer.\n",
        "        \"\"\"\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device)\n",
        "\n",
        "                logits = self(X).view(-1, self.vocab_size)\n",
        "                Y = Y.view(-1)\n",
        "                loss = loss_fn(logits, Y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            if val_loader:\n",
        "                self.eval()\n",
        "                total_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device)\n",
        "                        val_logits = self(Xv).view(-1, self.vocab_size)\n",
        "                        Yv = Yv.view(-1)\n",
        "                        val_loss = loss_fn(val_logits, Yv)\n",
        "                        total_val_loss += val_loss.item()\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_train_loss:.3f} | Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_train_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=16):\n",
        "        \"\"\"\n",
        "        Generate text from a given prompt using greedy sampling.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Initial text input.\n",
        "            max_new_tokens (int): Number of tokens to generate beyond the prompt.\n",
        "\n",
        "        Returns:\n",
        "            str: Generated continuation text.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        # Encode prompt and add batch dimension\n",
        "        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Use only last context_len tokens (truncate if input gets too long)\n",
        "                input_ids_cond = input_ids[:, -self.context_len:]\n",
        "\n",
        "\n",
        "                logits = self(input_ids_cond)\n",
        "\n",
        "                last_logits = logits[:, -1, :]  # logits for the last position, across all tokens in vocab\n",
        "                probs = torch.softmax(last_logits, dim=-1)  # convert to probabilities\n",
        "\n",
        "                next_token = torch.multinomial(probs, num_samples=1)  # sample one token\n",
        "                input_ids = torch.cat((input_ids, next_token), dim=1)  # append to sequence\n",
        "\n",
        "        return self.tokenizer.decode(input_ids[0].tolist())  # remove batch dimension and decode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_p-rQDvCp4j"
      },
      "source": [
        "Because our model now includes architectural components that depend on the context window size (like attention masks and positional embeddings), we must ensure that our data is structured accordingly. **Both the dataset and the model require the same `context_len` value for proper alignment of input sequences and model expectations**.\n",
        "\n",
        "To avoid potential mismatches, we instantiate both the dataset and the model using a **shared configuration object**. This ensures the training and validation data loaders are always consistent with the model's architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "p0eq_VD8Cpd3"
      },
      "outputs": [],
      "source": [
        "# Main Configuration file, stores parameters that determine how we conduct training\n",
        "config = {\n",
        "    \"n_embd\": 128,\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\",\n",
        "    \"context_len\": 128, # dataset context_len has to be smaller or equal to model_context len!\n",
        "    \"stride\": 64,\n",
        "    \"batch_size\": 64,\n",
        "    \"n_heads\": 8,\n",
        "    \"dropout\": 0.2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "_UWpsOo7CgeS"
      },
      "outputs": [],
      "source": [
        "#@title Re-Instantiate Dataset and Dataloaders\n",
        "\n",
        "ds = CustomDataset(\n",
        "    text=cleaned_text,\n",
        "    tokenizer=config[\"tokenizer\"],\n",
        "    context_len=config[\"context_len\"],\n",
        "    stride=config[\"stride\"]\n",
        ")\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "dataset_size = len(ds)\n",
        "train_size = int(0.9 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia7l-mIQTLMt",
        "outputId": "0a477f20-ff1f-479d-b3a3-7cebbe2d39a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5]  Train Loss: 6.410 | Val Loss: 5.474\n",
            "Epoch [2/5]  Train Loss: 5.230 | Val Loss: 4.994\n",
            "Epoch [3/5]  Train Loss: 4.845 | Val Loss: 4.750\n",
            "Epoch [4/5]  Train Loss: 4.616 | Val Loss: 4.617\n",
            "Epoch [5/5]  Train Loss: 4.442 | Val Loss: 4.522\n"
          ]
        }
      ],
      "source": [
        "# runs about 20 seconds on a T4 GPU\n",
        "model = AttentionLanguageModel(\n",
        "    n_embd=config[\"n_embd\"],\n",
        "    tokenizer=config[\"tokenizer\"],\n",
        "    device=config[\"device\"],\n",
        "    context_len=config[\"context_len\"],\n",
        "    n_heads=config[\"n_heads\"],\n",
        "    dropout=config[\"dropout\"]\n",
        ")\n",
        "\n",
        "model.fit(train_loader, val_loader, epochs=5, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fzhTMD09zPE",
        "outputId": "7cd022c1-9d99-4da2-8a39-683623cb045d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I went out with Lady Elizabeth to the meadows. Bertram with Portsmouth,\n",
            "You discernening to be struck in all herself what\n",
            "in right care, sorry\n",
            "within,‚Äôs feelings of such\n"
          ]
        }
      ],
      "source": [
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYLv5MruT2ZM",
        "outputId": "0dea5c3d-863d-4f44-b596-f5e75c537d3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fluffy was not feeling well today, and his ease can\n",
            "know in house back accordingly her spirits to not to\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Fluffy was not feeling well today\"\n",
        "\n",
        "print(model.generate(prompt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIz8KBaOUMXx"
      },
      "outputs": [],
      "source": [
        "#@title Exercise: Play around with the model\n",
        "\n",
        "# experiment with prompts, try training the model with different configurations\n",
        "# what is your impression? what do you notice?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54_Ou4csW258"
      },
      "source": [
        "### **3. 4**.&nbsp; üîÅ **Feedforward Layers and Transformer Blocks**\n",
        "\n",
        "After the attention layer allows each token to gather context from other tokens in the sequence, we still need a way for the model to **transform and refine** this information. That is the job of the **Feedforward Neural Network (FFN)** block.\n",
        "\n",
        "Think of it this way: attention lets a token *look around*, but the feedforward block lets it *think*. It applies non-linear transformations to each token's representation **independently**, enabling the model to capture complex patterns, reweight features, and inject additional depth to the learned meaning.\n",
        "\n",
        "But one attention + feedforward combo is not enough to model the full richness of natural language. Language has multiple layers of structure ‚Äî syntax, semantics, discourse, etc. For that reason, we **stack multiple transformer blocks**. Each block learns increasingly abstract and meaningful representations of the input. Early layers might learn basic syntax; deeper ones grasp word sense, grammar rules, and long-range dependencies.\n",
        "\n",
        "Each transformer block is composed of:\n",
        "- **Multi-head attention** to gather contextual information\n",
        "- A **feedforward block** to process and transform it\n",
        "- **LayerNorm**, **residual (skip) connections**, and **dropout**, which help with training stability and generalization\n",
        "\n",
        "Together, these building blocks form the **core of a Transformer language model** ‚Äî scalable, expressive, and capable of capturing the full complexity of language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "form",
        "id": "KpgdrQK1w0i_"
      },
      "outputs": [],
      "source": [
        "#@title FeedForward and Transformer Block Classes\n",
        "\n",
        "class FeedForward(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Position-wise Feedforward Neural Network used inside each Transformer block.\n",
        "\n",
        "    This layer:\n",
        "    - Applies two linear transformations with a non-linearity (GELU) in between.\n",
        "    - Expands the embedding dimension by 4x and then reduces it back.\n",
        "    - Processes each token position independently (i.e., across the sequence dimension).\n",
        "    - Includes dropout to reduce overfitting.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Embedding dimension (input and output size).\n",
        "        dropout (float): Dropout probability applied after the second linear layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ff = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_embd, n_embd * 4),  # project up to a higher dimension (helps find and store more information for each token)\n",
        "            torch.nn.GELU(),                      # Non-linear activation (helps model complex relationships in data)\n",
        "            torch.nn.Linear(n_embd * 4, n_embd),  # project back to original embedding size (to feed to the next attention layer and let tokens learn from each other again!)\n",
        "            torch.nn.Dropout(dropout)             # Apply dropout to prevent memorising data, encourages generalisation!\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)  # Apply the feedforward network to every token embedding\n",
        "\n",
        "\n",
        "class TfBlock(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer block composed of:\n",
        "    - Multi-head self-attention (MHA)\n",
        "    - Feedforward neural network (FFN)\n",
        "    - Layer normalization (applied before each sub-block)\n",
        "    - Residual (skip) connections around both MHA and FFN\n",
        "\n",
        "    This design follows the Transformer architecture introduced in \"Attention is All You Need\".\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Embedding dimension (model hidden size).\n",
        "        n_heads (int): Number of attention heads.\n",
        "        context_len (int): Sequence length (for causal masking).\n",
        "        dropout (float): Dropout probability.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_embd, n_heads, context_len, dropout):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(n_embd, n_heads, context_len, dropout)  # Contextualization via attention\n",
        "        self.ff = FeedForward(n_embd, dropout)                                # Independent processing of token reps\n",
        "        self.norm_1 = torch.nn.LayerNorm(n_embd)  # Normalize before MHA (Pre-LN); this helps stabilise gradient flow! CRUCIAL for deep nns.\n",
        "        self.norm_2 = torch.nn.LayerNorm(n_embd)  # Normalize before FFN (Pre-LN); this helps stabilise gradient flow! CRUCIAL for deep nns.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # LayerNorm + Residual around attention\n",
        "        x = x + self.mha(self.norm_1(x))\n",
        "\n",
        "        # LayerNorm + Residual around feedforward\n",
        "        x = x + self.ff(self.norm_2(x))\n",
        "\n",
        "        return x  # Output of one transformer block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "form",
        "id": "PYK-QkLsyylE"
      },
      "outputs": [],
      "source": [
        "#@title Full Transformer Model\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer-based language model using multi-head self-attention and stacked Transformer blocks.\n",
        "\n",
        "    This model replaces recurrent layers with attention-based blocks to enable parallelization,\n",
        "    longer context modeling, and richer language representations.\n",
        "\n",
        "    Args:\n",
        "        n_embd (int): Dimensionality of token and positional embeddings.\n",
        "        tokenizer (SimpleTokenizer): Tokenizer for encoding/decoding text.\n",
        "        device (str): Device to run the model on (\"cpu\" or \"cuda\").\n",
        "        dropout (float): Dropout rate applied in attention and feedforward layers.\n",
        "        context_len (int): Maximum input sequence length (context window).\n",
        "        n_heads (int): Number of attention heads.\n",
        "        n_blocks (int): Number of Transformer blocks (depth).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_embd,\n",
        "        tokenizer,\n",
        "        device=\"cpu\",\n",
        "        dropout=0.2,\n",
        "        context_len=64,\n",
        "        n_heads=4,\n",
        "        n_blocks=3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "        self.dropout = dropout\n",
        "        self.vocab_size = tokenizer.n_vocab\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # Token embedding layer: learns a vector for each token ID\n",
        "        self.embd = torch.nn.Embedding(self.vocab_size, n_embd)\n",
        "\n",
        "        # Positional embedding layer: adds sequence order info to each token\n",
        "        self.pos_embd = torch.nn.Embedding(self.context_len, n_embd)\n",
        "\n",
        "        # Stack of Transformer blocks: each contains attention, FFN, norm, etc.\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "            TfBlock(n_embd, n_heads, context_len, dropout)\n",
        "            for _ in range(n_blocks)\n",
        "        ])\n",
        "\n",
        "        # Final layer norm after all blocks\n",
        "        self.norm_final = torch.nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Output projection: from final embedding to vocabulary logits\n",
        "        self.out = torch.nn.Linear(n_embd, self.vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor of shape [batch_size, context_len]\n",
        "\n",
        "        Returns:\n",
        "            logits (Tensor): Output tensor of shape [batch_size, context_len, vocab_size]\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Token and positional embeddings\n",
        "        embds = self.embd(x)  # (B, seq_len, n_embd)\n",
        "        pos = torch.arange(seq_len, device=self.device).unsqueeze(0)  # (1, seq_len)\n",
        "        embds = embds + self.pos_embd(pos)  # Add token + position embeddings\n",
        "\n",
        "        # Apply transformer blocks\n",
        "        blocks_out = self.blocks(embds)  # (B, seq_len, n_embd)\n",
        "\n",
        "        # Normalize and project to vocab space\n",
        "        normed_out = self.norm_final(blocks_out)\n",
        "        logits = self.out(normed_out)\n",
        "        return logits\n",
        "\n",
        "    def fit(self, train_loader, val_loader=None, epochs=5, lr=1e-3):\n",
        "        \"\"\"\n",
        "        Train the language model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): Training data.\n",
        "            val_loader (DataLoader, optional): Validation data.\n",
        "            epochs (int): Number of training epochs.\n",
        "            lr (float): Learning rate.\n",
        "        \"\"\"\n",
        "        self.to(self.device)\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=lr)\n",
        "        loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.train()\n",
        "            total_train_loss = 0.0\n",
        "\n",
        "            for X, Y in train_loader:\n",
        "                X, Y = X.to(self.device), Y.to(self.device)\n",
        "\n",
        "                logits = self(X).view(-1, self.vocab_size)\n",
        "                Y = Y.view(-1)\n",
        "                loss = loss_fn(logits, Y)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "            if val_loader is not None:\n",
        "                self.eval()\n",
        "                total_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for Xv, Yv in val_loader:\n",
        "                        Xv, Yv = Xv.to(self.device), Yv.to(self.device)\n",
        "                        val_logits = self(Xv).view(-1, self.vocab_size)\n",
        "                        Yv = Yv.view(-1)\n",
        "                        val_loss = loss_fn(val_logits, Yv)\n",
        "                        total_val_loss += val_loss.item()\n",
        "\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {avg_train_loss:.3f}\"\n",
        "                      f\"  |  Val Loss: {avg_val_loss:.3f}\")\n",
        "            else:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}]\"\n",
        "                      f\"  Train Loss: {avg_train_loss:.3f}\")\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=16, temperature=1.0, top_k=50):\n",
        "        \"\"\"\n",
        "        Generate text from a prompt using temperature and top-k sampling.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Prompt string to continue from.\n",
        "            max_new_tokens (int): Number of tokens to generate.\n",
        "            temperature (float): Controls randomness. Lower = more confident predictions.\n",
        "            top_k (int): Only sample from the top-k most likely next tokens.\n",
        "\n",
        "        Returns:\n",
        "            str: The generated text.\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        input_ids = torch.tensor([self.tokenizer.encode(prompt)], device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_new_tokens):\n",
        "                # Only keep the most recent tokens for context\n",
        "                input_ids_cond = input_ids[:, -self.context_len:]\n",
        "\n",
        "                logits = self(input_ids_cond)\n",
        "                logits = logits[:, -1, :]  # Focus only on the last token position, but get scores for all tokens in vocab!\n",
        "\n",
        "                # Scale logits using temperature\n",
        "                logits /= max(temperature, 1e-5) # max(t, 1e-5) ensures zero can never be provided\n",
        "\n",
        "                # Apply top-k sampling\n",
        "                if top_k > 0:\n",
        "                    top_k_values, _ = torch.topk(logits, k=top_k)\n",
        "                    min_top_k = top_k_values[:, -1].unsqueeze(-1)\n",
        "                    logits = torch.where(\n",
        "                        logits < min_top_k,\n",
        "                        torch.tensor(float('-inf'), device=self.device),\n",
        "                        logits\n",
        "                    )\n",
        "\n",
        "                # Convert to probabilities and sample next token\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        return self.tokenizer.decode(input_ids[0].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS4TQVj8KvOl"
      },
      "source": [
        "### **Decoding Strategies**\n",
        "\n",
        "Notice we have changed the way `.generate()` method operates. Now we have added the possibility to adjust how model picks up tokens from the probability distribution over the vocab! Here is what we have done, and some other strategies we could have implemented:\n",
        "\n",
        "**Decoding strategies** define how a language model selects the next token when generating text. While the model outputs a probability distribution over the entire vocabulary at each step, decoding methods determine *how* we sample from that distribution‚Äîbalancing between coherence and creativity. ‚ú®\n",
        "\n",
        "- üîÅ **Greedy decoding** (what we sort of had before) picks the token with the highest probability at each step. It is fast and simple, but often leads to repetitive or boring outputs because it always follows the most likely path, ignoring diversity.\n",
        "\n",
        "- üåê **Beam search** (not implemented, but very popular!) keeps multiple candidate sequences (called *beams*) active at once and selects the best overall sequence. It explores more possibilities than greedy decoding, producing more coherent results, but still tends to lack diversity and is slower.\n",
        "\n",
        "- üå°Ô∏è **Temperature scaling** adjusts the sharpness of the probability distribution before sampling. A **low temperature** (e.g. 0.7) makes the model more confident and deterministic. A **high temperature** (e.g. 1.5) makes the model more creative and random.\n",
        "\n",
        "- üéØ **Top-k sampling** limits sampling to the top *k* most likely tokens, ignoring the rest. This reduces the chance of sampling very unlikely words, keeping generations more plausible while still allowing for variety.\n",
        "\n",
        "Together, **temperature** and **top-k** give fine-grained control over generation: top-k narrows the field of possible tokens üéØ, while temperature controls how boldly the model explores within that field üå°Ô∏è. This makes it possible to generate outputs that are both coherent and engaging‚Äîperfect for storytelling, dialogue, and other creative tasks. ‚úçÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "6kTF5mvIIqro"
      },
      "outputs": [],
      "source": [
        "#@title New Config, separating Data and Model configurations\n",
        "\n",
        "# Main Configuration file, stores parameters that determine how we conduct training\n",
        "CONTEXT_LEN = 128\n",
        "\n",
        "data_config = {\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"context_len\": CONTEXT_LEN, # dataset context_len has to be smaller or equal to model_context len!\n",
        "    \"stride\": 64,\n",
        "    \"batch_size\": 64,\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    \"n_embd\": 128,\n",
        "    \"tokenizer\": bpe_tokenizer,\n",
        "    \"device\": \"cuda\",\n",
        "    \"context_len\": CONTEXT_LEN, # dataset context_len has to be smaller or equal to model_context len!\n",
        "    \"n_heads\": 16,\n",
        "    \"n_blocks\": 3, # we added number of transformer blocks into config!\n",
        "    \"dropout\": 0.1\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "cellView": "form",
        "id": "MwT7R-pwIn-T"
      },
      "outputs": [],
      "source": [
        "#@title Re-Instantiate Dataset and Dataloaders\n",
        "\n",
        "ds = CustomDataset(\n",
        "    text=cleaned_text,\n",
        "    tokenizer=data_config[\"tokenizer\"],\n",
        "    context_len=data_config[\"context_len\"],\n",
        "    stride=data_config[\"stride\"]\n",
        ")\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "dataset_size = len(ds)\n",
        "train_size = int(0.9 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=data_config[\"batch_size\"],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=data_config[\"batch_size\"],\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wllHUbyu2a1V",
        "outputId": "7fc73167-5585-4801-83e5-5bd17560fbbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15]  Train Loss: 6.236  |  Val Loss: 5.369\n",
            "Epoch [2/15]  Train Loss: 5.001  |  Val Loss: 4.777\n",
            "Epoch [3/15]  Train Loss: 4.468  |  Val Loss: 4.484\n",
            "Epoch [4/15]  Train Loss: 4.099  |  Val Loss: 4.314\n",
            "Epoch [5/15]  Train Loss: 3.811  |  Val Loss: 4.180\n",
            "Epoch [6/15]  Train Loss: 3.570  |  Val Loss: 4.049\n",
            "Epoch [7/15]  Train Loss: 3.353  |  Val Loss: 3.927\n",
            "Epoch [8/15]  Train Loss: 3.146  |  Val Loss: 3.809\n",
            "Epoch [9/15]  Train Loss: 2.949  |  Val Loss: 3.697\n",
            "Epoch [10/15]  Train Loss: 2.774  |  Val Loss: 3.596\n",
            "Epoch [11/15]  Train Loss: 2.600  |  Val Loss: 3.496\n",
            "Epoch [12/15]  Train Loss: 2.443  |  Val Loss: 3.380\n",
            "Epoch [13/15]  Train Loss: 2.295  |  Val Loss: 3.288\n",
            "Epoch [14/15]  Train Loss: 2.155  |  Val Loss: 3.189\n",
            "Epoch [15/15]  Train Loss: 2.038  |  Val Loss: 3.083\n"
          ]
        }
      ],
      "source": [
        "# runs for about 3 mins on T4\n",
        "model = LanguageModel(**model_config)\n",
        "model.fit(train_loader, val_loader, epochs=15, lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5AcnhlYI2HX",
        "outputId": "9a5fb8c1-93f9-4da1-d47d-f72f2cb60806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I went out with Lady Elizabeth to the meadows they are so soon\n",
            "grievance lay.‚Äù\n",
            "\n",
            "‚ÄúMy dear Mr. Crawford, and is very good\n",
            "thoroughly accuse me‚Äî‚Äù was a face at this family, all\n",
            "with _that_, and without_\n",
            "ask her; and Henry is all agitation and kind\n",
            "waiting. ‚ÄúYes, I remember that one time music. Mrs. Price is a fine young man in this family.‚Äù\n",
            "\n",
            "‚ÄúAn after a sad flirt V child have a Moor phear there is himself, you evertold_ she did not offer will\n",
            "nothing. If C\n"
          ]
        }
      ],
      "source": [
        "# be careful not to overextend max_new_tokens! Cannot be larger than model context_len!\n",
        "\n",
        "print(model.generate(\"I went out with Lady Elizabeth to the meadows\", max_new_tokens=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ-N6y1XMfwJ"
      },
      "source": [
        "üß† **Exercise**!\n",
        "\n",
        "\n",
        "- Now that you have a working language model, it is prime time to experiment! Try changing the model configuration parameters‚Äîfor example, increase the embedding size, number of attention heads (**`n_embd` must be divisible by `head_size`** though, think why!), or the number of transformer blocks. Can you improve your model's performance or make it more expressive?\n",
        "\n",
        "- Try training model for longer! But beware overfitting (which you notice when val loss stops decreasing. If it starts increasing: you are in big trouble!).\n",
        "\n",
        "- Play with the generation parameters like temperature and top_k. How does adjusting temperature affect the creativity or determinism of the output? What happens when you make `top_k` smaller or larger? These decoding strategies control how the model samples words, so they have a huge impact on the style and diversity of the generated text.\n",
        "\n",
        "There's no single ‚Äúbest‚Äù setup‚Äîfeel free to test, compare, and observe! What surprises you? What combinations lead to the most natural (or most bizzare) completions? Share them with a friend!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nY7K3luM-SW"
      },
      "outputs": [],
      "source": [
        "### hint: uncomment and run in case CUDA out of memory issue\n",
        "# del model\n",
        "# torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPD+Kht8RgfUu8Y8BYEkqZ/",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
