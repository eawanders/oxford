---
title: "Instrumental variables"
author: "Tanisha Mohapatra & Fernando Sanchez Monforte"
date: "Week 4, Hilary Term 2025"
output:
  pdf_document: null
  word_document: default
subtitle: Causal Inference, DPIR, University of Oxford
number_sections: no
---

<style> body {text-align: justify} </style> <!-- Justify text. -->
```{r, eval = T, echo = F}
show_solutions <- TRUE
knitr::opts_chunk$set(warning = F, message = F, error = F)
```


```{r Setup, echo=FALSE}
# Let's get started!

## Global options:
knitr::opts_chunk$set(
  cache = FALSE, # Set this to TRUE if chunks are performing time taking calculations and you want to use those calculations at a later stage. The first time chunks run, R will store the results of chunk in your disk and use this copy for all subsequent runs
  echo = TRUE, # If TRUE, code chunk is printed to the pdf
  warning = FALSE,
  message = FALSE
)

## We use pacman to install and load all packages we need:
# install.packages("pacman") # Uncomment if you don't have pacman installed yet!
library(pacman)
pacman::p_load(
  tidyverse, # Cleaning data, plotting pretty plots ;)
  stargazer, # Neat regression tables
  htmltools, # Format regression table for HTML
  kableExtra, # Yet another package for tables
  tableone, # This one is for summary/balance tables
  rgenoud, # Helps with optimization problems
  ggplot2, # Plots!
  sjPlot, # Other plots!
  foreign, # Read different file formats
  rmdformats, # Style the RMD HTML output
  lmtest, # Diagnose your linear regression models
  ivdesc, # Balance between compliance groups
  AER # for instrumental regression models
)
```
# Let's get started!


In this lab session, we will implement to concepts from this week's lecture\footnote{This session builds on materials kindly shared by Tom O'Grady, Jack Blumenau, and Julia de Romémont}.

# Introduction

In this lab session[^1] our focus will be on **instrumental variables** (IV) models - situations in which we don't have a random assignment of the treatment $D_i$, but we do have an as-if random variable $Z_i$, also known as an instrument. It is assumed to affect the treatment status $D_i$, but not the outcome $Y_i$, thanks to which we can still estimate the effects of our treatment of interest (as you already know from the lecture!). In the exercises below we will implement and validate instrumental regression models. In addition to coding, we will also do some brain-storming around the IV research design. In fact, doing independent research efficiently might be more about that than the coding per se.

We will use two examples of IV models In the first one, we will look at the literacy effect of encouraging children to watch the Sesame Street TV show. Here we will see one of two key uses of IV designs - accounting for non-compliance with binary treatment assignment. Specifically we will:

1. Explore the proportions of different compliance groups and their characteristics
2. Calculate ITT and LATE (Wald estimator)
3. Discuss the assumptions needed for the above calculations

In the second example, we will generalize IV approach by using 2SLS (two-stage least squares) models to estimate the electoral backlash against the wind turbines in Canada, instrumenting wind turbine construction with continuous measure of local wind speeds. We will:

1. Calculate first- and second-stage 2SLS equations
2. Estimate F-statistic
3. Implement 2SLS models controlling for covariates
4. Learn to use `ivreg` function from the `AER` package


**Before starting this seminar**

1.  Create a folder called `lab4`.
2.  Download the data (`sesame_experiment.dta` and `Stokes.Rda`) from [Canvas](https://canvas.ox.ac.uk/courses/226614/modules)).
3.  Save the data in our `lab4` folder.
4.  Open the RMarkdown file (either download it from Canvas or start your own).
5.  Set your working directory using the `setwd()` function or by clicking on \"More\". For example, this may look like this: `setwd(\"\~/Desktop/Causal Inference/2024/lab4\")`.
6.  Prepare your R environment running the below.




# Exercise 1. Sesame Street Experiment -- [Gladwell (2002)](https://www.amazon.co.uk/Tipping-Point-Little-Things-Difference/dp/0349113467) [^2]

</center>

Can educational television programmes improve children's learning outcomes? Sesame Street is an American television programme aimed at young children. The creators of Sesame Street decided from the very beginning of the show's production that a central goal would to be educate as well as entertain its audience. As [Malcolm Gladwell](https://www.amazon.co.uk/Tipping-Point-Little-Things-Difference/dp/0349113467) argued, "Sesame Street was built around a single, breakthrough insight: that if you can hold the attention of children, you can educate them". In addition to building the show around a carefully constructed educational curriculum, the show's producers also worked closely with educational researchers to determine whether the show's content was effectively improving its young viewers' numeracy and literacy skills.

The dataset contained in `sesame_experiment.dta` includes information on **240 children** who were randomly assigned to two groups. The treatment of interest here is watching Sesame Street, but clearly it is not possible to force children to watch a TV show or (perhaps even harder) to refrain from watching, and so **watching** the show cannot be randomized. Instead, in this study, researchers randomized whether children were **encouraged** to watch the show. More specifically, when the study was run in the 1970s, Sesame Street was on the air each day between 9am and 10am. The parents of children in the treatment group were encouraged to show Sesame Street to their children on a regular basis, while parents of the children in the control group were given no such encouragement. Because it is only encouragement that is randomized here, there is the possiblity of non-compliance -- i.e. some children will not watch Sesame Street even though they are in the treatment condition, and some children will watch Sesame Street even though they are in the control condition. The list of variables in the dataset we are going to use is provided below.


Table: Variables of interest in the 'Sesame Street experiment' dataset from [Gladwell](https://www.amazon.co.uk/Tipping-Point-Little-Things-Difference/dp/0349113467)

| **Variable**    | **Description**  |
|----------|--------------------------------------------------------------|
| `encour` | 1 if the child was encouraged to watch Sesame Street, 0 otherwise |
| `watched` | 1 if the child watched Sesame Street regularly, 0 otherwise |
| `letters` | the score of the child on a literacy test |
| `age` | age of the child (in months) |
| `female` | 1 if the child is female, 0 otherwise |


Load and explore the data first.

```{r, echo=T, warning=FALSE, message=FALSE,eval=T}
sesame <- read.dta("sesame_experiment.dta")
glimpse(sesame)
```

> Task 1.1. *(no coding)* In the context of this specific example, define the following unit types:
>     1. Compliers
>     2. Always-takers
>     3. Never-takers
>     4. Defiers




> Task 1.2. Calculate the proportion of children in the treatment group who did not watch Sesame Street. Calculate the proportion of children in the control group who did watch Sesame Street. What type of non-compliance occured in this experiment? What are the age and gender means across different sub-groups?


> \mdseries <small>*Hint*: You might find the `table()` and `prop.table()` functions helpful here.


```{r, results='asis', echo=show_solutions, eval=show_solutions}
# counts in each assignment/treatment group
table(sesame$encouraged, sesame$watched)

# proportions in each assignment/treatment group
prop.table(table(sesame$encouraged, sesame$watched), 1)
```

Out of the 88 children assigned to the control condition, 48 actually watched Sesame Street.
Out of the 152 children assigned to the treatment condition, 14 did not watch Sesame Street.

In addition to the fact that clearly Sesame Street was a very popular programme in the 1970s, this analysis tells us that we have two-sided non-compliance in this experiment. A number of treated units failed to take the assigned treatment, and a number of units took the treatment even though they were assigned to the control group.





Now we are gonna get the values of covariates for compliers, never-takers (NT group) and always-takers (AT). Deriving values for NT and AT is relatively straightforward since we know exactly who is in that group - respectively people with `1` for `encouraged` and `0` for `watched` variables (NT) and with `0` for `encouraged` and `1` for `watched`  (AT). But what about the compliers? That's described in greater detail in Marbach and Hangartner (2020) [Profiling Compliers and Noncompliers for Instrumental-Variable Analysis](doi:10.1017/pan.2019.48). Below, we present how to derive mean value of age for compliers in our experiment, based on the following formulas, where $N$ is the sample size, $K_{nt}$ / $\hat{\pi_{nt}}$ the number / proportion of observed never-takers, $K_{at}$  / $\hat{\pi_{at}}$ the number  / proportion of observed always-takers,  $\hat{\pi_{co}}$ the observec proportion of compliets, and $x_{i}$ is the value of covariate $x$ for unit $i$.


$\hat{\mu} = \frac{1}{N}\sum_{i=1}^{N}{x_i}$

$\hat{\mu_{nt}} = \frac{1}{K_{nt}}\sum_{i=1}^{K_{nt}}{x_i}$

$\hat{\mu_{at}} = \frac{1}{K_{at}}\sum_{i=1}^{K_{at}}{x_i}$

We also know that $\hat{\mu}$ can be expressed as weighted mean of all three groups, so:

$\hat{\mu} = \hat{\pi_{co}}\hat{\mu_{co}} + \hat{\pi_{nt}}\hat{\mu_{nt}} + \hat{\pi_{at}}\hat{\mu_{at}}$

After re-arranging to have just $\hat{\mu_{co}}$ on the left-hand side we get:

$\hat{\mu_{co}} = \frac{\hat{\mu}}{\hat{\pi_{co}}} - \frac{\hat{\pi_{nt}}}{\hat{\pi_{co}}} \hat{\mu_{nt}}  - \frac{\hat{\pi_{at}}}{\hat{\pi_{co}}} \hat{\mu_{at}}$

Let's calculate the relevant values in R:

```{r, echo=T,eval=T}
# demographic differences between complinace groups?

library(dplyr)
library(kableExtra)

# Define compliance types based on Z (encouragement) and D (treatment)
sesame <- sesame %>%
  mutate(
    compliance_type = case_when(
      encouraged == 0 & watched == 1 ~ "Always-Taker",
      encouraged == 1 & watched == 0 ~ "Never-Taker"
    )
  )

# Compute summary statistics for each compliance type
age_diff <- sesame %>%
  group_by(compliance_type) %>%
  summarise(
    Age_Mean = mean(age, na.rm = TRUE),
    Age_SD = sd(age, na.rm = TRUE),
    Encouraged_Mean = mean(encouraged, na.rm = TRUE),
    Encouraged_SD = sd(encouraged, na.rm = TRUE),
    Watched_Mean = mean(watched, na.rm = TRUE),
    Watched_SD = sd(watched, na.rm = TRUE),
    n = n()
  )

# Format output
kable(age_diff, booktabs = TRUE) %>%
  kable_paper()

# Compute summary statistics for each compliance type
gender_diff <- sesame %>%
  group_by(compliance_type) %>%
  summarise(
    Gender_Mean = mean(female, na.rm = TRUE),
    Gender_SD = sd(female, na.rm = TRUE),
    Encouraged_Mean = mean(encouraged, na.rm = TRUE),
    Encouraged_SD = sd(encouraged, na.rm = TRUE),
    Watched_Mean = mean(watched, na.rm = TRUE),
    Watched_SD = sd(watched, na.rm = TRUE),
    n = n()
  )

# Format output
kable(gender_diff, booktabs = TRUE) %>%
  kable_paper()
# How to derive the mean for compliers?

# Get share of compliers (co), never-takers (NT) and always-takers (AT)
pi_co <- mean(sesame$watched[sesame$encouraged == 1] == 1) -
  mean(sesame$watched[sesame$encouraged == 0] == 1)
pi_nt <- mean(sesame$watched[sesame$encouraged == 1] == 0)
pi_at <- mean(sesame$watched[sesame$encouraged == 0] == 1)

# Get overall age mean, as well as means for NT and AT
mua <- mean(sesame$age)
mua_nt <- mean(sesame$age[sesame$encouraged == 1 & sesame$watched == 0])
mua_at <- mean(sesame$age[sesame$encouraged == 0 & sesame$watched == 1])

# Calculate mean for compliers
mua_co <- (mua / pi_co) - (pi_nt / pi_co) * mua_nt - (pi_at / pi_co) * mua_at
mua_co

# Get overall gender mean, as well as means for NT and AT
mug <- mean(sesame$female)
mug_nt <- mean(sesame$female[sesame$encouraged == 1 & sesame$watched == 0])
mug_at <- mean(sesame$female[sesame$encouraged == 0 & sesame$watched == 1])

# Calculate mean for compliers
mug_co <- (mug / pi_co) - (pi_nt / pi_co) * mug_nt - (pi_at / pi_co) * mug_at
mug_co
```

Children who comply with the encouragement are somewhat younger (50.1 months) than either never-takers (52.7 months) or always-takers (52.3 months). A smaller fraction of compliers are female (49%) than is true for always-takers (52%) or never-takers (64%).


> Task 1.3 Calculate the proportion of compliers in this experiment. Which assumptions are required for us to identify this quantity?


> \mdseries <small>*Hint*: We can calculate the proportion of compliers via $E[D_i|Z_i = 1] - E[D_i|Z_i = 0] = \bar{D}_{Z_i = 1} - \bar{D}_{Z_i = 0}$</small>

```{r,echo=show_solutions, eval=show_solutions}
d_z_1 <- mean(sesame$watched[sesame$encouraged == 1])
d_z_0 <- mean(sesame$watched[sesame$encouraged == 0])
proportion_compliers <- d_z_1 - d_z_0
proportion_compliers

# OR use OLS regression (= first stage regression)
coef(lm(watched ~ encouraged, data = sesame))
```


Roughly 36% of respondents in the sample are compliers.


What about the assumptions needed? Why exactly we do we need those? Do you think they hold in our case?

  1. **No defiers (monotonicity)** -- we have to rule out any defiers from the sample
  2. **Independence of the instrument** -- we assume that the instrument ($Z_i$, whether a child was encouraged to watch Sesame Street) is randomly assigned.


This assumption allows us to infer that the proportion of always-takers in the control group (something that is observable) is equal to the proportion of always-takers in the treatment group (something that is not observable).


> Task 1.4. Calculate the Intention-to-Treat effect (ITT). Is it substantive? What about statistical significance? Ultimately, how would you interpret ITT here?

> \mdseries <small>*Hint*: We can calculate the ITT via $E[Y_i|Z_i = 1] - E[Y_i|Z_i = 0] = \bar{Y}_{Z_i = 1} - \bar{Y}_{Z_i = 0}$</small>


```{r, echo=show_solutions,eval=show_solutions}
# Using the difference in means:
y_z_1 <- mean(sesame$letters[sesame$encouraged == 1])
y_z_0 <- mean(sesame$letters[sesame$encouraged == 0])
itt1_1 <- y_z_1 - y_z_0
itt1_1

# Using OLS regression (= reduced form regression)
model1 <- lm(letters ~ encouraged, data = sesame)
summary(model1)

itt1_2 <- summary(model1)$coef[2, 1]
p1_2 <- summary(model1)$coef[2, 4]


# Substantive effect? Find out in terms of standard deviations
itt1_scaled <- itt1_2 / sd(sesame$letters)
itt1_scaled
```


The ITT estimate is equal to `r itt1_1 %>% round(3)`

The ITT estimates the causal effect of *treatment assignment* on the outcome of interest. Here, the ITT estimates the causal effect of being encouraged to watch Sesame Street on a child's score on the literacy test. The estimate of `r itt1_1 %>% round(3)` implies that this encouragement increases literacy scores by nearly `r itt1_1 %>% ceiling` points, on average. Note however that this effect is not very precisely estimated (p = `r p1_2 %>% round(3)`) and does not represent a substantively large effect - `r itt1_scaled %>% round(3)` standard deviations.



> Task 1.5. Now calculate the local average treatment effect (LATE). What does the LATE estimate? Estimate the LATE for this example.:

> \mdseries <small>*Hint*: Use Wald estimator, manual 2SLS using `lm`, or `ivreg` from the `AER` package (the latter two will be discussed in greater detail in Exercise 2) </small>



```{r,echo=show_solutions,eval=show_solutions}
## LATE using Wald Estimator
late1_1 <- itt1_1 / proportion_compliers
```
The LATE estimate our example is equal to `r late1_1 %>% round(3)`


The LATE estimates the average effect of the treatment on the outcome for those units in the sample who complied with the encouragement.
In our example, LATE estimate tells us that the causal effect of watching Sesame Street for those children who complied with the encouragement increases a child's score on the literacy test by nearly `r late1_1 %>% ceiling` points, on average.


> Task 1.6. *(no coding)*  In addition to 1) monoticity and 2) independence of the instrument assumptions, there are 2 other assumptions that we need to make to claim the estimates are causal. What are those? Do you think they are satisfied?

  3. **Exclusion restriction** -- treatment assignment only influences outcomes by affecting the *treatment received*, otherwise ITT might be biased [^3]

  4. **Relevance (strong instruments)** -- instrument must be effective, i.e. leads to significant uptake of treatment by the assigned units, otherwise the proportion of compliers might inflate the LATE estimate; we will re-visit that later on when calculating F-statistic


The exclusion restriction states that the instrument, Z, can only affect the outcome, Y, through its affect on the treatment, D. Here, this implies that for those children whose behaviour would not have been changed by the encouragement (i.e. never-takers and always-takers), there can be no effect of the encouragement on outcomes. In other words, there is no effect of encouragement on learning outcomes aside from when encouragement successfully prompts children to watch Sesame Street. It seems likely that the exclusion restriction is a reasonable assumption in this setting. If the parents of a child are encouraged to sit their child in front of Sesame Street, it is difficult to think of a way that that assignment might affect their child's literacy skills other than if they actually comply with the treatment.

The relevance assumption is perhaps not as clear-cut as in some other settings (>50% of control group still receives the treatment), but still the treatment uptake is almost double as a result of the encouragement


> Task 1.7. *(no coding)* You have now estimated two treatment effects: the ITT and LATE. Which is of greater interest to the TV show's producers?


The LATE seems like a much more valuable quantity of interest to the producers of Sesame Street than the ITT. Because the ITT combines information on both the extent to which the treatment was adhered to by the respondents, and the effect of the treatment itself on the outcome, it obscures clear conclusions about the effectiveness of Sesame Street as an educational programme.

By contrast, the LATE gives a very clear answer: it tells us that, for those children who complied with the encouragement to watch or not watch Sesame Street, the causal effect of watching the programme was to increase their literacy skills by `r late1_1 %>% ceiling` points on average. From the point of view of the TV producers, this is helpful information as it directly informs them about the educational impact of their show on those who watch. Of course, it may be the case that the compliers in this example are very different from the always-takers or never-takers, and so the *generalizability* of this result cannot be established from this single experiment.




# Exercise 2. Electoral Backlash against Climate Policy -- [Stokes (2016)](doi.org/10.1111/ajps.12220) [^4]

</center>

Leah Stokes (2016) studies whether governments are punished at the ballot box for building wind farms. Construction of such projects is a policy that can help to mitigate climate change but it might also impose costs on the communities where turbines are sited. She looks at Ontario in Canada, where from 2009 the centre-left provincial government removed local communities' right to make planning decisions on the building of wind turbines. Instead, decision-making was centralised and turbines were imposed by the government. It built turbines in places where they would generate the most electricity: in places with higher prevailing wind speeds. In general certain broad areas are better-suited for turbines (rural and elevated places, and areas closer to the windy Great Lakes), Stokes argues that within these areas wind speed varies at random at the local level. Local areas with high wind speeds should not be more supportive of the government than local areas with low wind speeds. This is therefore a natural experiment where wind speed is an instrument that randomly encouraged the government to site turbines in particular places. Her outcome of interest is change in support for the incumbent government from 2007 (before the wind farm policy) to 2011 (after it began) at a highly localised level known as precincts in Canada, which typically contain around 300 voters. Using GIS software, she geo-located all wind turbines that were built or proposed in the period and matched them to precincts, where she collected voting data, localised prevailing wind speeds, and background covariates. The dataset for this question is contained in `Stokes.Rda` and contains the following variables.


Table: Variables of interest from [Stokes 2016](doi.org/10.1111/ajps.12220)

| **Variable**    | **Description**  |
|----------|--------------------------------------------------------------|
| `chng_lb` | **outcome** - percentage point change in support for the incumbent government, 2007-2011|
| `prop_3km` | **treatment** - dummy indicating whether a wind turbine was built or proposed within 3km (1), or not (0)
| `avg_pwr_log` | **instrument** - prevailing wind speed in the precinct, logged |
| `longitude` | geographical longitude (East-West) of the precinct |
| `latitude` | geographical latitude (South-North) of the precinct |
| `ed_id` | the broader district within which the precinct is located |
| `mindistlake` | distance to the Great Lakes in km |
| `mindistlake_sq` | distance to the Great Lakes in km, squared |



```{r, echo=T}
load("Stokes.Rda")
stokes <- s # rename the dataset
rm(s)
```


> Task 2.1. Assess whether wind speed can be considered to be as-if randomly assigned geographically,
by regressing `avg_pwr_log` on all of the geographical covariates. What do you conclude?

> \mdseries <small>*Hint*: Remember to use factor() for the `ed_id` variable </small>


```{r, results='asis', echo=show_solutions,eval=show_solutions}
model21 <- lm(avg_pwr_log ~ factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq,
  data = stokes
)


stargazer(model21,
  type = "latex",
  dep.var.labels = "",
  dep.var.caption = "Preccint-average wind speed (log)",
  omit = c("Constant", "ed_id"),
  omit.stat = c("ser", "f"),
  title = "Instrument randomization"
)
```

There is a lot of imbalance in terms of geographical covariates. There are statistically significant relationships between `avg_pwr_log` and `longitude`, `mindistlake`, `mindistlak_sq` as well as many of the districts. Distance to the Great Lakes is negatively related to wind speed (areas closer to the Lakes are windier) and longitude is positively related to wind speed (more westerly areas are windier). This is not particularly surprising. It is bound to be the case that different areas of Ontario are naturally windier than others.


> Task 2.2. Estimate the first-stage relationship between `prop_3km` and `avg_pwr_log` using a regression
with no added covariates. Interpret the results.

```{r,results='asis',echo=show_solutions,eval=show_solutions}
model22 <- lm(prop_3km ~ avg_pwr_log,
  data = stokes
)

stargazer(model22,
  type = "latex",
  dep.var.labels = "",
  dep.var.caption = "Wind turbine (within 3km)",
  omit.stat = c("ser", "f"),
  title = "First-stage model"
)
```

There appears to be a fairly strong relationship between the instrument and the treatment. A 1% increase in wind speed is estimated to lead to a 0.01-point increase in the probability of a turbine being proposed, an effect which is highly significant at all conventional levels (note that the dependent variable is binary and the independent variable is logged, and for logged variables with OLS regression coefficient $\beta$, a 1% increase in the variable leads to a change of $\frac{\beta}{100}$ in the dependent variable).


> Task 2.3. *(no coding)* Ok, is that all we need for the first stage? What about all the other covariates in the dataset? In fact, Stokes (2016) does use geographic controls for both first and second stage models. Why do you think that is? Even if we do that, do you think we control for every relevant variable?

Well, instrumental variables analysis relies on different values of the instrument being randomly assigned based on potential outcomes. Our answer to (a) suggested that this assumption may be violated, since more westerly areas and places closer to the great lakes are windier. Those places may have different potential outcomes than others. Perhaps, for example, communities nearer the Great Lakes are more rural, and therefore more conservative and naturally more opposed to a left-of-centre government. However, within the smaller geographic areas defined by district, latitude, longitude and proximity to the Great Lakes, it is very likely that windspeed is randomly assigned. Thus she controls for the geographic covariates because it ensures that the randomisation assumptions is more likely to hold.


> Task 2.4. Given the discussion above, re-estimate the first-stage relationship between `prop_3km` and `avg_pwr_log`, this time with a full set of geographic controls. Fully interpret the results. Does it differ from your results in Task 2.2 above?

```{r, results = 'asis', echo=show_solutions, eval=show_solutions}
model24 <- lm(
  prop_3km ~ factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq +
    avg_pwr_log,
  data = stokes
)

stargazer(model22, model24,
  type = "latex",
  dep.var.labels = "",
  dep.var.caption = "Wind turbine (within 3km)",
  omit = c("Constant", "ed_id"),
  omit.stat = c("ser", "f"),
  title = "First-stage model with covariates"
)
```

The relationship remains strong, with a 1% increase in wind speed estimated to lead to a 0.76% increase in the probability of a turbine being proposed. This is slightly weaker than the first-stage effect estimated without controls, because controlling for the geographic covariates means that we are focusing only on variation in wind speed within small geographic areas. Some of the estimated effect of wind speed on turbine construction in (2.2) merely reflected that high-wind areas may have also other characteristics (e.g., rural) making them more suitable for wind-farm construction.



> Task 2.5. Conduct an F-test for the strength of the `avg_pwr_log` instrument.

> \mdseries <small>*Hint*: use the function `waldtest` in the `lmtest` library. Your code should take the form `waldtest(model1,model2)` , where model1 and model2 are the names of estimated regression models with and without the instrument </small>


```{r,echo=show_solutions,eval=show_solutions}
model251 <- lm(prop_3km ~ factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq,
  data = stokes
)

model252 <- lm(
  prop_3km ~ factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq +
    avg_pwr_log,
  data = stokes
)

waldtest(model251, model252)
```
The F test produces an F-sstatistic of 70.7. This is much larger than the benchmark value of 10 below which an instrument is considered to be unacceptably weak.

What is this value anyway? F-statistic is based on the following formula:

$F = \frac{(RSS_1 - RSS_2) / (p_1 - p_2)}{RSS_2 / (n - p_2 - 1)}$

This can be re-arranged to:

$F = \frac{(RSS_1 - RSS_2)}{(RSS_2)} * \frac{n-p_2}{p_2 - p_1}$

where $p_1$ is the number of parameters in model 1 (restricted) and $p_2$ the number of parameters in model 2 (unrestricted), $n$ is the number of data points, and $RSS$ refers to residual sum of squares in a given model

Given the above, we might also re-calculate F-statistics manually. How?

> \mdseries <small>*Hint*: Applying `anova` function on your model will give you several summary statistics, including the residual sum of squares, which you can then save as separate value </small>


```{r,echo=show_solutions,eval=show_solutions}
# Get RSS of both models

rss1 <- anova(model251)["Residuals", "Sum Sq"]
rss2 <- anova(model252)["Residuals", "Sum Sq"]

# Number of data points
n <- nrow(stokes)

# Number of parameters
p1 <- 29 # number of predictor variables in model251 (counting each factor separately, but without reference category), so: n_distinct(stokes$ed_id) - 1
p2 <- 30

# Implement the formula above
first <- (rss1 - rss2) / rss2
second <- (n - p2) / (p2 - p1)

first * second
```

So, knowing all the above - is the instrument relevant? F-statistics strongly suggests so. The first-stage regression in (d) also showed a reasonably strong and statistically significant relationship between the treatment and the instrument. Together, these pieces of evidence suggest that the relevance assumption is clearly met here.



> Task 2.6. Knowing the above, try estimating the Local Average Treatment Effect using two-stage least squares specifying both the first and second stage equations (with covariates). How would interpret the results?


> \mdseries <small>*Hint*: Extract fitted values from the first stage using `fitted.values()` and use them as explanatory variable in the second stage regression.
 </small>



```{r, results='asis', echo=show_solutions,eval=show_solutions}
first_stage <- lm(prop_3km ~ avg_pwr_log + factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq,
  data = stokes
)


second_stage <- lm(chng_lib ~ fitted.values(first_stage) + factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq, data = stokes)

stargazer(second_stage,
  type = "latex",
  dep.var.labels = "",
  dep.var.caption = "Wind turbine (within 3km)",
  omit = c("Constant", "ed_id"),
  omit.stat = c("ser", "f"),
  title = "Second-stage"
)
```


The estimated Local Average Treatment Effect of turbine construction on voting is that proposing to build a turbine in a precinct led to a 7.4 percentage-point fall in the incumbent party's vote share in that precinct. The p-value indicates that the effect is statistically significant at the 1% level.


> Task 2.7. Now estimate the Local Average Treatment Effect of `prop_3km`  on `chng_lib` using two-stage least squares with `avg_pwr_log` as the instrument and the full set of geographic controls. Interpret the coeffcient on `prop_3km` and its statistical significance precisely.

> \mdseries <small>*Hint*: Use `ivreg` in the AER library. Your code should take the form: `ivreg(outcome ~ treatment + covariates | instrument + covariates)` </small>


```{r,results='asis',echo=show_solutions,eval=show_solutions}
model27 <- ivreg(
  chng_lib ~ prop_3km + factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq |
    avg_pwr_log + factor(ed_id) + longitude + latitude + mindistlake + mindistlake_sq,
  data = stokes
)

stargazer(second_stage, model27,
  type = "latex",
  dep.var.labels = "",
  dep.var.caption = "Wind turbine (within 3km)",
  omit = c("Constant", "ed_id"),
  omit.stat = c("ser", "f"),
  title = "2SLS model (build-in function vs. manual estimation)"
)
```

The estimated coefficient is identical to the one obtained previously (as expected). The standard error from doing 2SLS manually is slightly biased, however. This is why, in practice in-built functions, like the `ivreg` function, are used to properly correct for this bias.

> Task 2.8. *(no coding)* What about other assumptions behind instrumental variables - the independence of the instrument and exclusion restriction?

**Independence of the instrument** could be violated by residential sorting based on wind speed. For instance, if people with different potential outcomes (perhaps people who are already predisposed to dislike the government) choose to live in windier areas, then the instrument would not be randomly assigned. However, this seems very unlikely to be a serious threat to internal validity. Because the paper focuses on very small geographic areas, it seems implausible that there would be systematic differences between people in low- or high-wind places within those areas (e.g., between people living on top of a hill and people living at the bottom).

The **exclusion restriction** means that the only way that local wind speed affects voting is through its impact on the probability of a turbine being built. This seems highly plausible. it is unlikely that living in a high-wind place would, on its own, turn people against the government... (or would it?)




[^1]: This practical session is building on materials kindly shared by Tom O'Grady and Jack Blumenau. The design is inspired by Marie-Lou Sohnius.

[^2]: Gladwell M. (2002). [The Tipping Point: How Little Things Can Make a Big Difference
]https://www.amazon.co.uk/Tipping-Point-Little-Things-Difference/dp/0349113467)." *Abacus*

[^3]: Good explanation of the logic behind the exclusion restriction is provided in Cunningham S. (2021) "Causal Inference: The Mixtape" (pp. 321-323): *A necessary but not sufficient condition for having an instrument that can satisfy the exclusion restriction is if people are confused when you tell them about the instrument’s relationship to the outcome. Let me explain. No one is likely to be confused when you tell them that you think family size will reduce the labor supply of women. They don’t need a Becker model to convince them that women who have more children probably are employed outside the home less often than those with fewer children. But what would they think if you told them that mothers whose first two children were the same gender were employed outside the home less than those whose two children had a balanced sex ratio? They would probably be confused because, after all, what does the gender composition of one’s first two children have to do with whether a woman works outside the home? That’s a head scratcher. And there you see the characteristics of a good instrument. It’s weird to a lay person because a good instrument (two boys) only changes the outcome by first changing some endogenous treatment variable (family size) [added clarification: due to common preference of parents for mixed-gender offspring] thus allowing us to identify the causal effect of family size on some outcome (labor supply). And so without knowledge of the endogenous variable, relationships between the instrument and the outcome don’t make much sense. Why? Because the instrument is irrelevant to the determinants of the outcome except for its effect on the endogenous treatment variable. Ultimately, good instruments are jarring precisely because of the exclusion restriction— these two things (gender composition and work) don’t seem to go together. If they did go together, it would likely mean that the exclusion restriction was violated. But if they don’t, then the person is confused, and that is at minimum a possible candidate for a good instrument.*


[^4]: Stokes, L. (2016). [Electoral Backlash against Climate Policy: A Natural Experiment on Retrospective Voting and Local Resistance to Public Policy](doi.org/10.1111/ajps.12220)." *American Journal of Political Science*, 60 (4), pp. 958-974
