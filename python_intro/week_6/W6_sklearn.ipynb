{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMwrxKwN1Iy2bkHYymd4Cr8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antndlcrx/Intro-to-Python-DPIR/blob/main/Week%206/W6_sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://cdn.githubraw.com/antndlcrx/Intro-to-Python-DPIR/main/images/logo_dpir.png?raw=true:,  width=35\" alt=\"My Image\" width=175>  \n",
        "\n",
        "# Scikit Learn for Machine Learning"
      ],
      "metadata": {
        "id": "npeAYTFNuw8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1**.&nbsp; Why Scikit-Learn?\n",
        "\n",
        "<img src=\"https://cdn.githubraw.com/antndlcrx/Intro-to-Python-DPIR/main/images/W6/Scikit_learn_logo_small.png?raw=true:,  width=25\" alt=\"My Image\" width=175>\n",
        "\n",
        "Scikit-learn is one of the most widely used Python libraries for machine learning. It offers a clean, consistent API that simplifies tasks like data preprocessing, model selection, and evaluation. Scikit-learn supports a broad range of algorithms—classification, regression, clustering, and more—and integrates seamlessly with libraries like NumPy and pandas. Last but not least, it is a very well-documented and well-maintained library.\n",
        "\n",
        "See:\n",
        "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html#user-guide).\n",
        "- [Examples](https://scikit-learn.org/stable/auto_examples/index.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "wO8TUcPcu3xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A Reminder on Machine Learning\n",
        "\n",
        "[**Supervised Learning**](https://scikit-learn.org/stable/supervised_learning.html)  \n",
        "Supervised learning algorithms learn from labeled data where each sample has a known target output. They seek to predict or categorize new data based on patterns learned from these labels.  \n",
        "- **Classification** focuses on predicting discrete categories or classes (e.g., spam vs. not spam).  \n",
        "- **Regression** predicts continuous values (e.g., house prices).\n",
        "\n",
        "[**Unsupervised Learning**](https://scikit-learn.org/stable/unsupervised_learning.html)\n",
        "\n",
        "Unsupervised learning deals with unlabeled data. The algorithms aim to discover hidden structures or patterns without predefined targets.  \n",
        "- **Clustering** groups similar samples together (e.g., grouping customers by purchasing behavior).  \n",
        "- **Dimensionality Reduction** simplifies data by reducing its number of features while retaining important information (e.g., projecting data for visualisation in fewer dimensions).\n",
        "\n",
        "**Semi-Supervised Learning**  \n",
        "Semi-supervised learning combines both labeled and unlabeled data. The idea is to leverage a small amount of labeled data alongside larger amounts of unlabeled data to improve learning accuracy or reveal additional patterns when obtaining labels for every data point is costly or time-consuming.\n"
      ],
      "metadata": {
        "id": "jV26hxlVu8GP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Representation Glossary in Scilit-Learn"
      ],
      "metadata": {
        "id": "Ia4nKeL-weWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/antndlcrx/Intro-to-Python-DPIR.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZvo_LGuw2Z0",
        "outputId": "7caccfb5-5796-4fa6-872c-f998873ed69b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Intro-to-Python-DPIR'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (107/107), done.\u001b[K\n",
            "remote: Total 124 (delta 42), reused 58 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (124/124), 2.97 MiB | 9.93 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "mJOxP7aRw9D_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qog_link = '/content/Intro-to-Python-DPIR/datasets/qog2022.csv'\n",
        "qog = pd.read_csv(qog_link)"
      ],
      "metadata": {
        "id": "kEXJoqrWw5bU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ML, you might find people use sligtly different terminology to what you were used to in statistics. Variables are called **features** (your columns in the dataframe, although feature extends its use beyond 2d data). **Example** or **sample** is how people refer to an individual data point (your dataset row). The data table containing information on all features for all examples is **feature matrix**.\n",
        "The feature we are predicting is called **target** (your dependent variable).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cD_q6SkyxJJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make pairplot"
      ],
      "metadata": {
        "id": "xMZY-w3AyLJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2**.&nbsp; **Estimator API**"
      ],
      "metadata": {
        "id": "L_H0MXyKzBjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scikit-learn provides a unified interface called the Estimator API. At its core, every algorithm in Scikit-learn is implemented as a class with a consistent set of methods—primarily `fit()`, `predict()`, and for some models, `transform()`. Here's the typical workflow:\n",
        "\n",
        "\n",
        "1. Choose a Model Class and Import It.\n",
        "2. Instantiate the Model with Desired Hyperparameters.\n",
        "3. Arrange Data into a Feature Matrix (X) and a Target Vector (y).\n",
        "    - X is usually a 2d array of shape (`n_samples`, `n_features`)\n",
        "4. Fit the Model to Your Training Data.\n",
        "5. Apply the Model to New Data.\n",
        "    - predict for supervised learning\n",
        "    - transform or predict for unsupervised\n",
        "\n",
        "Estimator API gives you consistency, ease of use, clarity of code.\n"
      ],
      "metadata": {
        "id": "k3_udM5pzHVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3**.&nbsp; **Regression**"
      ],
      "metadata": {
        "id": "VxVFPUqB8-F1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vQCm8StwqnBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Implementation Example\n",
        "\n",
        "# 1 choose model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# instantiate model\n",
        "# mod = LinearRegression()\n",
        "mod = KNeighborsRegressor(n_neighbors=3)\n",
        "\n",
        "# arrange data\n",
        "X = qog.drop([\"hdi\", \"country\", \"region\", \"iso3c\", \"fh_status\"], axis=1)\n",
        "y = qog[\"hdi\"]\n",
        "\n",
        "X = X.fillna(X.mean())\n",
        "# X = X.values.reshape(-1,1)\n",
        "y = y.fillna(y.mean())\n",
        "\n",
        "# fit model\n",
        "mod.fit(X, y)\n",
        "# predict\n",
        "preds = mod.predict(X)\n",
        "\n",
        "# eval\n",
        "sns.scatterplot(x=preds, y=y);\n",
        "\n",
        "# coeffs\n",
        "# mod.coef_\n",
        "# mod.intercept_"
      ],
      "metadata": {
        "id": "A9p3C1U81Y4g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This fit command causes a number of model-dependent internal computations to take place, and the results of these computations are stored in model-specific attributes that the user can explore. In Scikit-Learn, by convention all model parameters that were learned during the fit process have trailing underscores."
      ],
      "metadata": {
        "id": "YLCS0FTY6nd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn does not give you uncertainty estimates for model parameters. This is because the focus of the library is on prediciton: interpreting model parameters is much more a statistical modeling question than a machine learning question. If you need the statistical modelling functionality, refer to the [statmodels library](https://www.statsmodels.org/stable/index.html)."
      ],
      "metadata": {
        "id": "ly1-SLmz683d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4**.&nbsp; **Data Splitting**\n",
        "\n",
        "A critical step in building and assessing machine learning models is to split your available dataset into different subsets for training and evaluation. Typically, you create a training set (used to fit the model) and a test set (used to evaluate how well the model generalises to unseen data). This approach helps you detect overfitting: if the model performs well on training data but poorly on the test set, it suggests your model has memorised the training data rather than learning generalisable patterns.\n",
        "\n",
        "Scikit-learn provides a convenient utility function called `train_test_split` to help you partition your data in one line of code."
      ],
      "metadata": {
        "id": "Ku1MKL-dEi7z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "abMgwa4cqkU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Sample Split Example\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# instantiate model\n",
        "mod = LinearRegression()\n",
        "# mod = KNeighborsRegressor(n_neighbors=2)\n",
        "\n",
        "# arrange data\n",
        "X = qog.drop([\"hdi\", \"country\", \"region\", \"iso3c\", \"fh_status\"], axis=1)\n",
        "y = qog[\"hdi\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "# X = X.values.reshape(-1,1)\n",
        "y_train = y_train.fillna(y_train.mean())\n",
        "\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "\n",
        "# fit model\n",
        "mod.fit(X_train, y_train)\n",
        "# predict\n",
        "preds = mod.predict(X_test)\n",
        "\n",
        "# eval\n",
        "sns.scatterplot(x=preds, y=y_test);"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VcA0WZ_EFvW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5**.&nbsp; **Performance Metrics**\n",
        "\n",
        "Evaluating your model with the right metric is crucial. Different tasks (classification vs. regression) and different data characteristics (imbalanced classes, outliers, etc.) often require different metrics. Scikit-learn provides a variety of metrics to help you assess model performance in a consistent manner.\n",
        "\n",
        "Below is a concise section about performance metrics in Scikit-learn, with a focus on the most commonly used ones. This includes a general overview, a table of key metrics, their interpretations, and example syntax to get you started quickly.\n",
        "\n",
        "**Regression Metrics**\n",
        "\n",
        "\n",
        "**[Mean Squared Error (MSE)](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)**\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "- **Interpretation**: Penalises large errors more heavily than small ones (due to squaring).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import mean_squared_error\n",
        "  mse = mean_squared_error(y_true, y_pred)\n",
        "  ```\n",
        "\n",
        "**[Mean Absolute Error (MAE)](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error)**\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} \\left|y_i - \\hat{y}_i\\right|\n",
        "$$\n",
        "- **Interpretation**: Provides a direct measure of how far predictions deviate from actual values on average; more robust to outliers than MSE.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import mean_absolute_error\n",
        "  mae = mean_absolute_error(y_true, y_pred)\n",
        "  ```\n",
        "\n",
        "**[\\( R^2 \\) Score](https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score)**\n",
        "$$\n",
        "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
        "$$\n",
        "- **Interpretation**: Proportion of variance in \\(y\\) explained by the model. A value of 1 is a perfect fit, while negative values mean the model is worse than a simple horizontal line.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import r2_score\n",
        "  r2 = r2_score(y_true, y_pred)\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Classification Metrics**\n",
        "\n",
        "**[Accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score)**\n",
        "\n",
        "$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n",
        "\n",
        "- **Interpretation**: Proportion of samples correctly predicted. Good for balanced datasets.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import accuracy_score\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  ```\n",
        "\n",
        "\n",
        "**[Precision](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)**\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "- **Interpretation**: Of all predicted positives, how many are actually positive? Useful when false positives are costly (e.g., spam detection).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import precision_score\n",
        "  prec = precision_score(y_true, y_pred, average='binary')\n",
        "  ```\n",
        "\n",
        "\n",
        "**[Recall](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)**\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "- **Interpretation**: Of all actual positives, how many did we correctly identify? Important in scenarios where missing positives is costly (e.g., disease screening).\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import recall_score\n",
        "  rec = recall_score(y_true, y_pred, average='binary')\n",
        "  ```\n",
        "\n",
        "**[F1 Score](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics)**\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "- **Interpretation**: Harmonic mean of Precision and Recall, balancing both in one metric. Often used for imbalanced classification.\n",
        "- **Code Example**:\n",
        "  ```python\n",
        "  from sklearn.metrics import f1_score\n",
        "  f1 = f1_score(y_true, y_pred, average='binary')\n",
        "  ```\n",
        "\n",
        "> **Note**: For multi-class problems, specify `average='macro'`, `average='weighted'`, etc.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "- **Pick Metrics Wisely**: Choose metrics relevant to your problem and data characteristics. For example, use precision/recall/F1 for imbalanced classification tasks, or MAE if you want to be less sensitive to outliers in a regression context.  \n",
        "\n",
        "- **Interpretation**: Always interpret metrics in the context of your domain. A 90% accuracy can be misleading if your classes are highly imbalanced.  \n",
        "\n",
        "- **Compare Multiple Metrics**: Using more than one metric (e.g., accuracy + F1 score) often gives a more complete picture of model performance.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NrIhk0oSXxTq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSttlOsDe4Ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Metrics Example\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# pick and inst model\n",
        "# mod = KNeighborsRegressor(n_neighbors=5)\n",
        "mod = LinearRegression()\n",
        "\n",
        "# data prep\n",
        "y = qog[\"hdi\"]\n",
        "X = qog.drop([\"country\", \"region\", \"iso3c\", \"fh_status\", \"hdi\"], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
        "                                                    test_size=0.3)\n",
        "\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "y_train = y_train.fillna(y_train.mean())\n",
        "y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "mod.fit(X_train, y_train)\n",
        "\n",
        "preds = mod.predict(X_test)\n",
        "print(mean_squared_error(y_test, preds),\n",
        "      r2_score(y_test, preds))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Q37H8ohbYB2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "\n",
        "# Use gdp_pc as a target, predict it using appropriate features from the data.\n",
        "# explore how well your prediction is doing."
      ],
      "metadata": {
        "id": "tjSoSkG-nbcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6**.&nbsp; **Data Preprocessing**\n",
        "\n"
      ],
      "metadata": {
        "id": "C7G0e1sWHrH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Data preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) is a crucial step in machine learning that ensures models work efficiently and produce reliable results. Raw data often contains inconsistencies such as missing values, different feature scales, or categorical variables that need to be transformed before training a model. Scikit-learn provides a variety of preprocessing tools to standardise, normalise, and encode data for better performance.\n",
        "\n",
        "- **Scaling and Normalisation**:\n",
        "\n",
        "    Many machine learning algorithms work better when numerical features are on similar scales. Scaling improves numerical stability and speeds up model convergence.\n",
        "\n",
        "    - `StandardScaler`: Standardises features by removing the mean and scaling to unit variance, making data normally distributed. See [docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler).\n",
        "\n",
        "    ```python\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    ```\n",
        "\n",
        "    - `QuantileTransformer`: Transforms data to follow a uniform or normal distribution, useful when features contain outliers or are highly skewed. See [docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer).\n",
        "\n",
        "    ```python\n",
        "    from sklearn.preprocessing import QuantileTransformer\n",
        "    transformer = QuantileTransformer(output_distribution='normal')\n",
        "    X_transformed = transformer.fit_transform(X)\n",
        "    ```\n",
        "\n",
        "- **Encoding Categorical Data**:\n",
        "\n",
        "    Many machine learning models cannot directly process categorical variables, so they must be converted into numerical representations. You might be familiar with this process as creating dummy variables.\n",
        "\n",
        "    - `OneHotEncoder`: Converts categorical variables into binary columns, creating a separate column for each category. See [docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder).\n",
        "\n",
        "    ```python\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    encoder = OneHotEncoder()\n",
        "    X_encoded = encoder.fit_transform(X_categorical)\n",
        "    ```\n",
        "\n",
        "- **Imputing Missing Data**\n",
        "\n",
        "    Many real-world datasets contain missing values, which must be handled before training a model. One approach is to use an imputer to fill in missing values based on a chosen strategy. See [docs](https://scikit-learn.org/stable/api/sklearn.impute.html).\n",
        "\n",
        "    - `SimpleImputer`:\n",
        "    ```python\n",
        "        from sklearn.impute import SimpleImputer\n",
        "        imputer = SimpleImputer(strategy='mean')  # Replace missing values with the mean\n",
        "        X_imputed = imputer.fit_transform(X)\n",
        "        ```\n",
        "\n",
        "        Common imputation strategies:\n",
        "        - 'mean' (default): replaces missing values with the column mean.\n",
        "        - 'median': replaces missing values with the median, useful for skewed distributions.\n",
        "        - 'most_frequent': replaces missing values with the most common value (mode).\n",
        "        - 'constant': fills missing values with a specified constant.\n",
        "\n",
        "\n",
        "- **Feature Engineering**:\n",
        "\n",
        "    Feature engineering can improve model performance by transforming or creating new features.\n",
        "\n",
        "    - `PolynomialFeatures`: Generates polynomial and interaction terms from existing numerical features, useful for capturing non-linear relationships.\n",
        "\n",
        "        ```python\n",
        "        from sklearn.preprocessing import PolynomialFeatures\n",
        "        poly = PolynomialFeatures(degree=2, interaction_only=False)\n",
        "        X_poly = poly.fit_transform(X)\n",
        "        ```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sJ0zj3WgZVS4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DKKPwWdSZlmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Data Scaling Example\n",
        "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
        "\n",
        "# orignial\n",
        "sns.scatterplot(x=\"gdp_pc\", y=\"hdi\", data=qog);\n",
        "\n",
        "# scaled\n",
        "X_ex = X_train['gdp_pc'].values.reshape(-1, 1)\n",
        "X_scaled = scaler.fit_transform(X_ex)\n",
        "sns.scatterplot(x=X_scaled[:, 0], y=y_train);\n",
        "\n",
        "\n",
        "scaler = QuantileTransformer()\n",
        "mod = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# data prep\n",
        "y = qog[\"hdi\"]\n",
        "X = qog.drop([\"country\", \"region\", \"iso3c\", \"fh_status\", \"hdi\"], axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
        "                                                    test_size=0.25)\n",
        "\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "y_train = y_train.fillna(y_train.mean())\n",
        "y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "mod.fit(X_train, y_train)\n",
        "\n",
        "preds = mod.predict(X_test)\n",
        "print(mean_squared_error(y_test, preds),\n",
        "      r2_score(y_test, preds))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Mf2PhkWfZwX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise\n",
        "\n",
        "# Try to improve your prediction from previous exercise by experimenting with\n",
        "# different data scaling options!\n",
        "\n",
        "# try including region (categorical variable) into the prediction.\n",
        "# Refer to documentation and user-guide."
      ],
      "metadata": {
        "id": "aEzwYojsnIp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7**.&nbsp; **Pipeline**"
      ],
      "metadata": {
        "id": "IekBgqJAwewM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pipeline in scikit-learn provides a structured way to automate the sequence of preprocessing steps and model training, ensuring consistency and preventing data leakage. Instead of manually applying transformations and then fitting a model separately, a pipeline chains multiple steps together, making the workflow cleaner and reproducible.\n",
        "\n",
        "Each step in a pipeline consists of a transformer (e.g., `StandardScaler`, `OneHotEncoder`, `PolynomialFeatures`) followed by an estimator (e.g., `LogisticRegression`, `RandomForestClassifier`). Once defined, the entire pipeline can be treated like a single model—fitting, transforming, and predicting in one step.\n",
        "\n",
        "Basic syntax is:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('step_name_1', transformer_1),\n",
        "    ('step_name_2', transformer_2),\n",
        "    ('model', estimator)\n",
        "])\n",
        "```\n",
        "\n",
        "For more info, see [Pipeline Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). Also, see [ColumnTransformer documentation](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) for processing different column types."
      ],
      "metadata": {
        "id": "Cm1ErocToHvd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBC9FflUqXLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pipeline Example\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "X = qog.drop([\"hdi\", \"country\", \"region\", \"iso3c\", \"fh_status\"], axis=1)\n",
        "y = qog[\"hdi\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "y_train = y_train.fillna(y_train.mean())\n",
        "y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "     (\"scaler\", StandardScaler()),\n",
        "     (\"knn\", KNeighborsRegressor(n_neighbors=5))]\n",
        ")\n",
        "\n",
        "pipe.fit(X_train, y_train)\n",
        "preds = pipe.predict(X_test)\n",
        "\n",
        "mean_squared_error(y_test, preds)\n",
        "\n",
        "\n",
        "## with column transformer\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# X = qog.drop([\"hdi\", \"country\", \"region\", \"iso3c\", \"fh_status\"], axis=1)\n",
        "# y = qog[\"hdi\"]\n",
        "\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# y_train = y_train.fillna(y_train.mean())\n",
        "# y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "# cat_cols = [\"region\", \"fh_status\"]\n",
        "# num_cols = [\"perc_wip\", \"gdp_pc\", \"corruption\", \"glob_index\", \"fh_polity\"]\n",
        "\n",
        "# # create separate pipes for each col type\n",
        "# numeric_pipeline = Pipeline([\n",
        "#     (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
        "#     (\"scaler\", StandardScaler())\n",
        "# ])\n",
        "\n",
        "# categorical_pipeline = Pipeline([\n",
        "#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "#     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "# ])\n",
        "\n",
        "# # pull them into column transformer\n",
        "# preprocessor = ColumnTransformer([\n",
        "#     (\"num\", numeric_pipeline, num_cols),\n",
        "#     (\"cat\", categorical_pipeline, cat_cols)\n",
        "# ])\n",
        "\n",
        "# # main pipe\n",
        "# pipe = Pipeline([\n",
        "#     (\"preprocessor\", preprocessor),\n",
        "#     (\"knn\", KNeighborsRegressor(n_neighbors=5))\n",
        "# ])\n",
        "\n",
        "# pipe.fit(X_train, y_train)\n",
        "# preds = pipe.predict(X_test)\n",
        "# print(\"R^2:\", r2_score(y_test, preds))\n"
      ],
      "metadata": {
        "id": "Cw42S4-0oQu4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8**.&nbsp; **Cross Validation and Grid Search**"
      ],
      "metadata": {
        "id": "cpxBR4tAsiBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine learning models often have hyperparameters (e.g., `n_neighbors` in k-NN) that significantly impact performance. Instead of manually guessing these values, [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and [Cross-Validation](https://scikit-learn.org/stable/modules/cross_validation.html) help find the best combination automatically."
      ],
      "metadata": {
        "id": "uthb20ZI1QY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**\n",
        "\n",
        "Cross-validation (CV) helps evaluate model performance by splitting data into multiple training and validation subsets. A common approach is k-fold cross-validation, where the dataset is divided into k subsets (folds), and the model is trained k times, each time using a different fold as the validation set.\n",
        "\n"
      ],
      "metadata": {
        "id": "dZVvgeJYt3V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X = qog.drop([\"hdi\", \"country\", \"region\", \"iso3c\", \"fh_status\"], axis=1)\n",
        "y = qog[\"hdi\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "y_train = y_train.fillna(y_train.mean())\n",
        "y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "     (\"scaler\", StandardScaler()),\n",
        "     (\"knn\", KNeighborsRegressor(n_neighbors=5))]\n",
        ")\n",
        "\n",
        "cv_scores = cross_val_score(pipe, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "print(\"Cross-Validation R^2 Scores:\", cv_scores)\n",
        "print(\"Mean R^2 Score:\", cv_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqVHvG5T1YfC",
        "outputId": "0347955f-f45c-48d4-d146-1d468e4bed3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation R^2 Scores: [0.6024366  0.76431027 0.63143364 0.78889832 0.64496714]\n",
            "Mean R^2 Score: 0.6864091930582085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search**\n",
        "\n",
        "`GridSearchCV` from scikit-learn systematically tests multiple hyperparameter combinations, using cross-validation to evaluate each set of parameters, and selects the combination that yields the best performance according to a specified scoring metric. See [docs](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_or_pipeline = Model()\n",
        "param_grid = {\n",
        "    \"<step_or_model>__<parameter_name>\": [value1, value2, ...],\n",
        "    # add more parameters as needed\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model_or_pipeline,    # the model or pipeline\n",
        "    param_grid=param_grid,       # the parameter combinations to try\n",
        "    cv=...,                # how many cross-validation folds or a CV splitter\n",
        "    scoring='...',          # metric to optimise (e.g., 'accuracy', 'r2')\n",
        "    # other optional arguments like n_jobs, refit, etc.\n",
        ")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "d0LpilZO12LW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rrL5jsGkwRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Grid Search Example\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "X = qog.drop([\"hdi\", \"country\", \"region\", \"iso3c\", \"fh_status\"], axis=1)\n",
        "y = qog[\"hdi\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "y_train = y_train.fillna(y_train.mean())\n",
        "y_test = y_test.fillna(y_test.mean())\n",
        "\n",
        "\n",
        "pipe = Pipeline(\n",
        "    [(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "     (\"scaler\", StandardScaler()),\n",
        "     (\"knn\", KNeighborsRegressor(n_neighbors=5))]\n",
        ")\n",
        "\n",
        "# define param grid\n",
        "param_grid = {\n",
        "    'knn__n_neighbors': [3, 5, 7, 9],  # different k values for k-NN;\n",
        "    'scaler': [StandardScaler(), None]  # scaling vs. no scaling\n",
        "}\n",
        "\n",
        "# do grid search\n",
        "grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='r2')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best R^2 Score:\", grid_search.best_score_)\n",
        "\n",
        "\n",
        "# pd.DataFrame(grid_search.cv_results_)"
      ],
      "metadata": {
        "id": "EoFS93Rlt3jg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification"
      ],
      "metadata": {
        "id": "mv7gzjXP9BKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Exercise: Implement Classificaiton on your own!\n",
        "\n",
        "# Predict \"fh_status\". For that, remove the continous equivalent of fh_status first.\n",
        "# Then, pick an appropriate estimator (refer to docs), preprocess data as you see fit.\n",
        "# Use grid search to find the best performing configuration for your estimator.\n",
        "\n"
      ],
      "metadata": {
        "id": "7sAvl96RVx-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework\n",
        "\n",
        "Read the description and download the [Credit Card Fraud Dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud). Your task is to predict the `class` feature, which records whether a transaction is an instance of credit card fraud or is a legitimate transaction.\n",
        "\n",
        "You need to implement data preprocessing, decide which estimator to use, what metric(s) to evaluate the performance on. The decision of how to organise the pipeline (including sample splitting, grid seach over hyperparams) is entirerly up to you.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vxd6sHQ-f2_o"
      }
    }
  ]
}