---
output:
    bookdown::pdf_document2:
        toc: false
        number_sections: true
        includes:
            in_header:
                header.tex
            before_body: rdp_titlepage.tex
        keep_tex: false

documentclass: article

zotero: true
link-citations: true
bibliography: /Users/edwardanders/Documents/GitHub/oxford/metadata/zotero_library.bib
csl: /Users/edwardanders/Documents/GitHub/oxford/metadata/harvard-cite-them-right.csl

urlcolor: blue
linkcolor: blue
---

```{r setup, include=FALSE}
library(here)
source(here("thesis", "analysis", "packages.R"))
source(here("thesis", "writing", "helpers", "load_outputs.R"))
model_results <- readRDS(here("thesis", "outputs", "helpers", "model_results.rds"))
```

\newpage
\pagenumbering{roman}
\setcounter{page}{1}

# Abstract {-}

Advancements in machine learning have made Artificial Intelligence (AI) capable of generating hyper-realistic textual and visual content, accelerating its adoption as a powerful informational tool. Yet as generative AI technologies remain unregulated and vulnerable to misuse, concerns are growing about their potential to distort political messaging. This research investigates the polarising effects of AI-generated political content, focusing on how perceptions of trust and associations with misinformation influence affective polarisation among partisans. Using survey experiments with labelled and unlabelled AI- and human-generated content, the project tests whether source provenance moderates respondents’ emotional and trait-based evaluations of political outgroups. A pilot study conducted with YouGov provides initial evidence that unlabelled AI content increases discomfort and reduces respect toward opposing partisans—supporting the view that undetected AI can act as a persuasive and polarising influence. To complement the experimental design, an agent-based model is developed to simulate repeated exposures and long-run dynamics. Together, these approaches offer new insights into the political consequences of AI being used in poltical communication.

\begin{flushleft}
\textbf{Keywords:} artificial intelligence, affective polarisation, fake news, survey experiment, agent-based modelling
\end{flushleft}

\newpage
\setcounter{tocdepth}{2}
\tableofcontents

\newpage
# List of Tables {-}
\renewcommand{\listtablename}{}
\vspace*{-2em}
\listoftables

\newpage
# List of Figures {-}
\renewcommand{\listfigurename}{}
\vspace*{-2em}
\listoffigures

\newpage
\mainmatter
\pagenumbering{arabic}
\setcounter{page}{1}

# Introduction {#sec-intro}

Machine learning advancements to efficiently handle sequential data inputs and outputs have popularised the field of Artificial Intelligence (AI) [@vaswani2017]. AI is rapidly evolving into a transformative informational tool, with applications ranging from drug discovery to climate change modelling. Generative AI has emerged as the fastest-growing application, with tools like ChatGPT, Claude, and Midjourney gaining popularity through their ability to create sophisticated text, images, and video from simple prompts. Yet, these technological advancements are raising serious concerns from leading academics and AI developers alike. The ‘Godfather of AI’, Geoffrey Hinton, left Google over fears that safety and governance were are being overlooked in the pursuit of Artificial General Intelligence (AGI) [@metz2023]. As AI systems develop the capability to set their own goals and operate autonomously, they present catastrophic risks through malicious actions, unsafe behaviour, or exploitation by bad actors [@hendrycks2023]. But, in the near-term, sub-catastrophic risks are equally present. In particular, this research project is interested in AI’s capacity to ‘amplify social injustice, erode social stability, […] customised mass manipulation, and pervasive surveillance’ [@bengio2024]. These social and political risks of AI are often discussed anecdotally, but there remains little research nor evidence on what these risks look like. The UK Government's @departmentforsciencetechnology&innovation2025 views ‘manipulation and deception of populations’ a significant threat to political systems and societies; but, the extent to which politically targeted generative AI can be used to distort, deceive, and direct an electorate  remains unclear. Therefore, this project aims to answer:

\begin{quote}
\textit{Does exposure to AI-generated political content increase affective polarisation?}
\end{quote}

This research seeks to address a pressing puzzle: why should we fear fake news or deceptive propaganda produced by generative AI more than that of earlier eras? Three factors stand out: the volume, realism, and micro-targeting of the content generated. Generative AI can confidently hallucinate political falsehoods and be directed to produce hyper-realistic, nearly undetectable 'fake news' [@rawte2023; @duberry2022; @flew2021]. With 45% of the US population reportedly using generative AI and social media providing fertile ground for virality, the likelihood of exposure to AI-generated misinformation is rising [@salesforce2025]. However, democracies have long withstood misinformation campaigns, even before the advent of digital technologies [@bernays1928]. So, are current fears about AI-driven manipulation justified [@ansell2023]? To approach this question, we must consider: can AI-generated content influence political attitudes, voting intentions, or even electoral outcomes? In particular, this research focuses on the critical dimension of affective polarisation to evaluate whether AI-generated content can exacerbate partisan hostility.

This focus is warranted. Fake news tends to spread rapidly in echo chambers, which are known to foster heightened animosity toward political out-groups [@tornberg2018; @hobolt2023]. Since polarisation is closely tied to democratic backsliding and populist appeal, understanding AI’s role in amplifying these dynamics is vital. Clarifying these effects holds significant implications for regulators, platforms, and policymakers. Moreover, this study considers the mechanisms of behavioural influence and the potential for mitigating interventions. One such intervention --- labelling AI-generated content --- is often seen as a straightforward solution. Yet, early evidence suggests that labelling may itself reinforce negative associations with fake news and deepen polarisation [@altay2024]. Thus, this study treats labelling not only as an intervention but as an independent variable of interest.

To identify these effects, the research combines survey experiments with an AI-augmented agent-based model that simulates repeated exposure scenarios. It begins by reviewing the literature on AI and affective polarisation, followed by a theoretical framework and set of hypotheses. A pilot study conducted with YouGov is then presented, offering preliminary evidence that unlabelled AI-generated content may be especially persuasive and polarising. Finally, an innovative use of synthetic agents is proposed to further explore AI’s influence on political attitudes.

\begin{quote}
\textit{Note:} Given the word count constraints, the literature review has been moved to the appendix. I do not expect any feedback on this section. The primary focus of this proposal is the theoretical framework, the pilot study, and the next steps. These are the sections I would most appreciate feedback on.
\end{quote}

\newpage
# Theoretical Framework {#sec-theory}

As described in \hyperref[sec-lit-review]{Section \ref*{sec-lit-review}} above, the literature on the effects of AI-generated content is still nascent. Developing a theoretical framework to understand effects of exposure therefore requires a number of assumptions and leaning on theories of fake news and affective polarisation. Of particular guidance are formal models of the spread of misinformation within networks, namely those by @acemoglu2024, @dellalena2024, and @jones2024. The formal theory developed in this section takes these models and applies them to the models and hypotheses used in the affective polarisation literature from @tornberg2021 and @hobolt2023.

The model presented is motivated by these aforementioned models, and uses a simplified Bayesian-inspired updating set up for modelling a utility function response to AI-generated political information. While classical Bayesian updating requires agents to form posterior beliefs using formally specified likelihood functions, a simplified, quasi-Bayesian updating framework is used for three reasons:

\begin{enumerate}[label=(\roman*)]
    \item \textbf{Empirical Tractability:} Full Bayesian inference requires assumptions about prior distributions and signal noise that are unobservable in survey settings.
    \item \textbf{Psychological Plausibility:} Individuals often rely on heuristics when processing political information, especially under uncertainty about source credibility.
    \item \textbf{Interpretative Clarity:} The simplified rule permits direct mapping between theoretical parameters (e.g., trust in AI, ideological distance) and experimental treatment conditions.
\end{enumerate}

## Model Setup

Let the individual's belief about the ideological position of the outgroup be denoted by:

\begin{itemize}
    \item \( \theta_0 \): prior belief
    \item \( \theta_1 \): posterior belief
    \item \( C \): ideological content of the article
    \item \( \delta = |C - \theta_0| \): ideological distance between article content and prior
    \item \( S \in \{\text{AI}, \text{Human}\} \): true source of the article
    \item \( \hat{S} \): perceived source
    \item \( \beta_i \in [0, \infty) \): responsiveness to detected AI-generated content (higher values indicate greater persuasiveness)
    \item \( \beta^* \in [1, \infty) \): responsiveness to undetected AI-generated content (relative to baseline human content)
    \item \( d_i \in [0, 1] \): probability individual \( i \) detects the true source
\end{itemize}

The individual updates their belief according to \hyperref[eq:update-rule]{Equation \ref*{eq:update-rule}}:

\begin{equation}
\theta_1 = \theta_0 + \bar{\beta}_i \cdot w(\delta) \cdot (C - \theta_0)
\label{eq:update-rule}
\end{equation}

where:

\[
\bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^*
\quad \text{and} \quad
w(\delta) = \frac{1}{1 + \lambda \cdot \delta}, \quad \lambda > 0
\]

The term \( \bar{\beta}_i \) reflects the expected responsiveness to the article, based on the detection probability and whether the content is believed to be AI-generated. When detected, responsiveness is governed by \( \beta_i \), which may reflect either discounting (\( \beta_i < 1 \)) or amplification (\( \beta_i > 1 \)). When undetected, the content is processed with responsiveness \( \beta^* \geq 1 \), which allows for the possibility that AI content is more persuasive than human content even when its origin is unknown, as suggested by @goldstein2024.

The function \( w(\delta) \) reflects ideological receptiveness, with greater distance reducing responsiveness.

## Detection and Responsiveness {#sec:detection-responsiveness}

As has been widely reported, trust in AI-generated content is often low. When individuals are aware that content is AI-generated, they may be less likely to trust it [@afroogh2024]. However, this is not universally the case: for some individuals, particularly those who view algorithmic content as high-quality or ideologically aligned, detected AI content may be treated as even more persuasive than human-generated content. The model captures this by allowing responsiveness to detected AI content to exceed 1 (i.e., \( \beta_i > 1 \)).

Importantly, the model also accounts for the possibility that undetected AI content may be more persuasive than human-generated content. In this case, the individual does not consciously adjust their beliefs based on the source, but may nonetheless respond more strongly than they would to equivalent human-written information. This is captured by allowing the baseline responsiveness to undetected AI content to be \( \beta^* \geq 1 \).

Therefore, the expected responsiveness to content is a convex combination of two components — detection probability and responsiveness to detected AI content — as shown in \hyperref[eq:bar-beta]{Equation \ref*{eq:bar-beta}}:

\begin{equation}
\bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^*
\label{eq:bar-beta}
\end{equation}

Detection probability (\( d_i \)) depends on observable individual characteristics, such as education and political attention, as shown in \hyperref[eq:detection-probability]{Equation \ref*{eq:detection-probability}}:

\begin{equation}
\frac{\partial d_i}{\partial \text{Education}_i} > 0, \quad \frac{\partial d_i}{\partial \text{Political Attention}_i} > 0
\label{eq:detection-probability}
\end{equation}

Education increases individuals' ability to detect linguistic and structural cues of AI authorship. Political attention increases motivation to scrutinise political content, enhancing vigilance in identifying the source even in the absence of explicit labelling [@chein2024].

Responsiveness to detected AI content is captured by the parameter \( \beta_i \in [0, \infty) \), which reflects how much individuals update their beliefs when they are aware the content is AI-generated. Individuals with greater familiarity with or trust in AI systems may respond more strongly to detected AI content, as reflected in \hyperref[eq:discount-factor]{Equation \ref*{eq:discount-factor}}:

\begin{equation}
\frac{\partial \beta_i}{\partial \text{Education}_i} > 0
\label{eq:discount-factor}
\end{equation}

In contrast, \( \beta^* \geq 1 \) represents the baseline responsiveness to AI content that is not detected as such. This term allows for the possibility that undetected AI content may be inherently more persuasive — for example, due to improved stylistic coherence, ideological tailoring, or perceived neutrality.

Notably, education and political attention may affect both detection and responsiveness components, creating a theoretically rich asymmetry: more educated individuals are more likely to detect AI content (\( d_i \uparrow \)), and may apply a smaller penalty or even an amplification factor when doing so (\( \beta_i \uparrow \)). But even if they do not detect the AI origin, they may still respond more strongly than to human content, due to \( \beta^* \geq 1 \).

## Maximisation Problem

We assume individuals seek to minimise epistemic loss, defined as the squared distance between their updated belief and the article content. This is captured by the utility function:

\begin{equation}
u(\theta_1, C) = -(\theta_1 - C)^2
\label{eq:epistemic-loss}
\end{equation}

Individuals choose a responsiveness parameter \( \mu_i \in [0, \infty) \) such that their updated belief is given by:

\begin{equation}
\theta_1 = \theta_0 + \mu_i (C - \theta_0)
\label{eq:mu-update}
\end{equation}

The individual's optimisation problem becomes:

\begin{equation}
\max_{\mu_i \in [0, \infty)} -\left[(1 - \mu_i)(\theta_0 - C)\right]^2
\label{eq:maximise-loss}
\end{equation}

The utility in \hyperref[eq:epistemic-loss]{Equation \ref*{eq:epistemic-loss}} is maximised when \( \mu_i = 1 \), corresponding to full alignment between updated beliefs and the article content. However, individuals do not always fully trust the content, and their responsiveness is shaped by both ideological distance and beliefs about the credibility of the source.

We therefore assume that responsiveness is endogenously constrained by detection and perceived source credibility:

\begin{equation}
\mu_i = \bar{\beta}_i \cdot w(\delta)
\label{eq:mu-determined}
\end{equation}

Because \( \bar{\beta}_i \in [0, \infty) \), the model allows for cases where individuals update less than, equally to, or more than the signal direction (i.e., \( \mu_i < 1 \), \( = 1 \), or \( > 1 \)), depending on how persuasive they find the content.

## Treatment Conditions

As noted, labelling content as AI-generated may affect the perceived source and trust in the information. The model therefore considers two treatment arms based on the combination of true source (S) and whether the article is labelled:

**1. AI + Labelled**
\begin{itemize}
    \item \( \hat{S} = \text{AI} \)
    \item \( \bar{\beta}_i = \beta_i \)
    \item \( \mu_i = \beta_i \cdot w(\delta) \)
\end{itemize}

**2. AI + Unlabelled**
\begin{itemize}
    \item \( \bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^* \)
    \item \( \mu_i = \bar{\beta}_i \cdot w(\delta) \)
\end{itemize}

These treatment conditions are in reference to the control group where the article is human-generated and not labelled:

**Human + Unlabelled**
\begin{itemize}
    \item \( \bar{\beta}_i = 1 \) (assumed human by default)
    \item \( \mu_i = w(\delta) \)
\end{itemize}

## Comparative Statics

We now examine how the responsiveness parameter \( \mu_i \) varies with respect to key exogenous parameters in the model. This comparative statics analysis focuses on understanding how belief updating is shaped by source detection, ideological distance, and trust in AI.

### Exogenous Parameters

\hyperref[tab:model-parameters]{Table \ref*{tab:model-parameters}} below lists the key exogenous parameters in the model. These are a subset of a likely much longer list of possible parameters, but these are the most relevant and prominent factors:

\begin{table}[H]
\centering
\caption{Key Parameters Used in the Theoretical Model}
\label{tab:model-parameters}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Type} \\
\midrule
\( C \) & Ideological content of the article & Experimental treatment \\
\( \theta_0 \) & Individual's prior belief & Observed (pre-treatment) \\
\( S \in \{\text{AI}, \text{Human}\} \) & True source of article & Experimental treatment \\
Label & Whether the source is labelled & Experimental treatment \\
\( \delta = |C - \theta_0| \) & Ideological distance & Derived (individual-level) \\
\( \beta_i \) & Discount factor (AI trust) & Observed/inferred \\
\( d_i \) & Probability of detecting AI source & Derived \\
\( \beta^* \) & Responsiveness to undetected AI content & Model parameter \\
\( \lambda \) & Responsiveness decay parameter & Model parameter \\
\bottomrule
\end{tabular}
\end{table}

Recall that responsiveness is defined in \hyperref[eq:mu-definition]{Equation \ref*{eq:mu-definition}}:

\begin{equation}
\mu_i = \bar{\beta}_i \cdot w(\delta) = \bar{\beta}_i \cdot \frac{1}{1 + \lambda \cdot \delta}
\label{eq:mu-definition}
\end{equation}

where:

\[
\bar{\beta}_i =
\begin{cases}
1 & \text{if } S = \text{Human or perceived as Human} \\
\beta_i & \text{if } S = \text{AI and detected as such (i.e., labelled)} \\
d_i \cdot \beta_i + (1 - d_i) \cdot \beta^* & \text{if } S = \text{AI and unlabelled}
\end{cases}
\]

We now derive the relevant partial derivatives, one parameter at a time.

###  Ideological Distance \( \delta \)

The responsiveness parameter \( \mu_i \) is inversely related to ideological distance \( \delta \). As the ideological distance between the article content and the individual's prior belief increases, the responsiveness decreases due to the diminishing returns of ideological receptiveness. This effect is more pronounced when source credibility is high (i.e., when \( \bar{\beta}_i \) is large). This is captured by the derivative in \hyperref[eq:mu-derivative]{Equation \ref*{eq:mu-derivative}}:


\begin{equation}
\frac{\partial \mu_i}{\partial \delta} = \bar{\beta}_i \cdot \frac{\partial w(\delta)}{\partial \delta}
= \bar{\beta}_i \cdot \left( \frac{-\lambda}{(1 + \lambda \cdot \delta)^2} \right) < 0
\label{eq:mu-derivative}
\end{equation}

### Source Detection Probability \( d_i \)

In the cases where \(S = \text{AI}\) and the content is unlabelled, then the detection probability affects the responsiveness parameter \( \mu_i \) through the discount factor \( \bar{\beta}_i \). The derivative in \hyperref[eq:detection-probability]{Equation \ref*{eq:detection-probability}} captures this relationship:

\begin{equation}
\bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^*
\quad \Rightarrow \quad
\frac{\partial \mu_i}{\partial d_i} = (\beta_i - \beta^*) \cdot w(\delta)
\label{eq:detection-probability}
\end{equation}

When individuals become more likely to detect that content is AI-generated, they apply a different responsiveness depending on their trust in AI. If \( \beta^* > \beta_i \), then detection reduces responsiveness, as detected content is trusted less than undetected content. However, if \( \beta_i > \beta^* \), detection increases responsiveness. This captures the idea that more sophisticated individuals --- while better at detecting AI --- may also treat detected content differently depending on their predispositions.

### Discount Factor \( \beta_i \)

For AI-generated articles (labelled or unlabelled), individuals apply a discount factor \( \beta_i \) to the content based on their trust in AI. Individuals who are more trusting of AI-generated content (higher \( \beta_i \)) update their beliefs more strongly in response to such content. The effect is larger when the source is detected (higher \( d_i \)) shown in both \hyperref[eq:beta-derivative-labelled]{Equation \ref*{eq:beta-derivative-labelled}} and \hyperref[eq:beta-derivative-unlabelled]{Equation \ref*{eq:beta-derivative-unlabelled}}:

\begin{itemize}
    \item \textbf{Labelled AI:}
    \begin{equation}
    \mu_i = \beta_i \cdot w(\delta)
    \quad \Rightarrow \quad
    \frac{\partial \mu_i}{\partial \beta_i} = w(\delta) > 0
    \label{eq:beta-derivative-labelled}
    \end{equation}
    \item \textbf{Unlabelled AI:}
    \begin{equation}
    \bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^*
    \quad \Rightarrow \quad
    \frac{\partial \mu_i}{\partial \beta_i} = d_i \cdot w(\delta) > 0
    \label{eq:beta-derivative-unlabelled}
    \end{equation}
\end{itemize}

### Responsiveness to Undetected AI Content \( \beta^* \)

In the case of unlabelled AI content, responsiveness also depends on the baseline credibility of content that is not recognised as AI-generated. This is captured by the parameter \( \beta^* \), which enters the convex combination that defines \( \bar{\beta}_i \) when the source is AI and unlabelled:

\[
\bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^*
\]

The partial effect of \( \beta^* \) on the responsiveness parameter \( \mu_i \) is given by \hyperref[eq:beta-star-derivative]{Equation \ref*{eq:beta-star-derivative}}:

\begin{equation}
\frac{\partial \mu_i}{\partial \beta^*} = (1 - d_i) \cdot w(\delta) > 0
\label{eq:beta-star-derivative}
\end{equation}

This derivative reflects the idea that responsiveness increases with \( \beta^* \) when AI content goes undetected. In other words, the more persuasive undetected AI content is (higher \( \beta^* \)), the more the individual updates their beliefs in response to it. This effect is stronger when detection probability \( d_i \) is low.

### Responsiveness Decay Parameter \( \lambda \)

This parameter is a theoretical parameter which assumes that an individual's responsiveness to ideological content decays as the ideological distance increases. This is captured by the derivative in \hyperref[eq:lambda-derivative]{Equation \ref*{eq:lambda-derivative}}:

\begin{equation}
\frac{\partial \mu_i}{\partial \lambda} = \bar{\beta}_i \cdot \frac{\partial w(\delta)}{\partial \lambda}
= \bar{\beta}_i \cdot \left( \frac{-\delta}{(1 + \lambda \cdot \delta)^2} \right) < 0
\label{eq:lambda-derivative}
\end{equation}

Higher values of \( \lambda \) imply sharper declines in responsiveness with ideological distance. A higher \( \lambda \) implies more resistance to persuasion at larger ideological distances, but this effect flattens out as the distance increases. This parameter governs how ideologically resistant individuals are in general. It could be treated as a theoretical parameter or estimated at the population level.

### Summary of Comparative Statics

\begin{table}[H]
\centering
\caption{Summary of Comparative Statics for each Parameter}
\label{tab:comparative-statics}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Parameter} & \textbf{Partial Derivative} & \textbf{Sign} & \textbf{Interpretation} \\
\midrule
\( \delta \) & \( \frac{\partial \mu_i}{\partial \delta} \) & Negative & Greater ideological distance reduces responsiveness \\
\( d_i \) (AI + unlabelled) & \( \frac{\partial \mu_i}{\partial d_i} \) & Negative & Detection increases discounting and reduces updating \\
\( \beta_i \) (AI only) & \( \frac{\partial \mu_i}{\partial \beta_i} \) & Positive & More trust in AI increases responsiveness \\
\( \beta^* \) (AI + unlabelled) & \( \frac{\partial \mu_i}{\partial \beta^*} \) & Positive & Greater persuasiveness of undetected AI content increases responsiveness \\
\( \lambda \) & \( \frac{\partial \mu_i}{\partial \lambda} \) & Negative & More rigidity reduces responsiveness across the board \\
\bottomrule
\end{tabular}
}
\end{table}

## Belief Updating to Affective Polarisation

The model presented above provides a theoretical framework for understanding how individuals update their beliefs in response to AI-generated political content. The key parameters and comparative statics highlight the complex interplay between ideological distance, source detection, trust in AI, and responsiveness to content. Affective polarisation is defined as the difference between in- and out-group evaluations, shown in \hyperref[eq:affective-polarisation]{Equation \ref*{eq:affective-polarisation}}:

\begin{equation}
\text{AP}_i = L^{\text{in}}_i - L^{\text{out}}_i
\label{eq:affective-polarisation}
\end{equation}

where:
\begin{itemize}
    \item \( L^{\text{in}}_i \): affective evaluation of the in-group,
    \item \( L^{\text{out}}_i \): affective evaluation of the out-group.
\end{itemize}

We are interested in how the treatment-induced belief change \( \Delta \theta_i = \theta_1 - \theta_0 = \mu_i (C - \theta_0) \), where \( \mu_i \) incorporates detection and source responsiveness, affects the change in affective polarisation:

\begin{equation}
\Delta \text{AP}_i = \Delta L^{\text{in}}_i - \Delta L^{\text{out}}_i
\label{eq:change-affective-polarisation}
\end{equation}

### Asymmetric Malleability of Attitudes

The malleability of affective evaluations is not symmetric. @lee2022 find that most people are positive partisans, meaning they identify with a party because they like their side, rather than opposing the other side. Greater in-group identification — or particularly strong animosity toward the out-group — suggests that the malleability of affective evaluations declines with stronger initial feelings. This implies diminishing marginal returns to new information: individuals who already feel very positively or negatively about a group are less likely to change their attitudes. This is formalised in \hyperref[eq:asymmetric-malleability-out]{Equation \ref*{eq:asymmetric-malleability-out}} and \hyperref[eq:asymmetric-malleability-in]{Equation \ref*{eq:asymmetric-malleability-in}}:

\begin{equation}
\Delta L^{\text{out}}_i = \frac{1}{|L^{\text{out}}_i| + \varepsilon} \cdot \Delta \theta_i
\label{eq:asymmetric-malleability-out}
\end{equation}

\begin{equation}
\Delta L^{\text{in}}_i = \frac{1}{|L^{\text{in}}_i| + \varepsilon} \cdot f(\Delta \theta_i)
\label{eq:asymmetric-malleability-in}
\end{equation}

where:
\begin{itemize}
    \item \( \varepsilon > 0 \) ensures continuity at zero affect,
    \item \( f(\cdot) \) is a scaling function determining how belief updates about the out-group influence in-group feelings,
    \item If \( f(\cdot) = -\phi \cdot \Delta \theta_i \), with \( \phi \geq 0 \), then belief improvements about the out-group reduce in-group warmth due to contrast or identity differentiation. Here, \( \phi \) captures the extent to which belief updates favouring the out-group lead to reductions in in-group warmth.
\end{itemize}

Substituting into the expression for \( \Delta \text{AP}_i \), we obtain the final expression for the change in affective polarisation in \hyperref[eq:change-affective-polarisation]{Equation \ref*{eq:change-affective-polarisation}}:

\begin{equation}
\Delta \text{AP}_i = \left( \frac{-\phi}{|L^{\text{in}}_i| + \varepsilon} - \frac{1}{|L^{\text{out}}_i| + \varepsilon} \right) \cdot \Delta \theta_i
\label{eq:change-affective-polarisation}
\end{equation}

### Interpretation of Affective Polarisation Change

\begin{itemize}
    \item \textbf{Direction of change:} If \( \Delta \theta_i > 0 \) (i.e., the individual updates in a more favourable direction toward the out-group), then affective polarisation may decrease or increase depending on which affect is more malleable.

    \item \textbf{Attitude strength asymmetry:} The more entrenched an individual's dislike of the out-group, the less likely that attitude is to change. In such cases, belief change is more likely to influence in-group evaluations, potentially increasing polarisation if \( \phi > 0 \).

    \item \textbf{Symmetry:} If in-group and out-group attitudes are of similar strength, then affective polarisation is more likely to respond symmetrically to belief change.

    \item \textbf{Contrast effect:} When \( \phi > 0 \), positive updates about the out-group may reduce in-group warmth (e.g., due to identity threat or cognitive balancing), further decreasing polarisation.
\end{itemize}

This framework allows us to capture heterogeneity in the direction and magnitude of affective polarisation change as a function of both belief updating and initial affective attachments.

## Affective Polarisation Comparative Statics

We can now derive how changes in the model's exogenous parameters affect the change in affective polarisation \( \Delta \text{AP}_i \). As derived in \hyperref[eq:change-affective-polarisation]{Equation \ref*{eq:change-affective-polarisation}}, and restated below, we can define the responsiveness of affective polarisation to a belief change \( A_i \), and \( \mu_i \) in \hyperref[eq:responsiveness-affective-polarisation]{Equation \ref*{eq:responsiveness-affective-polarisation}} to give a cleaner expression for the change in affective polarisation in \hyperref[eq:rearanged-change-affective-polarisation]{Equation \ref*{eq:rearanged-change-affective-polarisation}}:

\[
\Delta \text{AP}_i = \left( \frac{-\phi}{|L^{\text{in}}_i| + \varepsilon} - \frac{1}{|L^{\text{out}}_i| + \varepsilon} \right) \cdot \Delta \theta_i
\quad \text{where} \quad \Delta \theta_i = \mu_i \cdot (C - \theta_0)
\]

Letting:

\begin{equation}
A_i = \left( \frac{-\phi}{|L^{\text{in}}_i| + \varepsilon} - \frac{1}{|L^{\text{out}}_i| + \varepsilon} \right)
\quad \text{and} \quad
\mu_i = \left( d_i \cdot \beta_i + (1 - d_i) \cdot \beta^* \right) \cdot \frac{1}{1 + \lambda \cdot \delta}
\label{eq:responsiveness-affective-polarisation}
\end{equation}

we can write:

\begin{equation}
\Delta \text{AP}_i = A_i \cdot \mu_i \cdot (C - \theta_0)
\label{eq:rearanged-change-affective-polarisation}
\end{equation}

### Ideological Distance \( \delta \)

As the ideological distance between the article content and the individual's prior belief increases, the individual's responsiveness declines, reducing belief updating and therefore the effect on affective polarisation. This is formalised in \hyperref[eq:delta-affective-polarisation]{Equation \ref*{eq:delta-affective-polarisation}}:

\begin{equation}
\frac{\partial \Delta \text{AP}_i}{\partial \delta} = A_i \cdot \frac{\partial \mu_i}{\partial \delta} \cdot (C - \theta_0)
= A_i \cdot \left( d_i \cdot \beta_i + (1 - d_i) \cdot \beta^* \right) \cdot \left( \frac{-\lambda}{(1 + \lambda \cdot \delta)^2} \right) \cdot (C - \theta_0)
< 0
\label{eq:delta-affective-polarisation}
\end{equation}

### Detection Probability \( d_i \)

This condition is relevant in the scenario where the article is AI-generated and unlabelled. In this case, more accurate detection of AI content increases the likelihood of discounting it, reducing belief updating and attenuating affective response, given by \hyperref[eq:detection-affective-polarisation]{Equation \ref*{eq:detection-affective-polarisation}}:

\begin{equation}
\frac{\partial \Delta \text{AP}_i}{\partial d_i} = A_i \cdot \frac{\partial \mu_i}{\partial d_i} \cdot (C - \theta_0)
= A_i \cdot (\beta_i - \beta^*) \cdot \frac{1}{1 + \lambda \cdot \delta} \cdot (C - \theta_0)
< 0 \quad \text{if } \beta_i < \beta^*
\label{eq:detection-affective-polarisation}
\end{equation}

As shown in \hyperref[sec:detection-responsiveness]{Section \ref*{sec:detection-responsiveness}}, \( d_i \) can be thought of as a function of individual characteristics such as education and political attention. Therefore, as education and political attention increase, the detection probability increases, leading to a decrease in affective polarisation if \( \beta_i < \beta^* \).

### Discount Factor \( \beta_i \)

The discount factor \( \beta_i \) captures the individual's trust in AI-generated content when it is detected. Higher trust leads to greater belief updating and potentially greater affective polarisation. In the case of labelled AI content, the source is explicitly known and \( \beta_i \) governs responsiveness directly. For unlabelled AI content, \( \beta_i \) only affects affective polarisation to the extent that the content is detected (i.e., \( d_i > 0 \)); otherwise, belief updating is governed by \( \beta^* \). This is formalised in \hyperref[eq:beta-affective-polarisation-labelled]{Equation \ref*{eq:beta-affective-polarisation-labelled}} and \hyperref[eq:beta-affective-polarisation-unlabelled]{Equation \ref*{eq:beta-affective-polarisation-unlabelled}}:

\begin{itemize}
    \item \textbf{Labelled AI:}
    \begin{equation}
    \frac{\partial \Delta \text{AP}_i}{\partial \beta_i} = A_i \cdot w(\delta) \cdot (C - \theta_0) > 0
    \label{eq:beta-affective-polarisation-labelled}
    \end{equation}

    \item \textbf{Unlabelled AI:}
    \begin{equation}
    \frac{\partial \Delta \text{AP}_i}{\partial \beta_i} = A_i \cdot d_i \cdot w(\delta) \cdot (C - \theta_0) > 0
    \label{eq:beta-affective-polarisation-unlabelled}
    \end{equation}
\end{itemize}


### Responsiveness to Undetected AI Content \( \beta^* \)

The parameter \( \beta^* \) governs how strongly individuals respond to AI-generated content that is not detected as AI This term enters into the responsiveness expression \( \mu_i \), and therefore affects belief updating and affective polarisation in unlabelled AI conditions. The partial derivative of affective polarisation with respect to \( \beta^* \) is given by \hyperref[eq:beta-star-affective-polarisation]{Equation \ref*{eq:beta-star-affective-polarisation}}:

\begin{equation}
\frac{\partial \Delta \text{AP}_i}{\partial \beta^*} = A_i \cdot (1 - d_i) \cdot w(\delta) \cdot (C - \theta_0)
\label{eq:beta-star-affective-polarisation}
\end{equation}

This effect is strictly positive, indicating that increases in the persuasiveness of undetected AI content \( \beta^* \), increase the change in affective polarisation. This effect is strongest when the detection probability \( d_i \) is low, and declines as detection increases. This implies that as AI content becomes more realistic, it has greater potential to persuade individuals and affect their in-group and out-group evaluations.

### Contrast Parameter \( \phi \)

Higher contrast sensitivity implies that more positive beliefs about the out-group reduce in-group warmth, thus reducing affective polarisation more strongly, given by \hyperref[eq:contrast-affective-polarisation]{Equation \ref*{eq:contrast-affective-polarisation}}:

\begin{equation}
\frac{\partial \Delta \text{AP}_i}{\partial \phi} = \frac{-1}{|L^{\text{in}}_i| + \varepsilon} \cdot \mu_i \cdot (C - \theta_0)
\label{eq:contrast-affective-polarisation}
\end{equation}

### Initial Affective Attachments

Stronger in-group warmth reduces in-group responsiveness, shifting weight to the out-group channel. Stronger out-group hostility makes it harder to reduce polarisation via changing out-group attitudes. For in-group warmth, this is captured by \hyperref[eq:in-group-affective-polarisation]{Equation \ref*{eq:in-group-affective-polarisation}} and for out-group hostility by \hyperref[eq:out-group-affective-polarisation]{Equation \ref*{eq:out-group-affective-polarisation}}:

\begin{equation}
\frac{\partial A_i}{\partial |L^{\text{in}}_i|} = \frac{\phi}{(|L^{\text{in}}_i| + \varepsilon)^2} > 0
\quad \Rightarrow \quad \frac{\partial \Delta \text{AP}_i}{\partial |L^{\text{in}}_i|} > 0 \text{ if } \Delta \theta_i > 0
\label{eq:in-group-affective-polarisation}
\end{equation}

\begin{equation}
\frac{\partial A_i}{\partial |L^{\text{out}}_i|} = \frac{1}{(|L^{\text{out}}_i| + \varepsilon)^2} > 0
\quad \Rightarrow \quad \frac{\partial \Delta \text{AP}_i}{\partial |L^{\text{out}}_i|} < 0 \text{ if } \Delta \theta_i > 0
\label{eq:out-group-affective-polarisation}
\end{equation}

### Summary of Affective Polarisation Comparative Statics

The comparative statics for the change in affective polarisation \( \Delta \text{AP}_i \) are summarised in \hyperref[tab:affective-polarisation-comparative-statics]{Table \ref*{tab:affective-polarisation-comparative-statics}} below:

\begin{table}[H]
\centering
\caption{Comparative statics of affective polarisation: summary of partial effects}
\label{tab:affective-polarisation-comparative-statics}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Parameter} & \textbf{Partial Derivative} & \textbf{Sign} & \textbf{Interpretation} \\
\midrule
\( \delta \) & \( \frac{\partial \Delta \text{AP}_i}{\partial \delta} \) & Negative & Greater ideological distance reduces belief updating \\
\( d_i \) & \( \frac{\partial \Delta \text{AP}_i}{\partial d_i} \) & Negative & Detection reduces responsiveness to AI content \\
\( \beta_i \) & \( \frac{\partial \Delta \text{AP}_i}{\partial \beta_i} \) & Positive & More trust in AI increases responsiveness \\
\( \beta^* \) & \( \frac{\partial \Delta \text{AP}_i}{\partial \beta^*} \) & Positive & More persuasive undetected AI content increases affective polarisation \\
\( \phi \) & \( \frac{\partial \Delta \text{AP}_i}{\partial \phi} \) & Negative & In-group contrast reduces affective polarisation \\
\( |L^{\text{in}}_i| \) & \( \frac{\partial \Delta \text{AP}_i}{\partial |L^{\text{in}}_i|} \) & Positive & In-group affect less malleable → greater weight on out-group \\
\( |L^{\text{out}}_i| \) & \( \frac{\partial \Delta \text{AP}_i}{\partial |L^{\text{out}}_i|} \) & Negative & Strong out-group dislike reduces scope for affective change \\
\bottomrule
\end{tabular}
}
\end{table}

## Hypotheses {#sec-hypotheses}

From this formal model of belief updating and affective polarisation, several testable hypotheses regarding the effects of AI-generated content on individuals' affective evaluations of in- and out-groups can be derived.

We now map the theoretical model onto the experimental design, which comprises one control group and two treatment conditions. The design is defined by whether the article is AI-generated and whether it is labelled. Articles not labelled are assumed to be human-generated by default, consistent with participants' likely priors in naturalistic settings.

### Treatment Structure

\begin{table}[H]
\centering
\caption{Treatment conditions by source and labelling}
\label{tab:treatment-conditions}
\begin{tabular}{@{}lll@{}}
\toprule
 & \textbf{Labelled (AI)} & \textbf{Unlabelled} \\
\midrule
\textbf{Human} & (not used) & (1) Control Group \\
\textbf{AI} & (2) Source Discount Condition & (3) Detection Condition \\
\bottomrule
\end{tabular}
\end{table}

### Theoretical Predictions and Heterogeneity

The primary theoretical prediction, and hypothesis this research project aims to test is:

\begin{quote}
\textbf{Hypothesis 1:} Exposure to AI-generated political content \textit{can} increase affective polarisation, particularly when the AI origin is not detected and the content is ideologically aligned with the individual’s priors.
\end{quote}

**(1) Human + Unlabelled — \textit{Control Group}**

\begin{itemize}
    \item Participants are expected to assume the article is human-generated.
    \item Belief responsiveness is high: \( \mu_i = w(\delta) \)
    \item No discounting is applied, and content is assumed credible.
    \item Affective polarisation change depends on the size of \( \Delta \theta_i \) and affective malleability.
\end{itemize}

\textit{Treatment effect heterogeneity:}
\begin{itemize}
    \item Higher ideological distance \( \delta \) → lower responsiveness
    \item Stronger affective priors → reduced attitude change
\end{itemize}

**(2) AI + Labelled — \textit{Source Discount Condition}**

\begin{itemize}
    \item Participants are explicitly told the article is AI-generated.
    \item Belief responsiveness: \( \mu_i = \beta_i \cdot w(\delta) \)
    \item Direct awareness of AI authorship reduces trust and updating.
    \item Affective polarisation change is smaller relative to the control.
\end{itemize}

\textit{Treatment effect heterogeneity:}
\begin{itemize}
    \item Higher \( \beta_i \): more similar to control group
    \item Lower \( \beta_i \): minimal belief updating and polarisation change
    \item Higher education → likely higher \( \beta_i \), attenuating the discount
\end{itemize}

\textit{Key comparison:} (2) vs. (1) — \textbf{Source Credibility Effect}

**(3) AI + Unlabelled — \textit{Detection Condition}**

\begin{itemize}
    \item Participants are not told the source; belief about source depends on detection probability \( d_i \).
    \item Responsiveness: \( \mu_i = \bar{\beta}_i \cdot w(\delta) \), where \( \bar{\beta}_i = d_i \cdot \beta_i + (1 - d_i) \cdot \beta^* \)
    \item Affective polarisation depends on detection probability \( d_i \) and the relative size of \( \beta_i \) vs. \( \beta^* \)
\end{itemize}

\textit{Treatment effect heterogeneity:}
\begin{itemize}
    \item High \( d_i \), low \( \beta_i \): strong discounting → lower responsiveness
    \item Low \( d_i \), high \( \beta^* \): content treated as credible → stronger responsiveness
    \item High education → increases detection \( d_i \) and may raise both \( \beta_i \) and \( \beta^* \), producing mixed effects
\end{itemize}

\textit{Key comparisons:}
\begin{itemize}
    \item (3) vs. (2) — \textbf{Detection Effect}
    \item (3) vs. (1) — \textbf{Combined Discounting and Detection Effect}
\end{itemize}

### Summary of Theoretical Comparisons

\begin{table}[H]
\centering
\caption{Interpretation of comparisons between treatment conditions}
\label{tab:treatment-comparisons}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Comparison} & \textbf{Name} & \textbf{Interpretation} \\
\midrule
(2) vs. (1) & Source Credibility Effect & Trust penalty for labelled AI content \\
(3) vs. (2) & Detection Effect & Role of source detection in moderating discounting \\
(3) vs. (1) & Combined Discounting and Detection & Total effect of AI content when not labelled \\
\bottomrule
\end{tabular}
\end{table}

As a result of this formal modelling, the model dentifies conditions under which AI-generated content can either increase, attenuate, or reduce affective polarisation. The key moderating mechanisms are:

\begin{itemize}
    \item Ideological distance \(( \delta \)) — the distance between the content and the individual's prior beliefs.
    \item Detection probability \(( d_i \)) — whether participants recognise the content is AI-generated.
    \item Trust in AI \(( \beta_i \)) — how much participants discount detected AI content.
    \item Persuasiveness of undetected AI content \(( \beta^* \)) — how influential undetected AI content is.
    \item Contrast sensitivity \(( \phi \)) — affects how in-group evaluations respond to out-group belief changes.
    \item Initial affective attachments — strength of existing in-group and out-group feelings.
\end{itemize}

\newpage

# Methodological Approaches {#sec-methods}

This research seeks to estimate the direction and magnitude of causal effects. To test the hypotheses set out in \hyperref[sec-hypotheses]{Section \ref*{sec-hypotheses}}, a survey experiment is used to isolate the effect of AI-generated content on affective polarisation. In addition, this proposal introduces an innovative extension: the use of agent-based modelling, enhanced by Large Language Models (LLMs), to simulate repeated exposures and complement the survey findings. This section outlines the survey design, presents initial pilot results, and discusses how agent-based methods could further develop the analysis.

As discussed in the literature review (\hyperref[sec-lit-review]{Section \ref*{sec-lit-review}}), this project focuses on the United Kingdom. While most research on affective polarisation centres on the United States, recent evidence shows that UK partisans also exhibit negative emotions toward out-groups [@berntzen2024]. Given the rise of populism and the increasingly polarised political climate in the UK, it is critical to assess whether new technologies, such as generative AI, pose risks to democratic processes—concerns raised during the 2024 General Election [@simon2024b]. Therefore, the following research design is tailored to the UK context, with the potential for future studies to assess external validity in other settings.

## Pilot Study: YouGov UniOM Survey Experiment {#sec-data-analysis}

To test whether effects are observable, and within which groups, a pilot study was conducted using a single survey experiment featuring exposure to an AI-generated article. The study was run with YouGov’s UniOM panel, a nationally representative sample of the UK population. The experiment employed two treatment conditions: AI-generated (unlabelled) and AI-labelled content. A between-subjects design was used to avoid pre-survey sensitivity issues identified by @levendusky2021.

The primary outcomes of interest are measures of affective polarisation, as discussed in \hyperref[sec-experiment-variables]{Section \ref*{sec-experiment-variables}}. Both treatment groups are compared against a control group exposed to human-generated content.[^human-labelled] The treatment conditions are summarised in \hyperref[tab:experiment-treatment-conditions]{Table \ref*{tab:experiment-treatment-conditions}}.

\begin{table}[H]
\centering
\caption{Treatments and Control for AI-generated Content Exposure}
\label{tab:experiment-treatment-conditions}
\begin{tabular}{@{}lll@{}}
\toprule
 & \textbf{No Labels} & \textbf{Labelled as AI} \\
\midrule
\textbf{Control} & Human-Generated Article & Human-Generated Article \\
\textbf{Treatment} & AI-generated Article & AI-generated Article \\
\bottomrule
\end{tabular}
\end{table}

[^human-labelled]: A third treatment condition of human-generated content labelled as AI-generated is also tested. This condition can be used to help understand whether participents discount based on content or whether they discount based on the source (i.e., AI vs. human). This additional analysis is not included in this proposal, but may be included in the final thesis.

### Experimental Variables {#sec-experiment-variables}

The survey experiment measures the effect of AI-generated content on affective polarisation, using both emotional and behavioural indicators. Respondents are randomly assigned to view either human-written, AI-generated, or AI-labelled news articles on a divisive topic—immigration in the UK. The treatment content is designed to provoke strong emotional responses and test whether such content can shift partisan attitudes. Full versions of the treatment articles are included in the Appendix (\hyperref[sec-treatment-articles]{Section \ref*{sec-treatment-articles}}).

Affective polarisation is assessed using standard measures from the literature. These include feeling thermometer scores towards in- and out-party leaders (used to compute a net difference, `thermo_gap`), as well as trait-based evaluations of respect and trust in out-parties. A behavioural proxy for polarisation is also included, asking respondents how they would feel if their child married a supporter of their least preferred party. These variables are discussed in full, along with question wordings and coding details, in the Appendix (\hyperref[sec-codebook]{Section \ref*{sec-codebook}} and \hyperref[sec-experiment-variables-appendix]{Section \ref*{sec-experiment-variables-appendix}}).

### Regression Specification {#sec-reg-spec}

To test the causal Average Treatment Effect (ATE) of respondents being exposed to AI-generated and AI-lebelled content on the set of affective polarisation measures, a series of regression models are estimated. The model specification is given by \hyperref[eq:reg-spec]{Equation \ref*{eq:reg-spec}}:

\begin{equation}
Y_i = \beta_0 + \beta_1 D_i + \beta_2 \mathbf{X}_i + \beta_3 (D_i \times \mathbf{Z}_i) + \varepsilon_i (\#eq:reg-spec)
\end{equation}

*where:*

\begin{itemize}
    \item \( Y_i \): outcome variable (\texttt{thermo\_gap}, \texttt{MLthermoMean}, \texttt{LLthermoMean}, \texttt{agreedisagree}, \texttt{xtrust}, \texttt{child})
    \item \( D_i \): treatment indicator (\texttt{AI-Generated Content, Labelled or Unlabelled})
    \item \( \mathbf{X}_i \): vector of covariates (see Balance Check in \hyperref[sec-balance]{Section \ref*{sec-balance}})
    \item \( \mathbf{Z}_i \): vector of interaction terms (treatment \(\times\) moderator)
    \item \( \varepsilon_i \): stochastic error term
\end{itemize}

In this specification, \( \beta_1 \) estimates the average treatment effect when the moderator(s) are at their reference level. Estimates are calculated with survey-weighted least squares and ordinal logistic models so results can be generalised to the UK more broadly. \( \beta_2 \) measures the effect of a one-unit change of a covariate on the outcome variable. \( \beta_3 \) captures the treatment effect heterogeneity across different sub-groups of the moderator, where statistically significant non-zero values suggest the ATE is different for different sub-group characteristics.

### Preliminary Results {#sec-results}

The results are presented in two parts. First, the continuous thermometer measures of affective polarisation are reported, followed by the ordinal emotional and trait-based outcomes. Each is evaluated across the three theoretical treatment conditions defined in \hyperref[sec-hypotheses]{Section \ref*{sec-hypotheses}}: (1) Source Credibility Effect, (2) Detection Effect, and (3) Combined Discounting and Detection Effect.

Regression tables include three models. Model (1) provides a baseline without covariates. Model (2) includes all pre-treatment covariates, justified both by theory and to improve efficiency, despite the balance check confirming covariate balance across treatment groups (\hyperref[sec-balance]{Section \ref*{sec-balance}}). Model (3) introduces interaction terms with likely moderators—education, political attention, and ideological distance—as theorised in \hyperref[sec-theory]{Section \ref*{sec-theory}}. All models apply survey weights to ensure representativeness of the UK population.

The thermometer score results suggest that unlabelled AI content slightly increases affective polarisation, while labelling reduces it. Notably, larger effects are seen among ideologically distant Liberal Democrat voters, though none reach statistical significance. In contrast, the trait- and emotional-based outcomes show stronger evidence: unlabelled AI-generated content increases discomfort and reduces respect for opposing partisans. These effects are most pronounced when the AI source is undisclosed, indicating that undetected AI content may be more persuasive than human-generated content and more likely to entrench affective divisions.

For the core measure—the thermometer gap between ratings of in- and out-party leaders—a higher score indicates greater polarisation. Regression results for this outcome are reported in \hyperref[tab:thermo-gap-overall]{Table \ref*{tab:thermo-gap-overall}}, \hyperref[tab:thermo-gap-source-cred]{Table \ref*{tab:thermo-gap-source-cred}}, and \hyperref[tab:thermo-gap-detection]{Table \ref*{tab:thermo-gap-detection}}.[^thermo-outcomes]

[^thermo-outcomes]: The full models for the outcome variables of `MLthermoMean` and `LLthermoMean` are available in the appendix in \hyperref[sec-thermo-means]{Section \ref*{sec-thermo-means}}.

\input{../../outputs/tables/thermo_gap_overall_ai_effect.tex}
\input{../../outputs/tables/thermo_gap_source_credibility.tex}
\input{../../outputs/tables/thermo_gap_detection_effect.tex}

All three treatments provide positive treatment effects on affective polarisation, but none are statistically significant. For the overall effect of unlabelled AI-generated content in in \hyperref[tab:thermo-gap-overall]{Table \ref*{tab:thermo-gap-overall}}, the effect was ``r round(model_results$ai_thermo_gap_coef, 3)`` points, with Liberal Democrats (compared to a base of Reform Party supporters) showin the largest subgroup positive treatment effect of ``r round(model_results$gap_ai_mostlikely_ldem, 3)`` points. With unlabelled AI content, we predict that if detection rates are low, and the persuasiveness of the content is high, then polarisation will increase. The positive point estimate is directionally consistent with this, but the null result suggests heterogeneity or insufficient power.

For the labelled AI-generated content in \hyperref[tab:thermo-gap-source-cred]{Table \ref*{tab:thermo-gap-source-cred}} compared to the human-generated content, the treatment effect is ``r round(model_results$label_thermo_gap_coef, 3)`` points. This is a test of the trust discount applied when content is known to be AI. The theory predicted lower responsiveness, a lower gap, which is seen on average. This aligns with the discounting hypothesis: when content is clearly labelled as AI, participants may apply a credibility penalty. However, Green Party respondents see a large (but not statistically significant) positive effect of ``r round(model_results$gap_ai_mostlikely_green, 3)`` points. This result challenges the core discounting prediction and might suggest that Green Party respondents did not discount labelled AI content; instead, labelling content as AI could fuel polarisation as viewers may associate the 'fake' AI content as a target from their out-group, pushing them further away.

The reslts from the dection effect model of labelled versus non-labelled AI-generated content in \hyperref[tab:thermo-gap-detection]{Table \ref*{tab:thermo-gap-detection}} shows labelled AI content reduced thermometer gap by ``r round(model_results$labelled_thermo_gap_coef, 3)`` points relative to unlabelled AI content. This result captures the pure effect of detection via labelling. While statistically insignificant, the direction and size of effects strongly support the theory that detection (via labelling) reduces affective polarisation, consistent with reduced belief updating from discounted AI content.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{/Users/edwardanders/Documents/GitHub/oxford/thesis/outputs/figures/thermo_patchwork_overall.pdf}    \caption{Thermometer Ratings: Average Treatment Effects}
    \label{fig:thermo-patchwork-overall}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{/Users/edwardanders/Documents/GitHub/oxford/thesis/outputs/figures/thermo_patchwork_ldem.pdf}    \caption{Thermometer Ratings: Average Treatment Effects for Liberal Democrat Respondents}
    \label{fig:thermo-patchwork-libdem}
\end{figure}

These results are summarised in \hyperref[fig:thermo-patchwork-overall]{Figure \ref*{fig:thermo-patchwork-overall}}, which show how the thermometer gap is driven by further in- or out-group polarisation. As described above, the overall effect of AI-generated content shows a weak trend toward greater polarisation, primarily via more positive in-group affect which is consistent with undetected AI content being more persuasive. On the contrary, the labelled AI content shows a trend towards reduced polarisation, primarily via reduced in-group affect. The detection effect shows a similar trend, with reduced in- and out-group affect, suggesting that when AI content is detected, it is less persuasive and reduces polarisation.

\hyperref[fig:thermo-patchwork-libdem]{Figure \ref*{fig:thermo-patchwork-libdem}} shows the same patchwork plot for respondents most likely to vote for the Liberal Democrats. This subgroup likely has greater ideological distance from the content, which is assumed to target right-wing themes (e.g., Reform Party). According to the theory, this group is expected to show lower responsiveness overall, but may still exhibit increased in-group warmth due to contrast effects when the AI source is undetected. Consistent with this, the results show that unlabelled AI content increases the thermometer gap for LD voters, while labelling the content (i.e. detection) significantly reduces in-group warmth and narrows the gap — providing clear support for the predicted role of ideological distance and detection in moderating affective polarisation.

To further assess treatment effects, ordinal logistic regression models are estimated for the outcomes *respect*, *trust*, and *discomfort* towards opposing partisans. These models account for the ordinal nature of the responses, with coefficients representing the log-odds of being in a higher response category (i.e. greater respect, trust, or comfort). The main ATE results for each measure and treatment are summarised above.[^ordinal-outcomes] [^pred-probs]

\input{../../outputs/tables/ordinal_results_matrix.tex}

Exposure to unlabelled AI-generated content provides compelling evidence that undetected AI increases affective polarisation. The log-odds coefficients for discomfort and respect are ``r round(model_results$ai_agreedisagree_coef, 3)`` and ``r round(model_results$ai_child_coef, 3)``, respectively --- both statistically significant at the 0.05 level. Negative coefficients across all measures indicate that unlabelled AI content reduces comfort, respect, and trust toward opposing partisans. This supports the hypothesis that undetected AI content reinforces polarisation, even after a single exposure in a small pilot. Notably, Liberal Democrat respondents show weaker effects, suggesting that ideological distance moderates responsiveness, with some respondents potentially persuaded by content on its own merits.

For labelled AI content, there is limited evidence of treatment effects. Aside from a small significant result on discomfort under the detection condition, all other coefficients are statistically insignificant. While the direction of effects remains negative—suggesting reduced persuasion due to detection—estimates are too small to draw firm conclusions.

The key takeaway is that undetected AI-generated content can heighten affective polarisation, particularly through emotional and trait-based reactions. These early results support the hypothesis that AI can be a persuasive tool for polarising attitudes. A follow-up YouGov survey will explore the emotional pathways driving these effects and inform the development of the agent-based model.

[^ordinal-outcomes]: The full models for the outcome variables of `agreedisagree`, `xtrust`, and `child` are available in the appendix in \hyperref[sec-ordinal-results]{Section \ref*{sec-ordinal-results}}.
[^pred-probs]: The predicted probabilities of each outcome category for each treatment condition are not calculated due to limitations of the `polr()` package, especially when using survey weights.

## Agent-based Modelling

A primary limitation of the pilot study lies in its statistical power, due to the restricted sample size. As detailed in \hyperref[sec-power-analysis]{Section \ref*{sec-power-analysis}}, detecting a treatment effect of five percentage points with 80% power requires a minimum sample of 3,000 respondents. The YouGov UniOM sample includes only 2,001 individuals—insufficient for detecting effects once subsetting by treatment group and respondent characteristics, where cell sizes often fall below 500. This constraint limits the capacity to test heterogeneous treatment effects or run repeated exposure trials. To address this, a complementary approach is proposed: agent-based modelling (ABM) using synthetic respondents generated by fine-tuned large language models (LLMs).

ABM simulates the beliefs, behaviours, and interactions of individual agents in a virtual environment, enabling dynamic testing of treatment effects beyond the constraints of sample size [@aher2023; @anthis2025]. Inspired by recent advances in computational social science, this approach draws on @argyle2023’s method of conditioning LLMs on real-world sociodemographic and survey data to create synthetic digital twins. These agents can be embedded in realistic scenarios to estimate how exposure to AI-generated political content—especially under repeated or cumulative conditions—shapes affective polarisation. The value of this method lies in its ability to replicate complex social dynamics while dramatically lowering the cost and sample-size demands of traditional RCTs [@angelopoulos2023a].

However, the effectiveness of ABM relies heavily on how well the LLM is fine-tuned. Evidence from @park2024 and @zhang2025 shows that conditioning models on specific, high-quality training data—especially survey and political response data—can significantly improve their predictive and behavioural validity. Yet this method is not without limitations. As shown by @santurkar2023 and @larooij2025, models trained on non-representative sources (e.g., X/Reddit) risk introducing bias, limiting generalisability. The next steps for this research will be to collect data from an array of sources such as X, the British Election Study, and sociological surveys to ensure the LLM is well-conditioned for the UK context. With the help of the training pipeline developed by @zhang2025 and the *Talking to Machines* project at Nuffield College, Oxford, I hope to be able to fine-tune a model to simulate experimental conditions and test not only single exposures but also repeated and conditional treatments, enabling an exploration of cumulative polarisation effects. The biggest challenge is expected to be the collection of high-quality, representative training data, along with the computational resources required to fine-tune the LLM. Results from this approach will be compared against the survey experiment findings to validate the model's predictive accuracy and generalisability.

\newpage

# Conclusion {#sec-conclusion}

This project proposes an integrated empirical and computational approach to examine the polarising effects of AI-generated political content. By combining survey experiments with agent-based modelling (ABM), the study advances our understanding of how trust, detection, and ideological distance moderate affective polarisation. The survey design enables causal identification of short-term exposure effects, while the ABM extends this to simulated long-term dynamics as well as other treatment articles, addressing a gap in existing literature. Fine-tuning LLM agents to replicate realistic public opinion patterns presents technical and ethical challenges, but studies which demonstrat this as a feasible and promising strategy are encouraging. The next steps are to get more granualar data on the emotional mechanisms driving affective polarisation through an additional YouGov UniOM survey. Data collection and initial fine-tuning will follow before Michalmas 2025. These new modelling techniques specific to polarisation in the United Kingdom have potential to offer novel theoretical and policy-relevant insights into the political risks posed by generative AI.

\newpage
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
# Appendix {-}
## Literature Review {#sec-lit-review}

This question builds upon the rise of fake news and affective polarisation with a distinct, new focus on the effects of AI-generated content, an area yet to be explored in the academic literature. This section first provides the context for the question’s focus, then gives an overview of the existing, limited literature on AI-generated content, and assesses the mixed literature on affective polarisation.

This research focuses on the United Kingdom (UK). Structural effects of globalisation and economic liberalism, coupled with individual political failings and electoral shocks, have created an increasingly unequal and divided world. Consequent disillusionment and disconnected identities have encouraged voter volatility and rising populist narratives, notably in the UK [@norris2019; @fieldhouse2019: 28–32]. This environment, combined with social media, has encouraged the dangerous spread of fake news, which has been shown to favour populists, affect voting behaviour, and strengthen identities and affective polarisation within online echo chambers [@cantarella2023; @pfister2023]. Given this volatile political landscape in the UK, with rising populist challengers, fears of the widespread dissemination of deceitful AI-generated information are justified. This research hopes to illuminate the extent to which we should be concerned about AI’s effects in the UK setting.

Generative AI is a subfield of Artificial Intelligence with the ability to generate new content in the form of text, images, and video based on generative models that use machine learning to identify patterns in training data [@sengar2024]. This AI-generated content, while often produced via human prompts, is generally computationally generated using probabilities rather than fact-checked, predefined truths. This research defines AI-generated content as any material produced by AI-based models, primarily through human prompting, to provide people with information, news, or arguments on any question or topic, including political ones. The focus is on whether such AI-generated content can affect political attitudes and behaviour, and therefore increase affective polarisation: the gap between the emotional warmth and attachment towards one’s in-group political party and the hostility shown to the out-group party [@green2004; @iyengar2012]. While affective polarisation is returned to later, it is important to raise why there is such concern about AI-generated content. As emphasised by @iyengar2019, “exposure to messages attacking the out-group reinforces partisans’ biased views of their opponents.” This negative messaging often takes the form of ‘fake news,’ a term @tandoc2018 associate with questions of facticity and audience perception. This issue is critical in the context of AI-generated content and may help explain why increased divisions are emerging within political societies. If AI generates fake or misleading content that spreads widely and targets out-groups within in-group echo chambers, polarisation may inevitably increase.

Despite the infancy of these AI tools, their use in politics is already a point of contention. @hackenburg2024 show that GPT-4 can have a persuasive influence in political microtargeting. While the research found evidence only for increased support for pre-existing policy perspectives, AI could potentially be used to persuade volatile voters to switch sides. For this reason, OpenAI sought to limit political bias by ensuring “ChatGPT did not express political preferences or recommend candidates even when asked explicitly,” whereas others, such as X’s Grok, have been caught sharing divisive political disinformation [@openai2024; @conger2025; @globalwitness2024]. These early, unregulated yet widely used models are therefore raising concerns that the content they generate may have negative consequences for elections through the spread of misinformation. The @worldeconomicforum2024 identifies this as a severe short-term risk, stating that “AI is amplifying manipulated and distorted information that could destabilise societies.”

But AI-generated misinformation goes beyond inaccurate chatbots. There is also fear around the deliberate and manipulative use of AI to generate deepfake images and videos that perpetuate divisive stereotypes or false narratives. However, because the technology remains nascent, many deepfakes are still detectable, highlighting the importance of including detectability as a variable in research design [@kapoor2024]. But what happens if deepfakes or AI-generated misinformation go undetected? Despite limited political science literature on AI, early findings suggest that AI-generated messages can be persuasive, and that propaganda produced by AI can be compelling [@bai2023; @goldstein2024]. As the quality of machine learning improves, AI-generated content is likely to become increasingly realistic and undetectable. Studies have shown that AI chatbots can be more persuasive than humans, in part because their personalised responses can exploit heterogeneity in users’ views, especially in relation to in-group and out-group perspectives [@salvi2025]. Yet at the same time, these GPTs frequently hallucinate the facts they present, lending credence to fears that AI will perpetuate fake news, partly due to the way reinforcement learning algorithms function [@thornhill2025].

Another issue concerns unintended consequences linked to perceptions of AI. It has been found that when readers are aware that political content is AI-generated, they may become sceptical of its validity, even when the content is factually accurate [@altay2024]. A possible mechanism here is trust. Users may associate AI-generated content—whether due to their own detection or because it is labelled—with fake news, thereby increasing scepticism. Consequently, labelling content as AI-generated may represent a flawed intervention. With detection, labelling, and the association of AI content with falsehoods, @cashell2024 argues that deepfakes and AI-generated materials are often used to reinforce existing stereotypes, rather than to persuade audiences to adopt new views.

To understand the causal effect of AI-generated content on affective polarisation—and how this may be driven by its overlap with fake news—the conceptualisation and drivers of affective polarisation must be considered. At its core, political polarisation refers to the distribution of ideological positions across a population [@hare2022]. This explains policy polarisation. However, partisan identity has grown in salience, to the point where it now better predicts voting behaviour than ideological disagreement [@algara2023].[^affective-polarisation-hypothesis] Recent research suggests that the “affect” in affective polarisation—emotional hostility toward out-partisans—is driven by fear, anxiety, disgust, and animosity [@bakker2024]. Summarised as partisan disdain, affective polarisation reinforces itself. These emotions shape information engagement and selectivity. Consequently, the information environment, especially on social media, distorts perceptions of political opponents, encouraging readers to engage with content that reflects negatively on the out-group.

This is where AI poses a threat. Angry partisans seek disconfirming information that affirms their beliefs [@mackuen2010]. AI can quickly and easily generate such disconfirming content. It enables affectively polarised individuals to disseminate fictitious, divisive, and realistic-seeming material, often tailored to resonate with the correct in- and out-groups. As a result, affective polarisation may intensify, eroding trust in democratic institutions, increasing discrimination, and reducing political engagement [@kingzette2021; @layman2006]. With this conceptual grounding, the next section presents a formal model to predict how exposure to AI-generated content might influence affective polarisation.

[^affective-polarisation-hypothesis]: Recent literature has also suggested that the ideological and policy differences between parties is also related to growing affective polarisation [@hobolt2021; @gidron2020].

\newpage
## Codebook {#sec-codebook}

The codebook below provides a summary of the variables used in the YouGov UniOM analysis. The variable names are provided in the first column, followed by the type of variable (e.g., categorical, continuous), a description of the variable, and the values that the variable can take. Note that the outcome variables of `agreedisagree`, `xtrust`, and `child` are ordinal variables on an ordered Likert scale.

\input{../../outputs/tables/codebook_table.tex}

\newpage

## Experimental Variables {#sec-experiment-variables-appendix}

Although politics is about persuasion, it is hard to move people's beliefs --- at least, not very quickly. @cobb1997 argue that voters often assess political arguments through emotional, feeling-based heuristics. Therefore, the treatment in the survey experiment was designed to be on a divise topic, which is likely to elicit strong feelings to encourage movement, especially as a single exposure is unlikely to change attitudes significantly. The treatment was an article on rising immigration in the UK, a topic which is prominment amongst populist rhetoric and polarised groups. The human-generated control article was written by @mckiernan2024 for the BBC. The AI-generated article was generated by OpenAI's GPT-4 model using the BBC article as an initial prompt, but instructed to re-write the article in a more divisive and exaggerated tone. Full versions of the articles can be found in the Appendix in \hyperref[sec-treatment-articles]{Section \ref*{sec-treatment-articles}}. Given the nature of the imflammatory topic of immigration used particularly by right-wing populists, the treatment is likely aligned with the ideological priors of right wing Conservative or Reform Party supporters, minimising their ideological distance \(( \delta \)), and therefore maximising the potential for belief updating and further affective polarisation shown in \hyperref[eq:delta-affective-polarisation]{Equation \ref*{eq:delta-affective-polarisation}}.

The measures required to understand AI's affect on affective polarisation are multi-faceted. Different measures can be used to understand the primary outcome of affective polarisation; however, the implication of each measure differs. @druckman2019 clearly outline the best practices for these affective polarisation measures, and how the measures interact. Therefore, this research chooses to follow these measurement recommendations for use in survey self-reporting [@iyengar2019].

The most common measure of someone's identifiction with a political party is through a feeling thermometer score. This aims to understand how warmly or coldly someone feels towards the political parties they most and leat prefer. The thermometer scores are measured on a scale of `0` to `100`, where `0` is the coldest and `100` is the warmest.[^thermo-scale] This survey experiment firstly asks respondents to identify their most and least preferred party (`mostlikely` and `leastlikely`), allowing for in- and out-party identities to be exposed. We then ask respondents to firstly rate how warmly they feel towards each of these party's leaders, `MLthermo_XY` and `LLthermo_XY`, where `XY` is replaced by each party leader's initials. The use of party-leader thermometers is a common measure, leaning on valence theory's emphasis on the importance of party leaders in shaping party identification and voting behaviour [@garzia2023].[^green-leaders] Moreover, Druckman and Levendusky's (2019: 119) findings show that respondents are more  negative towards party elites rather than party voters; thus, the focus on party leaders here helps elicit the more visceral feelings. Alongside these in- and out-group measures, a net-difference score (`thermo_gap`) is also calculated as the difference between the thermometer scores (`MLthermoMean - LLthermoMean`) [@iyengar2012].

The next indicator of affective polarisation is a trait-based rating. This measure identifies the traits that respondents associate with opposing parties [@garrett2014]. The limited scope of the survey experiment meant we focussed on the trait of positive trait of *respect*, and whether respondents associated this trait with oppossing parties. Respondents were asked: "To what extent do you agree or disagree with the following statement: `[leastlikely]` party voters respect my political beliefs and opinions." This question — coded as `agreedisagree` — was asked in a Likert scale format of levels of agreement.[^codebook]

Additionally, a similar trait-based measure focussed on *trust* was used [@levendusky2013]. Here, we ask "And how much of the time do you think you can trust `[leastlikely]` party to do what is right for the country?". This question was also asked in a Likert scale format, with the options of `Almost never`, `Once in a while`, `About half of the time`, `Most of the time`, and `Always`. This measure is coded as `xtrust`. Along with the themometer score, the trait-based views of respect, and trust in opposing parties, Druckman and Levendusky (2019: 119) argue that these measures are good, general measures of prejudices held towards opposing parties.

On the other hand, affective polarisation should also be interested in actual tangible discriminatory behaviour. Therefore an emotional, social-distance-based question is included to understand how comfortable respondents are with having opposing partisans in their lives. For example, @iyengar2012 popularised the use of the @almond1963 five-nation survey question "Suppose you had a child who was getting married. How would you feel if they married a `[leastlikely]` party voter?". Coded as `child`, respondents were given options of `Extremely upset`, `Somewhat upset`, `Neither happy nor upset`, `Somewhat happy`, and `Extremely happy`.

[^thermo-scale]: The wording for the theremoeter score questions is as follows: "We’d like to get your feelings toward some of our political leaders and other groups who are in the news these days. On the next page, we’ll ask you to do that using a 0 to 100 scale that we call a feeling thermometer. Ratings between 50 degrees and 100 degrees mean that you feel favourable and warm toward the person. Ratings between 0 degrees and 50 degrees mean that you don't feel favourable toward the person and that you don't care too much for that person. You would rate the person at the 50-degree mark if you don't feel particularly warm or cold toward the person."
[^green-leaders]: The Green Party has two co-leaders, Carla Denyer and Adrian Ramsay. Therefore, ratings of both leaders are asked, and the thermometer scores for the Green Party are averaged to create a single score for the party. The variables `MLthermoMean` and `LLthermoMean` are used as the final thermometer measures for in- and out-group thermometer scores.

[^codebook]: All survey experiment variables and values are in the codebook \hyperref[sec-codebook]{Section \ref*{sec-codebook}} in the appendix.

\newpage
## Survey Experiment Treatments {#sec-treatment-articles}

Due to the limited space in the YouGov UniOM survey experiment, the human-generated BBC article was shortened to fit the survey. Each article was presented with a title, image, and text of roughly 200 words. The human-generated article is presented in \hyperref[fig:human-generated-visual]{Figure \ref*{fig:human-generated-visual}} below, and the AI-generated article is presented in \hyperref[fig:ai-generated-visual]{Figure \ref*{fig:ai-generated-visual}}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{/Users/edwardanders/Documents/GitHub/oxford/thesis/writing/objects/human-generated-visual.pdf}
  \caption{Human-Generated Article: BBC News}
  \label{fig:human-generated-visual}
\end{figure}
\begin{quote}
{\large\textbf{Dame Priti Patel Defends Record on Rising Immigration}}

Dame Priti Patel has defended her record on immigration, which hit record levels during her time as home secretary. The MP said it was "too simple" and "lazy" to see the figures without "context" due to the pandemic and the war in Ukraine.

Speaking to supporters she said changes to the system allow ministers to control migration. Patel launched a new points-based immigration system and signed the agreement with Rwanda to send some asylum seekers to the country.

After a drop during the Covid pandemic, net migration - the number of people coming to the UK, minus the number of people leaving - then rose sharply during her time in the role, which she held until September 2022.

In 2022 net migration reached a record 745,000, according to the Office for National Statistics. Some Tories have pointed to the party's failure to reduce migration as a key reason for their historic defeat at the general election.

Pressed over her record on migration, Dame Priti said: "On my watch from 2019 onwards we delivered the structural reforms to the immigration and asylum system. The introduction of a points-based immigration system means the government has the levers to control who comes into our country".
\end{quote}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{/Users/edwardanders/Documents/GitHub/oxford/thesis/writing/objects/ai-generated-visual.pdf}
  \caption{AI-Generated Article: OpenAI GPT-4}
  \label{fig:ai-generated-visual}
\end{figure}
\begin{quote}
{\large\textbf{Border Crisis: Non-EU Migration Skyrockers as EU Numbers Plunge}}

UK Overrun as Non-EU Migration Surges!

Britain is facing an immigration crisis, with non-EU migration skyrocketing to unprecedented levels, far outpacing European arrivals.

Official data shows EU migration has declined since 2016, while non-EU arrivals have surged dramatically. A once-steady trend shot up after 2020, as Britain opened its doors to record numbers of foreign nationals.

Critics warn this surge is overwhelming public services, straining housing, schools, and the NHS. “We’ve lost control of our borders,” said a senior political source. Despite government promises, migration numbers keep rising.

Experts blame lax visa rules, a failing asylum system, and reckless international student policies. “It’s unsustainable,” one warned. “Ordinary Britons struggle to buy homes and see a doctor, yet migration is out of control.”

The shift from EU workers—who integrated and contributed—to non-EU arrivals is creating tensions. Communities are struggling with rapid change, and voters are demanding urgent action.

Pressure is mounting for ministers to act. Will they prioritise British workers? Or continue ignoring public frustration as towns and cities buckle under the strain?

One thing is clear: Britain is at a breaking point.
\end{quote}

\newpage

## Data Cleaning

`2,001` respondents were provided with the survey experiment. Respondents who did not give consent to participate in the survey were removed. Respondents were given the option to skip questions. When skipped, a value of `997` was assigned to the question, which was then recoded to `NA`, as were `Not asked` values.

The survey was interested in understanding respondents' views towards their most and least preferred party. When asked who the `mostlikely` and `leastlikely` party was, respondents were given the option to select `None of these`. Respondents who selected `None of these` were removed from the sample as they were unable to answer the follow-up questions.

Categorical variables were recoded to be `factors` in R, these were `profile_gender`, `profile_GOR`, `voted_ge_2024`, `pastvote_ge_2024`, `pastvote_EURef`, `profile_education_level`, `education_recode`, `profile_work_stat`, `xconsent`, `mostlikely`, `leastlikely`, `agreedisagree`, `xtrust`, and `child`.

Each of the thermometer variables were recoded to be `numeric` variables: `MLthermo_KB`, `MLthermo_KS`, `MLthermo_NF`, `MLthermo_ED`, `MLthermo_CD`, `MLthermo_AR`, `LLthermo_KB`, `LLthermo_KS`, `LLthermo_NF`, `LLthermo_ED`, `LLthermo_CD`, and `LLthermo_AR`. As the Green Party has two co-leaders, a mean thermometer score is calculated and used for most and least likely party thermometer scores, coded as `MLthermoMean` and `LLthermoMean`.

For treatment effect analysis, respondents were classified into two treatment groups: those shown AI-generated content (`ai_treatment`), identified where the split variable equalled `1` or `2`; and those shown AI-labelled content (`label_treatment`), identified where the split variable equalled `2` or `3`. Participants in the other split groups were coded as receiving human-generated or unlabelled content. These variables were coded as binary variables, where `1` indicated the treatment group and `0` indicated the control group.

\newpage

## Balance Check {#sec-balance}

To ensure that the randomisation process of the treatment allocation was successful, a balance check is conducted to ensure that the treatment and control groups are comparable in every way other than their treatment assignment status. The tables below report the balance of the covariates across the treatment groups. The continuous variables of `age` and `political_attention` are reported as means with the standard deviations in parentheses. The remaining categorical variables are reported as a count from the sample, with the proportions in parentheses. If there was a significant difference between the treatment and control groups, this is indicated with a `*` for p < 0.05, `**` for p < 0.01, and `***` for p < 0.001. The balance check shows that randomisation was successful across all covariates for both treatment groups as no covariates were significantly different between the treatment and control groups.

Note that the p-values are reported at the variable level, not for each individual category within a categorical variable. For categorical variables (e.g., gender, vote choice), a single p-value is generated using a chi-squared test, which assesses whether the overall distribution of categories differs between treatment and control groups. The individual category rows are displayed for reference, but since the test is run at the variable level, no p-value is reported for each specific level, giving the `NA` values in the tables.

For each of the categorical variables, there is a base reference category. For example, `profile_gender` uses the base reference category `Male` (reported as `Gender (Male)` in the balance tables). This base acts as the comparison group for the other categories, the p-value compares whether the distribution of the other categories is significantly different from the base category.

\input{../../outputs/tables/balance_ai_treatment.tex}
\input{../../outputs/tables/balance_label_treatment.tex}

\newpage

## Thermometer `MLthermoMean` and `LLthermoMean` {#sec-thermo-means}

\input{../../outputs/tables/thermo_ml_overall_ai_effect.tex}
\input{../../outputs/tables/thermo_ml_source_credibility.tex}
\input{../../outputs/tables/thermo_ml_detection_effect.tex}

\input{../../outputs/tables/thermo_ll_overall_ai_effect.tex}
\input{../../outputs/tables/thermo_ll_source_credibility.tex}
\input{../../outputs/tables/thermo_ll_detection_effect.tex}

\newpage

## Ordinal Outcome Regression Results {#sec-ordinal-results}

\input{../../outputs/tables/agreedisagree_ai_results.tex}
\input{../../outputs/tables/agreedisagree_label_results.tex}
\input{../../outputs/tables/agreedisagree_labelled_ai_results.tex}

\input{../../outputs/tables/xtrust_ai_results.tex}
\input{../../outputs/tables/xtrust_label_results.tex}
\input{../../outputs/tables/xtrust_labelled_ai_results.tex}

\input{../../outputs/tables/child_ai_results.tex}
\input{../../outputs/tables/child_label_results.tex}
\input{../../outputs/tables/child_labelled_ai_results.tex}

\newpage

## Power Analysis {#sec-power-analysis}

Power analysis is simulated on a model estimating the `thermo_gap` outcome, where the assumed treatment effect is `5` and a residual standard deviation is fairly high at `50`. Estimated across sample sizes of `500-5000`, the power estimates show the share of simulations in which the treatment effect is statistically significant at (( \ alpha \)) = 0.05. To detect an effect at least `80%` of the time, the results show a minimum sample size of `3,000` respondent per treatment subset. These results are summarised in \hyperref[fig:power-analysis]{Figure \ref*{fig:power-analysis}} below.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{/Users/edwardanders/Documents/GitHub/oxford/thesis/outputs/figures/power_analysis.pdf}
    \caption{Power Analysis for Thermometer Gap Outcome}
    \label{fig:power-analysis}
\end{figure}

\newpage

# References {-}
