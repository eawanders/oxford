---
title: 'University of Oxford: MPhil in Politics'
author: '1090063'
date: '`r Sys.Date()`'
output:
  pdf_document:
    toc: true
    number_sections: true
    fig_caption: yes
    includes:
header-includes:
  - \setlength{\footnotesep}{1em}
  - \setlength{\skip\footins}{2em}
  - \usepackage{setspace}
  - \setstretch{1.25}
citation_package: biblatex
csl: /Users/edwardanders/Documents/GitHub/oxford/metadata/harvard-cite-them-right.csl
bibliography: /Users/edwardanders/Documents/GitHub/oxford/metadata/zotero_library.bib
link-citations: true
---

```{r File Setup, include=FALSE}
# Set working directory
setwd("/Users/edwardanders/Documents/GitHub/oxford/causal_analysis/assignments/problem_set_1")

# Set global options
knitr::opts_chunk$set(
  cache = FALSE, # Prevents caching of code chunks
  warning = FALSE, # Prevents printing of warnings
  message = FALSE, # Prevents printing of messages
  echo = TRUE # Echo set to TRUE will print your code in the output file.
)

# Load pacman package
if (!require("pacman")) install.packages("pacman")
library(pacman)

# Load (and install) packages needed for the analyses
pacman::p_load(
  tidyverse, # Cleaning data, plotting plots
  dplyr, # Data manipulation
  knitr, # Knitting documents
  kableExtra, # Table formatting
  modelsummary, # Table formatting
  tribble # Create tibbles
)

knitr::knit_hooks$set(inline = function(x) {
  if (is.numeric(x)) {
    return(format(round(x, 3), nsmall = 3))
  }
  return(as.character(x))
})
```

# Problem 1

We will examine how randomized experiments work by creating an imaginary experiment. Use the dataset `a` from the file called `experiment.Rda`. For each individual unit ($i$) in our sample, the dataset contains the potential outcome under control ($Y_i^0$ or $Y_0i$) and the potential outcome under treatment ($Y_i^1$ or $Y_1i$) in the columns `a$y0` and `a$y1`, respectively. This is a purely hypothetical scenario. In reality, we never observe potential outcomes under both treatment and control for the same units: we can only observe one of them (the fundamental problem of causal inference). By creating a randomized experiment with this dataset, we’ll demonstrate how experiments overcome this fundamental problem.


```{r Problem 1 Data Setup, echo=FALSE, results='hide'}
# Load the dataset
load("experiment-1.Rda")

# Assign the dataset to a new variable (for converting to .csv)
experiment_data <- a

# Save the dataset as a .csv and .rds file (for working in VSCode)
saveRDS(experiment_data, file = "experiment-1.rds")
write.csv(a, "experiment-1.csv", row.names = TRUE)

# Read the dataset (using the .csv file) and display summary statistics
data <- read.csv("experiment-1.csv")
head(data)
summary(data)
```

## Question 1
Find the “true” Average Treatment Effect across all units. **[5 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

```{r True ATE, echo=FALSE}
# Calculate the average treatment effect (ATE) across all units, i
true_ate <- mean(data$y1 - data$y0)
```

The true Average Treatment Effect (ATE) is the value obtained if we were able to observe both the potential outcomes (treatment and control) for each individual. The true ATE is **``r true_ate``** and is calculated by taking the mean difference between the potential outcomes under treatment ($Y_1i$) and control ($Y_0i$) for each unit. However, this is a purely hypothetical scenario as we can never observe both potential outcomes for the same unit in reality.

## Question 2

Next, we’ll implement a randomized experiment on this sample of 100 units. We randomly assign half of the units to treatment and half to control by creating a new variable indicating treatment status ($D_i$).

Conduct a test to assess whether the treatment and control groups have the same average potential outcomes under control. Has randomization succeeded in creating treatment and control groups with equivalent potential outcomes under control? Why? **[5 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

```{r Randomisation, echo=FALSE, results='hide'}
# Set seed for reproducibility
set.seed(1)

# Assign each unit a random number between 0 and 100 in a new column
data$rand <- sample(c(1:100))

# Re-order the dataset from lowest to highest value of rand
data <- data[order(data$rand), ]

# Create a treatment variable named tr that equals 1 for the first 50 units and 0 for the second 50
data$tr <- c(rep(1, 50), rep(0, 50))

# View the variables in the dataset
glimpse(data)
```

```{r Test for Randomisation, echo=FALSE, results='hide'}
# Testing whether randomisation has given us equivalent potential outcomes under control

## Mean potential outcomes under control for treatment group
mean_y0_tr <- mean(data$y0[data$tr == 1])

## Mean potential outcomes under control for control group
mean_y0_ctrl <- mean(data$y0[data$tr == 0])

## Calculate a t-test to compare average potential outcomes under control
control_t_test <- t.test(data$y0[data$tr == 1], data$y0[data$tr == 0])
```

To assess whether the randomisation succeeded in creating treatment and control groups with equivalent potential outcomes under control, we compare whether the average potential outcomes under control for the treatment and control groups are statistically different. The mean potential outcomes under control for the treatment group is **``r mean_y0_tr``** and for the control group is **``r mean_y0_ctrl``**. A t-test comparing the average potential outcomes under control for the treatment and control groups gives a p-value of **``r control_t_test$p.value``**.

Therefore, randomisation has succeeded in creating treatment and control groups with equivalent potential outcomes under control. The null hypothesis is that the average potential outcomes under control are equal between the treatment and control groups. Given that **``p(`r control_t_test$p.value`) > 0.05``**, we cannot reject the null hypothesis as the treatment and control groups are not statistically different from one another.

## Question 3

Estimate the Average Treatment Effect based on your experiment. How similar is it to the “true” Average Treatment Effect? Explain. **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

```{r Estimated ATE, echo=FALSE, results='hide'}
estimated_ate <- mean(data$y1[data$tr == 1] - data$y0[data$tr == 0])

# Calculate the difference between the true and estimated ATE
diff_ate <- true_ate - estimated_ate

# Calculate the percentage difference between the true and estimated ATE
percent_diff_ate <- (diff_ate / true_ate) * 100
```

Since we have now randomised the distribution of the treatment and control groups, we can calculate the estimated Average Treatment Effect (ATE) to give the expected difference in outcomes between the treated and the 'comparable control'.

The estimated ATE is **``r estimated_ate``**. This is calculated by taking the mean difference between the potential outcomes under treatment ($Y_1i$) of the treatment group and the potential outcomes under control ($Y_0i$) of the control group. The esimated ATE is lower than the true ATE **``r estimated_ate` < `r true_ate``**
by **``r percent_diff_ate``%** because the treatment and control groups are not perfectly balanced. However, the estimated ATE is still close to the true ATE because the randomisation has created treatment and control groups with equivalent potential outcomes under control, as shown in `1.2`.

## Question 4

Now, let’s see how the experimental procedure performs over repeated randomizations.

What is the average estimated ATE across your 10,000 experiments? Does this suggest that your estimator is unbiased? Why? **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

```{r Repeated Randomisation, echo=FALSE, results='hide'}
# Create a function that takes in the dataset `data`, carries out a randomized experiment, and reports the estimated ATE.

randomised_experiment <- function(data) {
  # Assign each unit a random number between 0 and 100 in a new column
  data$rand <- sample(c(1:100))

  # Re-order the dataset from lowest to highest value of rand
  data <- data[order(data$rand), ]

  # Create a treatment variable named tr that equals 1 for the first 50 units and 0 for the second 50
  data$tr <- c(rep(1, 50), rep(0, 50))

  # Calculate the estimated ATE
  estimated_ate <- mean(data$y1[data$tr == 1] - data$y0[data$tr == 0])

  return(estimated_ate)
}

# Run the function 10,000 times, and store these results in a variable.

replicated_results <- replicate(10000, randomised_experiment(data))

# Calculate the average estimated ATE across the 10,000 experiments
mean_replicated_results <- mean(replicated_results)
```

The mean estimated ATE across the 10,000 experiments is **``r mean_replicated_results``**. This suggests that the estimator is unbiased because the average estimated ATE across the repeated randomisations is very close to the true ATE, **``r true_ate``**, a difference of ``r abs(mean_replicated_results - true_ate)``. The repeated randomisations improve our mean estimate of the ATE significatly from the single ransomisation sample done in `1.3`.

## Question 5

Repeat Task (4), calculating the mean difference in potential outcomes under control (a$y0) between the treatment and control groups instead of the ATE. What is the mean difference from your 10,000 experiments? What does this signify? **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

```{r Mean Difference in Potential Outcomes, echo=FALSE, results='hide'}
# Function to calculate the mean difference in potential outcomes under control between the treatment and control groups.

randomised_mean_diff_experiment <- function(data) {
  # Assign each unit a random number between 0 and 100 in a new column
  data$rand <- sample(c(1:100))

  # Re-order the dataset from lowest to highest value of rand
  data <- data[order(data$rand), ]

  # Create a treatment variable named tr that equals 1 for the first 50 units and 0 for the second 50
  data$tr <- c(rep(1, 50), rep(0, 50))

  # Calculate the mean difference in potential outcomes under control
  mean_diff_y0 <- mean(data$y0[data$tr == 1]) - mean(data$y0[data$tr == 0])

  return(mean_diff_y0)
}

# Run the function 10,000 times, and store these results in a variable.

replicated_mean_diff_results <- replicate(10000, randomised_mean_diff_experiment(data))

# Calculate the average mean difference in potential outcomes under control across the 10,000 experiments
mean_replicated_mean_diff_results <- mean(replicated_mean_diff_results)
```

The mean difference in potential outcomes under control between the treatment and control groups across the 10,000 experiments is **``r round(mean_replicated_mean_diff_results, digits = 5)``**. This signifies that the randomisation has succeeded in creating treatment and control groups with equivalent potential outcomes under control. The mean difference in potential outcomes under control is close to zero, indicating that the treatment and control groups are not statistically different from one another. This is consistent with the results from `1.2`.

# Problem 2

Past research suggests that ballot secrecy influences turnout. A recent field experiment sent emails to a random group of nonvoters around the 2014 election in Mississippi, reminding them that their vote was secret.

For this exercise, we wish to establish if the results of this experiment hold when we focus on the female subsample (instead of the full sample).

```{r Problem 2 Data Setup, echo=FALSE, results='hide'}
# Read the dataset and assign it to a new variable
ballot_data <- read.csv("ballot-1.csv")

# Ballot data summary statistics
head(ballot_data)
summary(ballot_data)
glimpse(ballot_data)

# Create a filtered dataset for a women-only subsample
female_data <- ballot_data %>%
  filter(d_gend_female == 1)
```

## Question 1

Confirm that the randomization process was successful by making sure that women in treatment and control groups are similar in all relevant aspects, e.g., age, ethnicity, non-voting habits. Show your results either using a figure or by producing a publishable table. **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

```{r Randomisation Check, echo=FALSE, results='hide'}
randomisation_check <- function(female_data, variables) {
  # Create an empty data frame to store results
  results_df <- data.frame(
    Variable = character(),
    T_Test = numeric(), # Use underscores instead of hyphens
    P_Value = numeric(),
    stringsAsFactors = FALSE
  )

  # Loop through each variable
  for (variable in variables) {
    # Perform t-test
    t_test <- t.test(
      female_data[[variable]][female_data$mail == 1],
      female_data[[variable]][female_data$mail == 0]
    )

    # Store results in the data frame
    results_df <- rbind(
      results_df,
      data.frame(
        Variable = variable,
        T_Test = t_test$statistic,
        P_Value = t_test$p.value
      )
    )
  }
  return(results_df)
}

# Run the function on selected variables
randomisation_results <- randomisation_check(
  female_data,
  variables = c("d_age", "d_race_blk", "d_race_hsp", "d_race_oth", "never_voted", "vote_year")
)
```

To test whether the women in treatment and control groups are similar in all relevant aspects, t-tests were conducted to see whether the groups were statostically significantly different from one another on each variable of interest. Table 1 shows the results of t-tests comparing the characteristics of women in the treatment and control groups. The null hypothesis is that the average values of the variables are equal between the treatment and control groups. The p-values are all greater than 0.05 indicating we cannot reject the null hypothesis. This suggests that the randomisation process was successful in creating treatment and control groups with similar characteristics as there are no statistically significant differences between groups. Although differences between observed variables across groups is found, there could still be selection bias from unobserved variables; however, this is unlikely if randomisation was successful.

```{r Randomisation Table, echo=FALSE}
# Add the results to a table
randomisation_results %>%
  mutate(across(c(T_Test, P_Value), ~ round(.x, 3))) %>%
  rename("t-test" = T_Test, "p-value" = P_Value) %>%
  kable(
    format = "latex",
    booktabs = TRUE,
    caption = "t-test and p-value Results for Variables",
    row.names = FALSE,
    align = c("l", "r", "r") # Align first column left, others right
  ) %>%
  kable_styling(latex_options = "hold_position") %>%
  footnote(
    general = "Note: significance values * p < 0.05, ** p < 0.01, *** p < 0.001.",
    general_title = ""
  )
```

## Question 2

Estimate the Average Treatment Effect and test whether the effects you calculated are robust to the inclusion of covariates. Report all results in a single table. Is it necessary to include covariates when calculating the ATE? **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

Although randomisation appears to have been successful, we still want to test for the effects of covariates on the estimated Average Treatment Effect (ATE). Firstly, a simple model is estimated for the ATE without covariates. Secondly, to estimate the ATE and test whether effects are robust to the inclusion of covariates, a regression model is compared to ATE without the inclusion of `d_age`, `d_race_blk`, `d_race_hsp`, `d_race_oth`, `never_voted`, and `vote_year`. Results of these two models are shown in Table 2.

```{r ATE without covariates, echo=FALSE, results='hide'}
# Difference in mean estimate to compare to regression model
## Estimated Average Treatment Effect (ATE) without covariates
ate_no_covariates <- mean(
  female_data$vote2014[female_data$mail == 1]
) - mean(
  female_data$vote2014[female_data$mail == 0]
)
ate_no_covariates

# t-test for ATE without covariates
t_test_no_covariates <- t.test(
  female_data$vote2014[female_data$mail == 1],
  female_data$vote2014[female_data$mail == 0]
)

# Regression model for Estimated Average Treatment Effect (ATE) without covariates
model_1 <- lm(
  vote2014 ~ mail,
  data = female_data
)
```

```{r ATE with covariates, echo=FALSE, results='hide'}
# Regression model for Estimated Average Treatment Effect (ATE) with covariates
model_2 <- lm(
  vote2014 ~ mail +
    d_age + d_race_blk + d_race_hsp + d_race_oth + never_voted + vote_year,
  data = female_data
)
```

```{r ATE Table, echo=FALSE}
# Goodness-of-fit statistics to add to table table
custom_gof <- tibble::tribble(
  ~raw, ~clean, ~fmt,
  "nobs", "Num. Obvs", 0,
  "r.squared", "R-squared", 3,
  "adj.r.squared", "Adj. R-squared", 3,
  "fstatistic", "F-statistic", 3
)

# Create a table comparing the Estimated ATE for Mail Treatment with and without covariates
modelsummary(
  list(
    "ATE without Covariates" = model_1,
    "ATE with Covariates" = model_2
  ),
  title = "Estimated ATE for Mail Treatment (With and Without Covariates)",
  coef_map = c(
    "mail" = "Mail Treatment",
    "d_age" = "Age",
    "d_race_blk" = "Black",
    "d_race_hsp" = "Hispanic",
    "d_race_oth" = "Other Race",
    "never_voted" = "Never Voted",
    "vote_year" = "Years Since Last Vote"
  ),
  stars = c("*" = 0.1, "**" = 0.05, "***" = 0.01), # Custom significance levels
  statistic = NULL, # Removes default significance legend
  gof_map = custom_gof, # Custom goodness-of-fit statistics
  notes = "Note: Standard errors are in parentheses. Both models are estimated using OLS regressions.",
  output = "latex"
)
```

The ATE without covariates is **``r (ate_no_covariates)``**. This tells us the difference in probability that someone voted between the treatment and control groups. Therefore, as the `vote_2014` is binary, those who received mail reminding them their vote was a secret saw a decreased probability of voting of **``r (ate_no_covariates)*100``** percentage points compared to the control group,a very small amount. The t-test for the ATE without covariates gives a p-value of **``r t_test_no_covariates$p.value``** meaning the results are also not statistically significant. Whilsy ballot secrecy may theoretically influence voting turnout, the effects seen are neither large nor statistically significant.

The second model accounts for the covariates which were shown to be similar across treatment and control groups in `2.1`. The inclusion of covariates does not change the estimated ATE significantly, with the ATE with covariates being **``r (summary(model_2)$coefficients[2, 1])``**. The p-value for the ATE with covariates is **``r summary(model_2)$coefficients[2, 4]``**, showing signs of weak significance, unlike when estimated without covariates. The inclusion of covariates controlling for black voters and those who have never voted before are the most significant. The model with covariates also has a higher R-squared value of **(``r summary(model_2)$r.squared` > `r summary(model_1)$r.squared``)** compared to the model without covariates. This suggests that the inclusion of covariates improves the model's explanatory power, but whilst the estimated ATE's are similar and neither is significantly robust, the inclusion of covariates is not necessary when calculating the ATE in this case, but should be included to ensure a more robust model.

# Problem 3

A key problem incumbents encounter in civil wars is a lack of information to combat insurgency. Insurgents exploit information asymmetries at the local level to hide and become a difficult target for incumbents. In the absence of such information, incumbents often resort to indiscriminate violence, via large-scale reprisals against entire villages suspected to host insurgents. One example of indiscriminate violence is Aerial bombardment. Due to the nature of insurgency, bombing frequently occurs in and around settled areas, and leading to many civilian casualties.

Using data from the Vietnam War, Kocher, Pepinsky and Kalyvas (2011) examined the effect of bombings on Viet Cong support. In particular, they looked at the impact of the September 1969 bombings on hamlet control in December 1969.

The data comes from various sources. The United States compiled a gazetteer of South Vietnamese hamlets, identified their geographic coordinates, and conducted a census. District Senior Advisors (DSAs, army officers ranking major or above) were assigned to complete detailed questionnaires for every village and hamlet in their zones of operation. Some of these questionnaires were compiled monthly, others quarterly. DSAs were detached from U.S. units to live and work in the districts they rated. The RVN (Republic of Viet Nam) had 261 districts with a median area of 377 kilometers squared, or about one-fourth the size of the median U.S. county. There was a median of 36 hamlets per district in 1969. In this exercise, we will explore how matching models work, using the dataset in `Vietnam_matching.dta`. We will apply the matching procedure to estimate the effect of experiencing a bombing in September 1969 on insurgency control in December 1969.

## Question 1

Which covariates should you use in the matching procedure and why? **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

When looking at the effects of bombings on hamlet control, we want to estimate the average causal (treatment) effect of the bombing in September 1969 on the control of hamlets by the Viet Cong in December 1969. To estimate this average causal effect, we should compare the hamlets that were bombed in September 1969 to those that were not; however, as the bombings were not randomised, we need to identify suitable control hamlets where covariates are similar to those that were bombed. We can do this through a matching procedure to find hamlets which are as similar to one another as possible, other than whether they were bombed or not. Before selecting a matching technique to identify hamlets, a suitable set of covariates should be selected for use in the marching procedure.

There are three primary types of covariates that should be considered for the matching procedure:

- covariates likely associated with the treatment (bombing) and the outcome (hamlet control),
- covariates that occur pre-treatment, and
- covariates which are unaffected by the treatment (bombing).

Firstly, we should control for all covariates which determine both bombings and hamlet control as these are likely confounders. We want to control for confounders as these variables are ones which may infuence the likelihood of bombing or hamlet control, and therefore may bias the estimated effect of the bombingm giving spurious results. Secondly, covariates which are pre-treatment measures should be included to avoid post-treatment measures being affected by the treatment. And thirdly, only covariates which are not affected by the treatment should be included.

The following covariated should therefore be included in the matching procedure:

```{r Matching Covariates Table, echo=FALSE}
# Table of covariates to be used in the matching procedure and why
covariates_table <- data.frame(
  Covariate = c(
    "Rough terrain (`std`)",
    "Distance to boundary (`ln_dist`)",
    "Development index (`score`)",
    "Hamlet population (`lnhpop`)",
    "Pre-bombing control (`mod2a_1admn`)"
  ),
  Explanation = c(
    "More difficult terrain may influence both bombing likelihood and hamlet control.",
    "Hamlets closer to international borders may have strategic importance affecting both bombing decisions and control.",
    "More developed hamlets may have been targeted differently and had different control conditions.",
    "Larger hamlets may have been more strategically important, affecting both bombing decisions and control dynamics.",
    "If a hamlet was already contested or insurgent-controlled before bombing, this could affect both the likelihood of being bombed and later control."
  )
)

# Display the table
kable(covariates_table, format = "latex", booktabs = TRUE, align = "l") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  kableExtra::column_spec(2, width = "10cm") # Adjust column width
```


## Question 2

Choose a matching estimator – briefly describe your choice. Assess balance in pre-treatment covariates between treated and control units, before and after matching. What is the measure you used for assessing balance? **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

## Question 3

Estimate the causal effect of interest with matching. Are the results from your matching analysis different from using a simple OLS regression without covariates? If there is a difference: how do you explain it? **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}

## Question 4

How does the comparison between matching and OLS regression change when you include covariates (control variables)? Report the models in a single table and discuss. **[10 points]**

\hspace{0pt}\rule{100pt}{0.4pt}